Dear Edwin D Simpson:

As  TACL action editor for submission 1538, "Bayesian Ensembles of Crowds
and Deep Learners for Sequence Tagging",   I am writing to tell you that I
am not accepting your paper in its current form, but due to its current
strengths and potential, I encourage you to revise and submit it within 3-6
months.

As you see, all three reviewers agree that there is insufficient detail on
the proposed  methods, which is a major concern. In addition, experiments
are not set up in a way to allow comparison with prior work. At a minimum,
such decision should be justified in a discussion.

You can find the detailed reviews below. My judgment is that the submission
is therefore not currently acceptable and that it would not be feasible to
bring the submission to acceptable form within two months.  However, I do
think that with significant changes, which could be undertaken in the 3-6
month range, TACL would be very happy to reconsider a revised version.

If you do choose to revise and resubmit, please make use a *new* submission
number, and follow the instructions in section "Revision and Resubmission
Policy for TACL Submissions" at
https://transacl.org/ojs/index.php/tacl/about/submissions#authorGuidelines.
I am allowing you one to two additional pages in the revised version for
addressing the referees' concerns.

Please understand that while we have endeavored to provide some guidance on
how to revise the manuscript, we have NOT provided a complete list of
modifications that guarantee acceptance; this is the distinguishing
characteristic between the decision we have given your submission --- (c),
rejection, but with encouragement to resubmit --- and the next higher level
of evaluation, which is conditional acceptance ("(b)", in TACL terminology).
The paper will be **reviewed afresh** should you choose to resubmit
(possibly involving a change of action editor and reviewers), with **no
guarantee of acceptance**, even if you make all the changes suggested.  

Again, just to prevent misunderstandings, we repeat: **making all the
changes suggested here does not guarantee subsequent acceptance**.  A
resubmission is treated as a new submission, and the subsequent review may
identify different problems with the paper.


Please also note that if you do choose to revise and resubmit, TACL policy
is, generally, to try not to give a (c) resubmission another (c), but
rather, if the second revision does not meet the acceptance bar, to impose a
rejection with a 1-year moratorium on resubmission.  Thus, please be very
thorough in revising any resubmission.

Thank you for considering TACL for your work, and, although you should take
careful note of the caveats above, I do encourage you to revise and resubmit
within the specified timeframe.


Ani Nenkova
University of Pennsylvania
nenkova@seas.upenn.edu
------------------------------------------------------
------------------------------------------------------
....THE REVIEWS....
------------------------------------------------------
------------------------------------------------------
Reviewer A:

CLARITY: For the reasonably well-prepared reader, is it clear what was done
and why? Is the paper well-written and well-structured?: 
	4. Understandable by most readers.


INNOVATIVENESS: How original is the approach? Does this paper break new
ground in topic, methodology, or content? How exciting and innovative is the
research it describes?

Note that a paper can score high for innovativeness even if its impact will
be limited.
: 
	3. Respectable: A nice research contribution that represents a notable
extension of prior approaches or methodologies.

SOUNDNESS/CORRECTNESS: First, is the technical approach sound and
well-chosen? Second, can one trust the claims of the paper -- are they
supported by proper experiments and are the results of the experiments
correctly interpreted?: 
	4. Generally solid work, although there are some aspects of the approach or
evaluation I am not sure about.


RELATED WORK: Does the submission make clear where the presented system sits
with respect to existing literature? Are the references adequate?

Note that the existing literature includes preprints, but in the case of
preprints:
• Authors should be informed of but not penalized for missing very recent
and/or not widely known work.
• If a refereed version exists, authors should cite it in addition to or
instead of the preprint.
: 
	4. Mostly solid bibliography and comparison, but there are a few additional
references that should be included. Discussion of benefits and limitations
is acceptable but not enlightening.


SUBSTANCE: Does this paper have enough substance (in terms of the amount of
work), or would it benefit from more ideas or analysis?

Note that papers or preprints appearing less than three months before a
paper is submitted to TACL are considered contemporaneous with the
submission. This relieves authors from the obligation to make detailed
comparisons that require additional experiments and/or in-depth analysis,
although authors should still cite and discuss contemporaneous work to the
degree feasible.
: 
	4. Represents an appropriate amount of work for a publication in this
journal. (most submissions)

IMPACT OF IDEAS OR RESULTS: How significant is the work described? If the
ideas are novel, will they also be useful or inspirational? If the results
are sound, are they also important? Does the paper bring new insights into
the nature of the problem?: 
	4. Some of the ideas or results will substantially help other people's
ongoing research.

REPLICABILITY: Will members of the ACL community be able to reproduce or
verify the results in this paper?: 
	3. They could reproduce the results with some difficulty. The settings of
parameters are underspecified or subjectively determined, and/or the
training/evaluation data are not widely available.

IMPACT OF PROMISED SOFTWARE:  If the authors state (in anonymous fashion)
that their software will be available, what is the expected impact of the
software package?: 
	4. Useful: I would recommend the new software to other researchers or
developers for their ongoing work.

IMPACT OF PROMISED DATASET(S): If the authors state (in anonymous fashion)
that datasets will be released, how valuable will they be to others?: 
	1. No usable datasets submitted.


TACL-WORTHY AS IS? In answering, think over all your scores above. If a
paper has some weaknesses, but you really got a lot out of it, feel free to
recommend it. If a paper is solid but you could live without it, let us know
that you're ambivalent.

Reviewers: after you save this review form, you'll have to make a
confidential recommendation to the editors via pull-down menu as to: what
degree of revision would be needed to make the submission eventually
TACL-worthy?
: 
	4. Worthy: A good paper that is worthy of being published in TACL.


Detailed Comments for the Authors

Reviewers, please draft your comments on your own filesystem and then copy
the results into the text-entry box.  You will thus have a saved copy in
case of system glitches.
: 
	The paper proposes Bayesian models for sequence tagging based on a
combination of both noisy crowdsourced annotations and the predictions of
one or more potentially unreliable tagging algorithms.  The approach is to
take several standard Bayesian crowdsourcing models, including a slightly
novel sequential annotation extension of the Dawid and Skene model, and
extend them to the sequential setting by modeling the true sequence labels
via a Markov assumption.  Another innovation is to model the performance of
black box sequence tagging algorithms as members of the "crowd," and jointly
train these models as an ensemble together with the sequential crowdsourcing
model.  The authors argue that this joint ensemble procedure makes sense
even when the black box sequence models are not Bayesian, e.g. standard deep
neural networks.  The overall crowdsourced sequence tagging ensemble
meta-model is trained via variational inference.

The paper is well written and argued.  The proposed sequential tagging
crowdsourcing ensemble is interesting, and the proposal to model and jointly
train sequence tagging algorithms together with human annotators is
innovative.  Experiments are thorough, measuring performance along multiple
metrics for both real and synthetic data where several key problem
attributes are varied, with comparison to a selection of strong baselines. 
The small data experiments, including the use of active learning, are
valuable.  Only two relatively small datasets are used, although this is not
a major limitation since the experiments are quite detailed, and the focus
of the work is on the small data case.

My main concern is that no derivations, and only the vaguest hand-waived
arguments, are provided regarding the proposed variational inference
algorithm.  Update equations are not provided either.  I presume that the
algorithms can be derived by positing a fully factorized mean-field
assumption, and performing coordinate ascent on the ELBO, with all
individual updates derived based on the general case for mean field VI where
the complete conditionals are in the exponential family.  The case where the
sequence taggers are non-Bayesian can presumably be justified as a
variational EM algorithm.  The overall algorithm in Algorithm 1 looks very
plausible as the result of such a derivation.  However, it is currently
impossible to verify the correctness of the algorithms, or indeed to use
them as the basis for developing extensions to the proposed work by other
authors in future (short of reverse-engineering the source code that was
promised by the authors).

While I am sympathetic to the authors' claim that there is insufficient
space for this information, a proper derivation of, at minimum, the
high-level meta algorithm in Algorithm 1, is more essential than e.g. Figure
2, which is interesting but not vital to the story, and also takes up a lot
of space.  While I realize that these derivations may be pretty standard,
there still needs to be a precise mathematical description of the method in
Algorithm 1, including what the algorithm assumes (fully factorized mean
field assumption, all complete conditionals of the crowdsourced annotation
submodel are in the exponential family?), what objective function it is
optimizing, how the objective function differs and remains a valid
variational algorithm in the case where the sequence tagging sub-algorithms
are non-Bayesian (presumably the algorithm is now justified as VBEM?), and
the high-level mathematical steps that one would take to derive the updates
of Algorithm 1 and its particular instantiations.


Minor comments:

There seems to be a large number of typos in the equations, e.g. in Section
2, almost all of which have some small issues.  If I am not mistaken:

Eqn 1, t_r = i should be t_r = j

Eqn 2, need to define \xi in the text (the spam distribution).

Below Eqn 3, shouldn't i=j be i \neq j?

Eqn 5, c_{i-1} should be c_{\tau-1}.  To make this notation more consistent,
after fixing this, write c_{\tau-1} = k (or some other letter), instead of
just c_{\tau-1}, as the other terms in this equation are written this way.

Eqn 6, d^{(s)}_{i, \tau} should be d^{(s)}_{n, \tau}.  The left hand side of
the equation needs to condition on everything that's conditioned on on the
right hand side, including A^{(s)} and d^{(s)}_{n, \tau - 1}.  Also, I'm
guessing that \phi_n should be \phi_{n, \tau}?  Otherwise, on the second
line of this equation, you seem to assert that you can compute the
probability of the entire sequence of tags \phi_n for the document given
just one of the labels, d^{(s)}_{n, \tau}.


If it's possible to make enough space, a table of notation would be
extremely helpful for the reader.

Enforce capitals in the references (Bayesian, Markov...) using {} in bibtex.

Section 4 paragraph 1, the claim that "each latent variable ... has a
variational distribution ... that is of the same form as its prior
distribution" - this is only guaranteed under the assumption that all of the
complete conditional distributions of the variables in the model are in the
exponential family. (It does hold for all the models in question, but the
current statement is not true in general.)

Page 2, "Nguyen et al (2017) uses only a simple model conditional
independence model of text features"

Section 6.1 first paragraph, need a space before (Rosenblatt, 1956)

Section 7, "that models the effect of label sequences": "that" should be
"which"

REVIEWER CONFIDENCE: 
	4. Quite sure. I tried to check the important points carefully. It's
unlikely, though conceivable, that I missed something that should affect my
ratings.

------------------------------------------------------

------------------------------------------------------
Reviewer B:

CLARITY: For the reasonably well-prepared reader, is it clear what was done
and why? Is the paper well-written and well-structured?: 
	2. Important questions were hard to resolve even with effort.


INNOVATIVENESS: How original is the approach? Does this paper break new
ground in topic, methodology, or content? How exciting and innovative is the
research it describes?

Note that a paper can score high for innovativeness even if its impact will
be limited.
: 
	3. Respectable: A nice research contribution that represents a notable
extension of prior approaches or methodologies.

SOUNDNESS/CORRECTNESS: First, is the technical approach sound and
well-chosen? Second, can one trust the claims of the paper -- are they
supported by proper experiments and are the results of the experiments
correctly interpreted?: 
	4. Generally solid work, although there are some aspects of the approach or
evaluation I am not sure about.


RELATED WORK: Does the submission make clear where the presented system sits
with respect to existing literature? Are the references adequate?

Note that the existing literature includes preprints, but in the case of
preprints:
• Authors should be informed of but not penalized for missing very recent
and/or not widely known work.
• If a refereed version exists, authors should cite it in addition to or
instead of the preprint.
: 
	5. Precise and complete comparison with related work. Benefits and
limitations are fully described and supported.


SUBSTANCE: Does this paper have enough substance (in terms of the amount of
work), or would it benefit from more ideas or analysis?

Note that papers or preprints appearing less than three months before a
paper is submitted to TACL are considered contemporaneous with the
submission. This relieves authors from the obligation to make detailed
comparisons that require additional experiments and/or in-depth analysis,
although authors should still cite and discuss contemporaneous work to the
degree feasible.
: 
	4. Represents an appropriate amount of work for a publication in this
journal. (most submissions)

IMPACT OF IDEAS OR RESULTS: How significant is the work described? If the
ideas are novel, will they also be useful or inspirational? If the results
are sound, are they also important? Does the paper bring new insights into
the nature of the problem?: 
	3. Interesting but not too influential. The work will be cited, but mainly
for comparison or as a source of minor contributions.

REPLICABILITY: Will members of the ACL community be able to reproduce or
verify the results in this paper?: 
	2. They would be hard pressed to reproduce the results: The contribution
depends on data that are simply not available outside the author's
institution or consortium and/or not enough details are provided.

IMPACT OF PROMISED SOFTWARE:  If the authors state (in anonymous fashion)
that their software will be available, what is the expected impact of the
software package?: 
	3. Potentially useful: Someone might find the new software useful for their
work.

IMPACT OF PROMISED DATASET(S): If the authors state (in anonymous fashion)
that datasets will be released, how valuable will they be to others?: 
	1. No usable datasets submitted.


TACL-WORTHY AS IS? In answering, think over all your scores above. If a
paper has some weaknesses, but you really got a lot out of it, feel free to
recommend it. If a paper is solid but you could live without it, let us know
that you're ambivalent.

Reviewers: after you save this review form, you'll have to make a
confidential recommendation to the editors via pull-down menu as to: what
degree of revision would be needed to make the submission eventually
TACL-worthy?
: 
	3. Ambivalent: OK but does not seem up to the standards of TACL.


Detailed Comments for the Authors

Reviewers, please draft your comments on your own filesystem and then copy
the results into the text-entry box.  You will thus have a saved copy in
case of system glitches.
: 
	The paper proposes a new method for learning a sequential tagger from
labeled data from likely unreliable annotators. The key idea is to model the
problem in a Bayesian inference framework consisting of a true label model
(BSC), annotator models (sequential confusion matrix), and a sequence
tagger. Extensive experiments have been conducted to demonstrate the
effectiveness of the proposed method.

Pros.
1.  The problem addressed is an important problem.
2.  The proposed method appears to be reasonable and sound.
3.  Experimental results have shown the superiority of the proposed method
compared with existing methods.
4.  Existing work seems to be well cited. 

Cons.
1.  The presentation needs a lot of improvements.  I feel that it is
difficult to understand the key points of the proposed method, as explained
below.
2.  The writing of the paper can be further improved. I will give some
examples.

Overall, I think that this is a good work.  However, I doubt that it has
reached the quality of TACL.

Detailed comments.

1.  Description of method.

I think that the authors should make a lot of more effort to make the key
technical points of the proposed method clearer.

It is fine to omit the detailed derivation, but it is necessary to explain
the general idea more clearly (sections 3 and 4).

Here I list several issues which let me feel confusing.  Currently I can
only guess what is being done in the proposed method.

* It would be better to give a definition of the general model.
* Hidden variable z appears suddenly without explanation on what is hidden
in the model.
* In Algorithm 1,  \hat{d} appears without a definition.
* "We divide the model into three modules: (a) the true label model, (b) the
annotator model, and (c) black-box sequence taggers.”  What is the model
to be divided?  “The true label model” appears suddenly.
* At line 310,  \iota and j represent current and previous labels,  and at
line 327, \iota and j represent previous and current labels.
* At line 381,  \phi_n is explained as sequence of text tokens,  at line
391, it says “features \phi_n”.  Very confusing.
* The description from line 360 to line 390 needs improvement. It is not
clear enough.

2.  Writing

The writing can be further improved.  There are typos, improper expressions,
etc.  Here I only give several examples.

* line 102,  a period is missing.
* line 116,  taxonomy, what do you mean by taxonomy?
* line 201,  "nor trust the predictions",  perhaps you want to say "do not
assume that the predictions are reliable".
* line 245,  "annotators ..."  the sentence is difficult to understand.
* line 255-256,  the sentence is difficult to understand.
* line 261,  "J x J"?
* line 402,  "the sequence tagger need not",  "the sequence tagger does not
need"?
* line 438,  "worker model" -> "annotator model",  to make it consistent
* line 456,  "graphical model" appears suddenly
* lines 403 and 568,  "Where ...".  I do not think that it is common to
start a sentence with "Where" in English.

REVIEWER CONFIDENCE: 
	3. Pretty sure, but there's a chance I missed something. Although I have a
good feel for this area in general, I did not carefully check the paper's
details, e.g., the math, experimental design, or novelty.

------------------------------------------------------

------------------------------------------------------
Reviewer C:

CLARITY: For the reasonably well-prepared reader, is it clear what was done
and why? Is the paper well-written and well-structured?: 
	1. Much of the paper is confusing.


INNOVATIVENESS: How original is the approach? Does this paper break new
ground in topic, methodology, or content? How exciting and innovative is the
research it describes?

Note that a paper can score high for innovativeness even if its impact will
be limited.
: 
	2. Pedestrian: Obvious, or a minor improvement on familiar techniques.

SOUNDNESS/CORRECTNESS: First, is the technical approach sound and
well-chosen? Second, can one trust the claims of the paper -- are they
supported by proper experiments and are the results of the experiments
correctly interpreted?: 
	2. Troublesome. There are some ideas worth salvaging here, but the work
should really have been done or evaluated differently.


RELATED WORK: Does the submission make clear where the presented system sits
with respect to existing literature? Are the references adequate?

Note that the existing literature includes preprints, but in the case of
preprints:
• Authors should be informed of but not penalized for missing very recent
and/or not widely known work.
• If a refereed version exists, authors should cite it in addition to or
instead of the preprint.
: 
	3. Bibliography and comparison are somewhat helpful, but it could be hard
for a reader to determine exactly how this work relates to previous work or
what its benefits and limitations are.


SUBSTANCE: Does this paper have enough substance (in terms of the amount of
work), or would it benefit from more ideas or analysis?

Note that papers or preprints appearing less than three months before a
paper is submitted to TACL are considered contemporaneous with the
submission. This relieves authors from the obligation to make detailed
comparisons that require additional experiments and/or in-depth analysis,
although authors should still cite and discuss contemporaneous work to the
degree feasible.
: 
	4. Represents an appropriate amount of work for a publication in this
journal. (most submissions)

IMPACT OF IDEAS OR RESULTS: How significant is the work described? If the
ideas are novel, will they also be useful or inspirational? If the results
are sound, are they also important? Does the paper bring new insights into
the nature of the problem?: 
	3. Interesting but not too influential. The work will be cited, but mainly
for comparison or as a source of minor contributions.

REPLICABILITY: Will members of the ACL community be able to reproduce or
verify the results in this paper?: 
	3. They could reproduce the results with some difficulty. The settings of
parameters are underspecified or subjectively determined, and/or the
training/evaluation data are not widely available.

IMPACT OF PROMISED SOFTWARE:  If the authors state (in anonymous fashion)
that their software will be available, what is the expected impact of the
software package?: 
	2. Documentary: The new software will be useful to study or replicate the
reported research, although for other purposes it may have limited interest
or limited usability. (Still a positive rating)

IMPACT OF PROMISED DATASET(S): If the authors state (in anonymous fashion)
that datasets will be released, how valuable will they be to others?: 
	1. No usable datasets submitted.


TACL-WORTHY AS IS? In answering, think over all your scores above. If a
paper has some weaknesses, but you really got a lot out of it, feel free to
recommend it. If a paper is solid but you could live without it, let us know
that you're ambivalent.

Reviewers: after you save this review form, you'll have to make a
confidential recommendation to the editors via pull-down menu as to: what
degree of revision would be needed to make the submission eventually
TACL-worthy?
: 
	2. Leaning against: I'd rather not see it appear in TACL.


Detailed Comments for the Authors

Reviewers, please draft your comments on your own filesystem and then copy
the results into the text-entry box.  You will thus have a saved copy in
case of system glitches.
: 
	This paper presents a method for aggregating labels obtained by
crowdworkers and deep learning models for sequence tagging. The key insight
is that previous work in this has looked at classification tasks, where each
instance is labeled independently from the others, which is not the case for
sequence labeling tasks, such as named entity recognition.

While the idea could be interesting, the paper has two main issues: the
method proposed is not well described  well and the experiments are not
designed in a way that allows for comparison against previous work. Thus it
is not possible to assess the contribution and novelty of the paper. In what
follows I discuss these issues more thoroughly.

- does the model require access to some gold labels in order to learn the
parameters? It seems like this would be the case since we need to estimate
conditional distributions given the true labels (lin 316-329) and there is a
true label model specified in section 5. If so there needs to be an
experimental setup with training/dev/test, not just dev and test. Also,
majority vote and MACE are unsupervised, which should be taken into account
in the analysis of the experiments.

- Variational methods, while they do not suffer from the ills of
sampling-based methods, they have issues of their own, one of them being the
choice of the approximate probability function to be learned instead of the
exact one. See here for a recent overview:
https://arxiv.org/pdf/1601.00670.pdf. However, there is no specification
here of the approximate posterior used.

- Also I couldn't follow equation 6. A joint distribution p(A,B) is often
rewritten when using VI as P(A)P(B|A). However, this doesn't seem to happen
there. What is the meaning of the second term on the right handside,
probability of the tokens given the sequence of labels and the parameters of
a tagger?

- Algorithm 1 is very confusing. The only input, c seems to be never used.
Also, what is the meaning of E(lnB) or E(lnA)?

- How is d_n^(s) marginalised? I would strongly encourage to show the full
equations for the model, it is the main contribution of the paper.

- The first set of experiments tests the model against data generated by the
model itself. While this is a useful sanity check, this is not a useful
comparison against other models.

- It seems that BSC (the proposed method) sometimes worsens the results when
combined with e.g. majority vote for PICO in F1. Why is this the case? On
the whole the results from the experiments seem to be inconclusive as there
is no comparison to highlight explicitly the benefit of the Bayesian
treatment of the model, or a comparison that isolates the effect of the
transitions considered in this model.

- Aggregating annotations to infer the gold labels vs training a model using
aggregated data to achieve good results is two substantially different ways
of evaluation and they should be done with different experimental setups.
Also the hyperparameters of the methods should be tuned on a dev set.

- Active learning for sequence labeling has been explored by Settles and
Craven (2008):
https://www.biostat.wisc.edu/~craven/papers/settles.emnlp08.pdf There are
different ways of performing uncertainty sampling for sequences, which one
was used?

- Given the focus on sequence tagging and training models on crowdsourced
data, a comparison against this paper:  Barbara Plank, Dirk Hovy, & Anders
Søgaard. 2014. Learning POS taggers with inter-annotator agreement loss.
EACL (Best Paper at EACL 2014):
http://www.anthology.aclweb.org/E/E14/E14-1078.pdf. Also, there is a recent
TACL paper providing an overview and comparison of models for annotation
aggregation
(http://www.dirkhovy.com/portfolio/papers/download/bma_draft.pdf) that might
be useful to the authors of this paper.

REVIEWER CONFIDENCE: 
	4. Quite sure. I tried to check the important points carefully. It's
unlikely, though conceivable, that I missed something that should affect my
ratings.