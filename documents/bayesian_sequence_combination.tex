\documentclass[11pt,a4paper]{article}
\usepackage{times}
\usepackage{latexsym}
\usepackage{url}
\usepackage[T1]{fontenc}

% anonymous version
%\usepackage[]{tacl2018v2}

%version with authors displayed
\usepackage[]{tacl2018v2}

%%%%
%%%% Material in this block can be removed by TACL authors.
% It consists of things specific to generating TACL instructions
\usepackage{xspace,mfirstuc,tabulary}

\newcommand{\ex}[1]{{\sf #1}}

\usepackage{graphicx}
\usepackage{tabularx}

\usepackage{array}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{Y}{>{\raggedleft\arraybackslash}X}
\newcolumntype{?}{!{\vrule width 1pt}}
\newcolumntype{h}{!{\vrule width 0.7pt}}

\usepackage[fleqn]{amsmath}
\usepackage{amsfonts}
\usepackage{algorithm2e}

\usepackage[boldmath]{numprint}
\usepackage{url}
\usepackage[caption=false,font=footnotesize]{subfig}
\usepackage{float}
\captionsetup[subfigure]{margin=0pt}
\usepackage{booktabs}
\usepackage{tikz}

\makeatletter
\makeatother %some sort of hack related to the symbol @

\newcommand{\bs}{\boldsymbol} 
\DeclareMathOperator*{\argmax}{\arg\!\max\!} %argmax operator
\newcommand{\wrtd}{\mathrm{d}}

\newcommand{\rough}[0]{\color{purple}}

\newcommand{\Arrow}[1]{\parbox{#1}{\tikz{\draw[->](0,0)--(#1,0)}} }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Bayesian Ensembles of Crowds and Deep Learners for Sequence Tagging}%Multiple Unreliable Text Annotators}

\author{Edwin Simpson and Iryna Gurevych\\
  Ubiquitous Knowledge Processing Lab \\
  Department of Computer Science \\
  Technische Universit\"at Darmstadt \\
  \url{https://www.ukp.tu-darmstadt.de} \\
  {\tt \{simpson,gurevych\}@ukp.informatik.tu-darmstadt.de}
}

\begin{document}

\maketitle

% up to eight (8) pages of content, plus unlimited pages for references
% Each EMNLP 2018 submission can be accompanied by a single PDF appendix <--put equations in here
% + one .tgz or .zip archive containing software, and one .tgz or .zip archive containing data
% make sure that these are also anonymized.

\begin{abstract}
Current methods for sequence tagging, a core task in NLP, are data hungry.
Crowdsourcing is a relatively cheap way to obtain labeled data, but
the annotators are unreliable.
%, so redundant labeling
%and aggregation techniques are required.
To address this, we develop
 a modular Bayesian method for aggregating sequence labels from multiple annotators
 and evaluate different models of annotator errors and labeling biases.
%and propose a technique to boost performance by integrating existing black-box sequence taggers.
%Typically, the process of data collection, aggregation and training a sequence tagger 
%is a pipeline of discrete steps.
% Our approach integrates 
% %these steps by training 
% black-box sequence taggers as components in the model
% to improve the quality of predictions.
We evaluate our model on crowdsourced data for named entity recognition and information extraction tasks,
showing that our sequential annotator model outperforms previous methods.
%particularly in small data scenarios that
%are encountered at the beginning of a crowdsourcing process.
%Our experiments show that by integrating the training and aggregation steps into a Bayesian framework, 
%we can use active learning techniques to obtain data more efficiently without modifying the black-box method.
%We publish our code to encourage adaptation and reuse.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{sections/intro}
\input{sections/annotator_models}
\input{sections/bsc_model}
\input{sections/inference}
\input{sections/method} % could be renamed. Contains commented out stuff and the software design stuff.
\input{sections/experiments}
\input{sections/discussion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\section*{Acknowledgments}

% \addcontentsline{toc}{chapter}{Bibliography}
%\bibliographystyle{apalike}
% \bibliographystyle{IEEEtran}
\bibliographystyle{acl_natbib}
\bibliography{simpson}

\appendix
\input{sections/appendix}

\end{document}
