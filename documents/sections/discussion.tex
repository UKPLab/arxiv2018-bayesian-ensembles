\section{Future Work}

The model can also be applied to other sequential classification tasks beside span annotation.
For example, the order of tasks that are intended to be exchangeable may affect the likelihood
of the labels provided by the annotators\cite{mathur2017}. Seq-BCC could be applied to model the 
propensity of the workers to choose certain labels given their previous labels, while the 
ground truth sequence may be ignored.

It may be possible to improve the performance of BLSTM by refining the model through attention layers. However,  similar refinements could also be applied to the Bayesian approach, 
although it remains to be seen whether more complex models would be suitable for scenarios with limited data. 
It may be possible to take advantage of neural network models in combination with a Bayesian approach
by learning argument embeddings in domains using neural networks trained in domains with sufficient data, then 
using them to embed arguments in new domains to produce input data for a GP. This could be more successful than
simply using word or sentence embeddings, as the embeddings would be tailored to the task of modelling 
arguments. At the same time, these embeddings would be at a low level so that the GP could learn an appropriate model over the target domain.

In our experiments we used a single type of kernel for each feature and combined the kernels using a simple product or sum function.
While this makes it feasible to include thousands of features, in future work we plan to investigate other 
ways to incorporate textual features, such as string kernels, which map strings of varying lengths to vectors 
and may be used to improve semantic representation of word embeddings\cite{lodhi_text_2002}.
