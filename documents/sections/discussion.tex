\section{Conclusions}

Previous work has demonstrated the benefits of modeling annotator reliability when aggregating noisy data, 
such as crowdsourced labels. 
We proposed a sequential annotator model for sequence tagging, BSC-Seq,
and showed how it improves the state-of-the-art. 
%To enable more efficient training data collection,
To further improve the quality of aggregated labels,
we design a Bayesian wrapper that allows developers to integrate existing sequence taggers, 
such as deep neural networks, into BSC-Seq while
treating them as black boxes.
Our results show that integrating an LSTM in this manner can outperform an LSTM trained 
using labels that were aggregated in a separate pre-processing step. 
However, for active learning from crowds or learning with small datasets, 
we find that our purely Bayesian aggregation method, i.e. BSC-Seq without integrating the LSTM,
outperforms both LSTM-based approaches. This hints at the value of uncertainty information
in text models when data-efficient learning is required. 

Future work will investigate the integration of approximate Bayesian variants of neural network
sequence taggers, which may address the need for uncertainty information during active learning.
We will also consider
%  improving robustness of the active learning process with very small datasets by investigating 
alternative data selection strategies, and how to include %reduce over-confidence in predictions by including
prior information about the reliability of black-box classifiers on a given training set size.

%how to adapt hyper-parameters of the NN automatically in low-resource state?

% % In future, BSC-Seq could be applied to other sequential classification tasks beside span annotation.
% For example, the order of tasks that are intended to be exchangeable may affect the likelihood
% of the labels provided by the annotators\cite{mathur2017}. Seq-BCC could be applied to model the 
% propensity of the workers to choose certain labels given their previous labels, while the 
% ground truth sequence may be ignored.
