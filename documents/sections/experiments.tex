
\section{Experiments with Simulated Annotators}\label{sec:synexpts}

%
%\subsection{Synthetic Data}
%
%We use synthetic data to illustrate the strengths and weaknesses of different methods by varying one independent
%dataset and tracking the performance metrics of each method:
%\begin{enumerate}
%  \item When do other methods outperform simple majority voting? Show performance against worker accuracy.
%  Similar experiments were carried out with a different set of baselines in \cite{Simpson2014Thesis}, Section 2.5.2, with all workers having similar accuracies, and Section 2.5.4, where some workers are noisy and others are highly accurate. Here, we vary the average accuracy of workers, with lower average accuracy leading to more diversity between workers. Analyse whether the full IBCC confusion matrix offers benefits over MACE due to worker accuracy varying between classes when accuracy is lower. 
%  \item How well do MACE, IBCC, and seq-BCC handle worker bias? Show performance against worker bias toward one class. This is a comparable experiment to \cite{Simpson2014Thesis}, Section 2.5.3, which notably does not include MACE.
%  \item How well does each method handle data sparsity? (a) Vary the amount of observations per worker. Expect MACE to perform well, seq-BCC may suffer from the larger confusion matrix. (b) Vary the number of observations per data point. 
%  \item Do MACE and IBCC still work with unbalanced datasets? Test with unbalanced class distributions, i.e. starting from $p(B) = p(I) = p(O) = .3$, decrease
%  $p(B)$ and $p(I)$ until $p(O) = .99$.
%\end{enumerate}
%In each case, we use a set of default values for the variables that are not currently being tested. These are
% chosen so that all methods perform well (e.g. 90\% accuracy) under the best conditions of the test. E.g. for the test where we vary worker accuracy, we set bias and sparsity to be low so that the performance of all methods is good when workers are 80\% accurate.
%
%For all methods except seq-BCC, we compute performance metrics both before and after the valid annotation post-processing step, which is required to ensure that I tokens do not follow O tokens.
%
%Performance metrics:
%\begin{enumerate}
%  \item Metrics that evaluate the quality of the most probable class labels: recall, precision and F1-score (of B class and I class separately), accuracy (mean over classes)
%  \item Metrics that evaluate the confidence values output by the models: area under ROC curve or AUC (separate for B and I classes), and cross entropy error %or brier score (mean over classes)
%  \item Annotation count error: the mean difference between the number of annotations produced by the model and the true number.
%  \item Number of invalid labels that must be corrected by post-processing.
%  \item Mean length of annotations compared to the ground truth: show whether some methods find annotation fragments.
%\end{enumerate}
%
%We also evaluate the competence scores estimated by each method. First, we compute the ground truth from the 
%synthetic confusion matrices. We use a weighted average over classes (weighted by class frequency) to produce the 
%overall worker accuracy. Then, we compute accuracy from seq-BCC, MACE and IBCC using similar methods. 
%For each method, we can compute the mean and STD of cross entropy error between the estimated and the ground truth
% confusion matrices. We then reproduce the plots described above, but showing the cross entropy error for the 
% competence estimates. 

%\subsection{Real-world Data}
%
%We investigate the performance on some real datasets to show how well the methods work when combining real workers. Besides the performance metrics mentioned above, we also quantitatively analyse examples of where seq-BCC outperforms other methods to show where they may have trouble forming valid or grammatically correct annotations, e.g. where the starting token is incorrect. 
%Ideally, we would also analyse whether the annotations produced are grammatically sensible, e.g. if there is conflict between workers about where an annotation should start, the method should choose one valid start point, not an invalid start point that lies in between. These points can be evaluated by computing:
%\begin{enumerate}
%\item The percentage of "sensible" annotations, as judged by an expert for the task. For tasks where this data is not available, we can use the following metrics to gauge annotation quality.
%\item The percentage of each method's annotations that have an exact match in the gold standard (exact annotation precision)
%\item The percentage of gold standard annotations that have an exact match in the method's output (exact annotation recall)
%\item Mean and variance of no. tokens difference to nearest annotation; this is averaged over the annotations, rather than the number of tokens, so gives a greater indication of how well they matched the gold standard.
%\end{enumerate}

We run several method comparisons using two NLP datasets to test whether the quality of aggregated labels is 
improved by (a) the more sophisticated worker models described in Section \ref{sec:model},
(b) the inclusion of text features into the graphical model or (c) a Bayesian approach. 
We further test whether Bayesian approach facilitates more efficient active learning of sequential annotations from crowds and whether integrating the LSTM into the ensemble of annotators improves performance further.
Our experiments consist of three tasks: (1) aggregating crowdsourced labels, (2) training the LSTM sequence tagger of Lample et al. ~\shortcite{lample2016neural} using aggregated labels, and (3) actively selecting batches of documents for crowdsourced annotation.
\begin{figure}
\centering
\subfloat[Annotator accuracy]{
  \includegraphics[width=0.9\columnwidth, clip=True, trim=10 15 40 38]{figures/synthetic/acc_bias_exp/plot_f1-score-tokens}
}\\
\subfloat[Short span bias]{
  \includegraphics[width=0.9\columnwidth, clip=True, trim=10 20 40 38]{figures/synthetic/short_bias_exp/plot_f1-score-tokens}
}\\
\subfloat[Missed span bias]{
  \includegraphics[width=0.9\columnwidth, clip=True, trim=10 20 40 38]{figures/synthetic/class_bias_exp/plot_f1-score-tokens}
}\\
% \subfloat[Crowd size]{
%   \includegraphics[width=0.2\textwidth, clip=True, trim=0 10 0 27]{figures/synthetic/acc_bias_exp/plot_f1-score.png}
% }
\subfloat[Good workers out of a crowd of 10, where the rest are random.]{
  \includegraphics[width=0.9\columnwidth, clip=True, trim=10 20 40 38]{figures/synthetic/group_ratio_exp/plot_f1-score-tokens}
}
\caption{F1 scores with simulated annotators. Each plot shows the effect of varying one characteristic.}
\label{fig:simulated}
\end{figure}
 
\section{Experiments with Real Data}\label{sec:expts}
 
\subsection{Datasets}

We use two datasets containing both crowdsourced sequential annotations and gold annotations. 
The \emph{NER} dataset contains $1,393$ English documents from the CoNLL 2003 named-entity recognition dataset
~\cite{tjong2003introduction}, all of which contain gold labels for four named entity categories (PER, LOC, ORG, MISC). Of these, we use crowdsourced labels provided by \cite{rodrigues2014sequence} for $415$ documents.
We also test on the \emph{PICO} dataset, introduced by Nguyen et al. ~\cite{nguyen2017aggregating},
containing $4,740$ medical paper abstracts, all of which have been 
annotated by a crowd to indicate text spans that identify the population enrolled in a clinical trial. There are gold labels for $191$ documents.
\begin{table*}
\small
\begin{tabular}{| l || r | r | r | r || l | r | r | r || r | r |} \hline
Dataset & Docs & Gold & Sentences & Tokens & Span type & Gold & \multicolumn{2}{c||}{Span length} & Workers & Workers \\
 & & docs &  & &  & spans & mean & std. & & /doc \\
\hline
NER & 1393 & 1393 & 6503 & 179323 & PER & 6282 & 1.19 & 0.49 & 47 & 4.9  \\
       & & & & & LOC  & 6482 & 1.73 & 0.57 & & \\
       & & & & & ORG  & 5789 & 1.55 & 0.92 & & \\
       & & & & & MISC & 3059 & 1.44 & 0.80 & & \\ \hline
PICO & 4740 & 191 & 9480 & 1424721 & trial population & 700 & 7.74 & 7.38 & 312 & 6.0  \\ \hline
\end{tabular}
\label{tab:datasets}
\caption{Numbers of documents, spans, annotators, tokens and sentences for our test datasets.}
\end{table*}

\subsection{Evaluation metrics}

For NER we use the established CoNLL 2003 F1-score, which is computed at the level of annotated spans that must match exactly to be considered correct. This measure is intuitive because complete named entities must be marked to be of value. For PICO, we use the relaxed F1-measure defined in ~\cite{nguyen2017aggregating}, which counts the matching fractions of spans when computing precision and recall. 
%We additionally compute the root mean squared error in the span lengths, i.e. the difference between the  % actually it's not quite that. We computed the difference in mean span lengths. This is already captured by F1 score for pico and better described by the span-level-precision and recall. Our metric might be more if we didn't take the absolute so we could see if spans were often too long or too short.
To evaluate the probabilities produced by each aggregation method, which may be useful for decision-making tasks such as active learning, we also compute the cross entropy error.

\subsection{Evaluated methods}

As well-established non-sequential baselines, we include token-level majority voting (\emph{MV}), \emph{MACE} \cite{hovy2013learning}, Dawid-Skene (\emph{DS}) ~\cite{dawid_maximum_1979}.  We also test independent Bayesian classifier combination (\emph{IBCC}) ~\cite{kim2012bayesian}, which can be seen as a Bayesian treatment of Dawid-Skene. 

Next, we test the sequential \emph{HMM-Crowd} method \cite{nguyen2017aggregating}. This method uses a mixture of maximum \emph{a posteriori} (or smoothed maximum likelihood) estimates for the worker model, and variational inference for the transition matrix and feature model. The worker model uses a simplification of the DS confusion matrix that models only the probability that a worker labels correctly given each true label class. HMM-Crowd is the current state-of-the-art and allows us to compare our approach against a model without a fully Bayesian treatment. 

We test our proposed method, Bayesian sequence classifier combination (BSCC) in several configurations. Firstly, with the different worker models described in Section \ref{sec:annomodels}.
% \begin{enumerate}
% \item accuracy model (\emph{BSCC-acc}): each worker is represented by a single parameter encoding $p( correct label )$
% \item Spammer model (\emph{BSCC-MACE}): proposed for MACE \cite{hovy2013learning}, each worker has a parameter encoding 
% $p( worker is spamming )$ and a set of $L$ parameters encoding $p( label | worker is spamming)$.
% \item Confusion matrix (\emph{BSCC-IBCC}): as in \cite{kim2012bayesian}, each worker has a matrix of parameters containing $p( label l | true class j)$.
% \item sequential confusion matrix (\emph{BSCC-seq}): as described in Section \ref{sec:model}, extends the confusion matrix using an HMM to model label transitions.
% \end{enumerate}
Secondly, integrating different text models:
\begin{enumerate}
\item Independent text features (the default setup): the probability of a token is independent of the sequence conditioned on the true label of the token. This is a standard emission model for an HMM.
\item No text features (\emph{notext}): only the crowdsourced labels are taken into account when labelling each token. Has the advantage of being task-independent and hence may be more suitable for cases where individual words are uninformative.
\item Integrated LSTM (\emph{+LSTM}): the LSTM is integrated into the variational inference loop as described in Section \ref{sec:model}.
\end{enumerate}

\subsection{Aggregating Crowdsourced Labels}\label{sec:task1}

In this task, we use the aggregation methods to combine crowdsourced labels and evaluate their outputs against the gold standard.
For NER, we split the $415$ crowd-labelled documents into 50\% validation and test sets as in Nguyen et al. ~\shortcite{nguyen2017aggregating}. We run the methods on crowd labels from all $415$ documents, then evaluate on either the
validation or test set.
For PICO, we also split the gold-labelled documents randomly into 50\% validation and test sets. However, in this case, we run the methods on all $4,740$ crowd-labelled documents. The results for this dataset are not directly comparable
with those of Nguyen et al. ~\shortcite{nguyen2017aggregating}, since their test and train splits were not available 
and they appear to have used a subset of the publicly-available dataset with on average 5 annotators per documents, rather than the 6 per document in the complete dataset.

% HMM Crowd -- report their method's results without bugfixes. Cannot say the improvement is purely down to Bayesian treatment though. Let's do this for now, then we can add the more direct comparison to a revised draft if accepted. OR include both, and label the modified HMM_Crowd differently to highlight our contribution here. But we lack the un-revised version for PICO -- look for older runs to see if we have it?

%Sentence separation -- why does it improve performance if we don't split docs into sentences?

% The discrepancy between the latest results and some previous ones, e.g. DS, BSCC-seq-IF with 
% same priors, BSCC-Vec-IF for NER? For BSCC, it may be due to different conf mat for the IF data model.
% For DS, I think it was still using the priors chosen for IBCC, so results changed when nu0 changed.

Note that the token-level F1-score can be skewed upwards by matching a few long spans correctly, but is useful for PICO because it shows up cases where the spans matched but the predictions were split, i.e. B is used instead of I. With non-strict entity matching, the precision and recall can be 100\% even though the prediction is split into multiple spans.
Token-level F1-score catches this because it penalises the erroneous B tokens. With strict entity-level F1-score, the matches must be exact, so split spans would receive no credit.

% Removed the data on no. invalid spans:
% 15, 81, 81, 92, 74, 68, 0.... BSC-MACE=91
% 0, 0, 80, 30, 54, 37, 0, 40, 0000000000000, BSC->LSTM 80
\begin{table*}
\small
\nprounddigits{1}
\npdecimalsign{.}
\begin{tabularx}{\textwidth}{| l | n{2}{1} | n{2}{1} | n{2}{1} | n{2}{1} || Y | Y  | Y  ?  n{2}{1} | n{2}{1} | n{2}{1} | n{2}{1} || Y  | Y  | Y |}
\hline
%NER & \multicolumn{3}{|l|}{Span-level metrics}                     & \multicolumn{2}{|l|}{Token-level metrics} & Hyper.\\ \hline 
& \multicolumn{4}{l||}{NER} & \multicolumn{3}{l||}{Hyperparam.} & \multicolumn{4}{l||}{PICO} & \multicolumn{3}{l|}{Hyperparam.} \\ \hline
& Prec. & Rec. & \text{F1} & \text{CEE} & $\nu_0$ & acc. bias & $\alpha_0$ & Prec. & Rec. & \text{F1} & \text{CEE} & $\nu_0$ & acc. bias & $\alpha_0$ \\ \hline

Best worker & 76.4 & 60.1 & 67.3 & %69.1 & .8521 & 
17.13  & & & &
64.8 & 53.2 & 58.5 & 17.03 & & & \\
Worst worker & 55.7 & 26.5 & 35.9 & %43.5 & .6924 & 
31.94  & & & & 
50.7 & 52.9 & 51.7 & 40.96 & & &\\ \hline

MV & 79.9 & 55.3 & 65.4 & %69.2 & .9422 & 
6.24  & & & & 82.5 & 52.8 & 64.3 & %76.4 & .923 &
 2.55  & & & \\ 
%MV$\rightarrow$LSTM & 81.2 & 58.7 & 68.1 & %71.0 & .8447 & 
%16.30 & 2 & \\ 
MACE & 74.4 & 66.0 & 70.0 & 1.01 &  .1 & .1 & 0  & 25.4 & 84.1 & 39.0 &% 44.3 & .840 &
 58.23 & .1 & .1 & 0 %72.5 & .8300 & 
\\ 
DS & 79.0 & 70.4 & 74.4 & %76.9 & .9516 & 
2.84 & & & & 71.3 & 66.3 & 68.7 &% \textbf{79.3} & .934 &
 0.44 & & & \\ 
IBCC & 79.0 & 70.4 & 74.4 & %77.1 & .9550 & 
{\npboldmath} 0.49 & .1 & 1 & .1 & 72.1 & 66.0 & 68.9 & %\textbf{79.3} & .935 & 
{\npboldmath} 0.27 & .1 & 10 & 10\\ 
%IBCC$\rightarrow$LSTM & 79.8 & 67.6 & 73.2 & %74.2 & .9040 & 
%14.01 & 86 & .1, 1, .1 \\ 
\hline

HMMcrowd & 80.5 & 69.4 & 74.6 & %77.0 & .9762 & 
1.04 & 0 & .1 & 0 & 76.5 & 66.2 & 71.0 & %77.9 & \textbf{.944} & 
0.79 & 0 & .1 & 0 \\ 
%HMM-crowd$\rightarrow$LSTMp & 81.3 & 68.6 & 74.4 & %76.1 & .9887 & 
%0.25 & 134 &  \\ 
"$\rightarrow$LSTM & 81.8 & 69.5 & 75.2 & %77.3 & .8972 & 
12.20 & 0 & .1 & 0 & 76.5 & 66.5 & 71.2 & %78.2 & .868 & 
12.94 & 0 & .1 & 0\\ 
\hline

BSC-acc & 83.4 & 54.3 & 65.7 & %68.2 & .9610 & 
0.96 & 10 & .1 & 10 & \textbf{89.4} & 45.2 & 60.0 & %74.5 & .9069 & 
1.59 & .1 & .1 & 10 \\ 
BSC-MACE & 67.9 & 74.1 & 70.9 & %71.6 & .9658 & 
0.89 & 10 & 10 & 1 & 46.7 & 84.4 & 60.1 & %68.5 & \textbf{.944} &
 1.98 & .1 & 100 & .1\\ 
BSC-CV & 81.4 & 64.7 & 72.1 & %75.3 & .9715 & 
0.89 & 10 & 1 & 1 & 74.9 & 67.2 & 71.1 & %77.2 & .936 &
 0.84 & .1 & 1 & .1\\ 
BSC-CM & 79.9 & 72.2 & 75.8 & %77.8 & .9635 & 
1.46 & .1 & 100 & .1 & 60.1 & 78.8 & 68.2 & %74.5 & .9434 & 
1.49 & .1 & 100 & 1 \\ 
BSC-seq & 80.3 & 74.8 & 77.4 & %78.9 & .9598 & 
0.65 & .1 & 1 & 1 & 
72.9 & 77.6 & 75.1 & %57.9 & .9250 & 
1.10 & 100 & 1 & 1\\ \hline

''-notext & 81.0 & 69.8 & 75.0 & %76.9 & .9420 & 
0.52 & .1 & 1 & 1 & 81.2 & 59.2 & 68.5 & %59.8 & .922 &
 0.73 & .1 & .1 & .1\\ \hline

%BSC-seq$\rightarrow$LSTMp & 80.3 & 74.6 & 77.3 & %78.6 & .9910 & 
%0.20 & 129 & .1, 1, 1 \\
"$\rightarrow$LSTM & 80.2 & 75.3 & 77.7 & %79.6 & .9262 & 
11.02 & .1 & 1 & 1 & 
75.7 & 75.4 & 75.5 & %51.6 & .821 & 
25.48 & 100 & 1 & 1 \\
%BSC-seq+LSTMp & 81.2 & 75.3 & 78.1 & %79.3 & .9574 & 
%0.54 & 0 & .1, 1, 1 \\
"+LSTM & \textbf{82.3} & \textbf{75.9} & \textbf{78.9} & %79.6 & .9513 & 
0.59 & .1 & 1 & 1 & 
78.7 & \textbf{78.6} & \textbf{78.7} & %51.9 & .934 & 
1.15 & 100 & 1 & 1 \\
\hline
\end{tabularx}
\caption{NER dataset: estimating true labels for documents that have been labelled by the crowd.}
\label{tab:aggregation_results_ner}
\npnoround
\end{table*}

\begin{table*}
\small
\begin{tabularx}{\textwidth}{| l | X | X | X | X | X | X | X | X | X | X | X | X |}
\hline
Method & Data-set & exact match & type wrong only & partial match & mis-sing span & not a span & late start & early start & late finish & early finish & fused spans & split span \\ \hline
MV & NER & 4307 & 304 & 228 & 1773 & 100 & 96 & 10 & 15 & 85 & 17 & 26 \\
HMM-Crowd & NER & 4519 & 361 & 256 & 924 & 182 & 101 & 15 & 26 & 97 & 28 & 22 \\
BSC-CV & NER & 4431 & 275 & 243 & 1245 & 177 & 100 & 17 & 23 & 89 & 29 & 16 \\
BSC-CM & NER & 4534 & 387 & 258 & 734 & 269 & 111 & 23 & 37 & 86 & 39 & 12 \\
BSC-seq+LSTM & NER & 4581 & 351 & 261 & 564 & 195 & 93 & 42 & 33 & 85 & 39 & 17 \\
\hline
MV & PICO    & 168 & 0 & 32 & 185 & 48 & 9 & 11 & 1 & 0 & 3 & 9 \\
HMM-Crowd    & PICO & 190 & 0 & 47 & 124 & 81 & 13 & 21 & 0 & 0 & 5 & 8 \\
BSC-CV       & PICO & 196 & 0 & 46 & 117 & 81 & 10 & 25 & 0 & 0 & 11 & 0 \\
BSC-CM       & PICO & 203 & 0 & 54 & 77 & 192 & 18 & 15 & 8 & 0 & 4 & 18 \\
BSC-seq+LSTM & PICO & 81 & 0 & 421 & 75 & 216 & 20 & 6 & 232 & 3 & 24 & 393 \\
\hline
\end{tabularx}
\caption{Counts of different types of span errors.}
\label{tab:error_analysis}
\end{table*}
To get a deeper understanding of our proposed method, we examine the types of errors it makes
in comparison to the previous state-of-the-art, HMM-Crowd, and the majority vote baseline.
Table \ref{tab:error_analysis} lists counts for different types of span annotation error for these
three methods on both
NER and PICO datasets. Compared to HMM-Crowd, BSC-Seq reduces errors of type ..., which is ... but
increases errors of type ..., which is ...
This corroborates with the higher precision or recall? in tables ...
The majority baseline suffers from a large number of type ... errors.

\begin{figure*}
\centering
% \subfloat[BSCC-acc-IF]{
%   \includegraphics[width=0.9\textwidth, clip=True, trim=0 10 0 27]{figures/worker_models/acc}
% } \\
\subfloat[BSCC-Vec-IF]{
  \includegraphics[width=0.7\textwidth, clip=True, trim=0 10 0 27]{figures/worker_models/vec}
} \\
% \subfloat[BSCC-MACE-IF]{
%   \includegraphics[width=1\textwidth, clip=True, trim=0 10 0 27]{figures/worker_models/mace}
% } \\
\subfloat[BSCC-IBCC-IF]{
  \includegraphics[width=0.7\textwidth, clip=True, trim=0 10 0 27]{figures/worker_models/ibcc}
} \\
\subfloat[BSCC-seq-IF, previous label = I]{
  \includegraphics[width=0.7\textwidth, clip=True, trim=0 10 0 27]{figures/worker_models/seq_prev0}
} \\
\subfloat[BSCC-seq-IF, previous label = O]{
  \includegraphics[width=0.7\textwidth, clip=True, trim=0 10 0 27]{figures/worker_models/seq_prev1}
} \\
\subfloat[BSCC-seq-IF, previous label = B]{
  \includegraphics[width=0.7\textwidth, clip=True, trim=0 10 0 27]{figures/worker_models/seq_prev2}
} \\
\caption{Clusters of confusion matrix representations from each BSC-*** annotator model trained on PICO. 
% Show how worker representation benefits from richer model: e.g. show differences between rows in IBCC compared to acc.
% Represent all types as IBCC-confusion matrix plots. seq will need more >= 3 plots! We can focus on PICO data to make it easier to view,
% or combiner NER classes into BIO. 
% If each row corresponds to one model, we have 7 rows (2 extra for seq). 
% Each row can then show either a selection of 5 workers, or we can cluster into 5 groups.
}
\label{fig:conf_mat_clusters}
\end{figure*}
The results show a benefit to using a sequential annotator model over CM, CV and Acc.
To understand how BSC uses the richer model in practice, we plot learned the annotator models for
PICO as probabilistic confusion matrices in Figure \ref{fig:conf_mat_clusters}.
To enable us to visualize the large number of annotator models, we clustered them into five groups by
applying K-means  to their expected values.
...

%\begin{figure}
%\centering
%\subfloat[prev. = I]{
%  \includegraphics[width=.31\columnwidth, clip=True, trim=20 47 10 25]{figures/worker_models/seq+LSTM_prev0}
%}
%\subfloat[prev. = O]{
%  \includegraphics[width=.31\columnwidth, clip=True, trim=20 47 10 25]{figures/worker_models/seq+LSTM_prev1}
%}
%\subfloat[prev. = B]{
%  \includegraphics[width=.31\columnwidth, clip=True, trim=20 47 10 25]{figures/worker_models/seq+LSTM_prev2}
%}
%\caption{PICO dataset: confusion matrices inferred by BSC-Seq-IF+LSTM for the integrated LSTM. }
%\label{fig:conf_mat_lstm}
%\end{figure}

% \subsubsection{Examples of Aggregation}
%
% \begin{figure}
% \centering
% % \subfloat[Example 1]{
% %   \includegraphics[width=1\columnwidth, clip=True, trim=20 47 10 25]{figures/placeholder}
% % }\\
% % \subfloat[Example 2]{
% %   \includegraphics[width=1\columnwidth, clip=True, trim=20 47 10 25]{figures/placeholder}
% % }\\
% % \subfloat[Example 3]{
% %   \includegraphics[width=1\columnwidth, clip=True, trim=20 47 10 25]{figures/placeholder}
% % }
% \caption{Examples of different handling of annotator disagreement on PICO. 
% Lines above the text show the crowd's annotations. Lines below show the aggregated annotations from MV, IBCC, HMM-Crowd and BSCC-seq-IF.
% The sequential methods are able to resolve some issues, 
% while non-sequential methods can lead to invalid annotations. }
% \label{fig:disagreements}
% \end{figure}

\subsection{Active Document Selection}

We run an active learning simulation to evaluate whether the proposed Bayesian approach and integrated LSTM
can improve the efficiency of a crowdsourcing process. 
The simulation is run separately for each method tested, and begins with the same initial set of randomly-chosen
documents taken from the same crowd-labelled sets used in Section \ref{sec:task1}.
We retrieve the crowdsourced labels for the selected documents, run the aggregation method,
then use its posterior probabilities to select a new batch of the $N_{batchsize}$ most uncertain documents that have not yet been labelled. 
We retrieve the annotations for the selected batch of documents, then repeat the process until
all of the available crowd labels have been used.
We set $N_{batchsize}$ to one tenth of the crowd-labelled dataset size for each of the datasets. At each iteration,
we monitor progress by training an LSTM on the current output of the aggregation method, 
and testing its performance as in Section \ref{sec:task1}. 
With the NER dataset we also evaluate the output of aggregation method on the test set for the crowd-labelled documents. 
This is not possible with PICO datas because we do not have gold labels for documents labelled by the crowd.

The active learning process tested here employs \emph{uncertainty sampling}, which is a 
well-established heuristic~\cite{settles2010active}. 
The selection method and batch size could be fine-tuned for future applications -- the 
goal of our experiment in this paper was to test the benefits of the proposed aggregation methods,
rather than to establish a robust active learning approach.

\begin{figure}
\centering
\subfloat[Random sampling]{
  \includegraphics[width=0.9\columnwidth, clip=True, trim=0 0 0 0]{figures/NER_RAND/pool/plot_f1-score-spans-strict}
}\\
\subfloat[Active learning simulation]{
  \includegraphics[width=0.9\columnwidth, clip=True, trim=0 0 0 0]{figures/NER_AL/pool/plot_f1-score-spans-strict}
}
\caption{Small data subsamples from NER: increasing span-level F1-score.
%: prediction performance after each labelled batch is received. Mean scores over 10 repeats.
}
\label{fig:al}
\end{figure} 
% \begin{figure}
% \centering
% \subfloat[Random sampling]{
%   \includegraphics[width=0.8\columnwidth, clip=True, trim=35 0 40 38.4]{figures/PICO_RAND/pool/plot_f1-score-spans-relaxed}
% }\\
% \subfloat[Active learning simulation]{
%   \includegraphics[width=0.8\columnwidth, clip=True, trim=35 0 40 38.4]{figures/PICO_AL/pool/plot_f1-score-spans-relaxed}
% }
% \caption{Small data subsamples from PICO: increasing span-level F1-score.
% %: prediction performance after each labelled batch is received. Mean scores over 10 repeats.
% }
% \label{fig:rand}
% \end{figure}

\subsection{Prediction using an LSTM Trained by the Crowd}\label{sec:task2}

In this task, we use the aggregation methods to train an LSTM sequence tagger \cite{lample2016neural}
to show whether integrating the LSTM with the aggregation method improves performance.
For the NER dataset, we train the aggregation methods on the $415$ crowd-labelled documents, as before,
then use the outputs to train the LSTM. We then evaluate the LSTM on the validation and test sets
in the original CoNLL dataset.
With the PICO dataset, we run the aggregators on the $3,649$ documents without gold labels, 
use the outputs to train the LSTM, then evaluate the LSTM on the validation and test splits from the gold-labelled data.

\begin{table*}
\small
\begin{tabularx}{\textwidth}{| l | Y | Y | Y | Y | Y || Y | Y | Y | Y | Y |}
\hline
%PICO & \multicolumn{3}{|l|}{Span-level metrics (std.)}                          & \multicolumn{2}{|l|}{Token-level metrics (std.)} \\ \hline 
 & \multicolumn{5}{l||}{NER} & \multicolumn{5}{l|}{PICO}\\ \hline 
& Prec. & Recall & F1 & CEE & $N_{inv}$ & Prec. & Recall & F1 & CEE & $N_{inv}$ \\ \hline
HMM-Crowd$\rightarrow$LSTMd & \textbf{78.7} & 59.0 & 67.5 &% 69.4 & .8642 &
 15.88 & 0 & \textbf{75.6} & 61.6 & 67.9 & % \textbf{76.4} & .8380 &
13.46 & 0\\ \hline
BSC-seq$\rightarrow$LSTMd & 74.3 & \textbf{62.8} & \textbf{68.1} & %69.7 & .8868 & 
15.65 & 0 & 82.3 & \textbf{66.4} & \textbf{73.5} & %58.2 & .8349 &
 19.62 & 0 \\
BSC-seq+LSTM & 73.4 & 62.0 & 67.3 &% 67.6 & .9589 &
 0.48 & 0 & \textbf{87.4} & 57.9 & 69.7 & \textbf{0.93} & 0\\%60.7 & 52.8 & 56.4 & 54.0 & \textbf{.899} & \textbf{0.48} & 0\\
%BCC-seq+LSTM &\\
\hline
gold$\rightarrow$LSTMd & 76.4 & 77.0 & 76.7 & %77.3 & .9563 & 
11.10 & 3 & & & & & \\
\hline
\end{tabularx}
\caption{Prediction performance on test datasets with training on crowdsourced labels.}
\label{tab:prediction_results_pico}
\end{table*}

\begin{table}
\small
\begin{tabularx}{\columnwidth}{|l | Y | Y | Y |} \hline
No. Tokens & I & O & B \\ \hline
1486 & 0.22	& 0.978	& 0.648 \\
14860 & 0.502	& 0.819	& 0.612 \\
29704 & 0.695	& 0.539	& 0.533 \\ \hline
\end{tabularx}
\caption{Accuracies for the integrated LSTM estimated by BSC-Seq+LSTM for NER.}
\label{tab:lstm_accs}
\end{table}

\subsection{Discussion}

The benefits of sequential models are more evident on the PICO dataset than on NER, which may be due to the longer sequences or the smaller number of labels, since PICO target classes are only B, I, or O, whereas the B and I tags for NER 
are compounded with PER, LOC, ORG or MISC tags. <show an example from each dataset, with our predictions from HMM, BAC...>
