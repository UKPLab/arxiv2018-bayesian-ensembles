\section{Inference using Variational Bayes} \label{sec:vb}
 
%TODOn't: I think we should include the lower bound to state it is the objective function. Don't do this because we need to cut maths out.
%TODO equation 23 could be one line?
%TODO equation 24 on one line?
%TODO can we replace eq 12 with just a E p(c | t) term?
%TODO simplify the description of the forward/backward passes
%TODO cut eq 21, don't give the equation for CM, just replace with a statement about simplifying equations 19 and 20 for the other models. 
%TODO equations 19 and 22 can be on one line if we change equation 20 to include 
% the prior pseudocounts.
%TODO do we need the variational factor equations or just the updates? E.g. we cut out equation 10, 17, 19 and possibly 23 completely. Then we may keep equation 12 to help with reproducibility.
 
Given a set of annotations, $\bs c=\{\bs c^{(1)}, .., \bs c^{(K)} \}$ from $K$ annotators,
our aim is to obtain a posterior distribution over 
%the parameters, 
%$\bs T$, $\bs\theta = \{ \bs\theta^{(1)},..,\bs\theta^{(S)} \}$, 
%$\bs A=\{ A^{(1)},..,A^{(K)} \}$ and
%$\bs B = \{ B^{(1)},..,B^{(S)} \}$, 
%and thereby compute the posterior over 
sequence labels, $\bs t$.
To do this, we employ
\emph{variational Bayes} (\emph{VB})~\cite{attias_advances_2000}.
In comparison to other Bayesian approaches such as Markov chain Monte Carlo (MCMC),
VB is often faster, readily allows incremental learning, and provides easier ways
to determine convergence~\cite{bishop_pattern_2007}. 
Unlike maximum likelihood methods such as standard expectation maximization (EM),
VB considers prior distributions 
and accounts for parameter uncertainty in a Bayesian manner.
The trade-off is that VB requires us to approximate the posterior distribution: 
%over $\bs t$ and the model parameters, 
%$\bs T$, $\bs\theta = \{ \bs\theta^{(1)},..,\bs\theta^{(S)} \}$, 
%$\bs A=\{ A^{(1)},..,A^{(K)} \}$ and
%$\bs B = \{ B^{(1)},..,B^{(S)} \}$.
%The labels produced by the sequence taggers, $\bs d$, 
%can be marginalized, so do not appear in the approximate posterior, which is given by:
\begin{flalign} \label{eq:vb_posterior}
& p(\bs t, \bs A, \bs B, \bs T, \bs\theta | \bs c, \bs x, \bs \alpha, \bs \beta, \bs\gamma )  \approx \prod_{k=1}^K  q(A^{(k)}) &   \nonumber\\
& \prod_{j=1}^J q(\bs T_j) \prod_{n=1}^N q(\bs t_n) 
\prod_{s=1}^S\Big\{ q(B^{(s)})  q(\bs\theta^{(s)}) \Big\}. & % q(\bs d^{(s)} )
\end{flalign}

% We optimise this distribution using Algorithm \ref{al:vb_bac} to obtain approximate posterior
% distributions over $\bs t$, $\bs\pi^{(k)}, \forall k$ and $A_j, \forall j$.
% The algorithm iteratively increases the lower bound on the model evidence, $\mathcal{L}$, 
% by optimising one variational factor given the current estimates of the others.  
% Convergence can be checked cheaply by comparing values of $\mathbb{E}[t_{i,\tau}]$ between iterations. 
% However, a more reliable method is to check $\mathcal{L}$ for convergence. 
%
% Depending on the implementation of the text classifier, the training step of the text classifier 
% may correspond to the maximisation step in an EM-algorithm, 
% if the parameters of the classifier, $\theta_v$, are optimised to their maximum likelihood 
% or maximum a-posteriori solution, as is typical of neural network methods.
% In this case, our complete algorithm would incorporate a non-Bayesian text classification step.
% In contrast, a Bayesian classifier integrates out the parameters $\theta_v$ and outputs
% marginal probabilities over class labels. If a Bayesian classifier is integrated, 
% retraining the text classifier becomes a VB step, in which a variational factor, $q(\bs c^{(b)}$, is updated,
% making the complete algorithm a fully Bayesian approximation.
The variational approximation factorizes between subsets of parameters or latent variables, so that each subset, $z$, has a \emph{variational factor}, $q(z)$. 
The labels produced by the sequence taggers, $\bs d$, have been marginalized.
Due to our choice of conjugate priors, the variational factors for BSC are of the same types
as their prior distributions defined in Section \ref{sec:model},
and the parameters of each variational distribution can be computed in terms  of 
expectations over the other subsets of variables.
We perform approximate inference,
%we optimize Equation \ref{eq:vb_posterior} 
using coordinate ascent to update each variational factor, $q(z)$, in turn,
taking expectations with respect to the current estimates of the other variational factors.
Our VB approximation therefore makes use of the \emph{mean field} assumption.
%(see Algorithm \ref{al:vb_bac}).
Each iteration reduces the KL-divergence between the true and approximate posteriors
of Equation \ref{eq:vb_posterior}, and hence optimizes a lower bound on the 
log marginal likelihood, also called the evidence lower bound or ELBO
~\cite{bishop_pattern_2007,attias_advances_2000}.
The complete VB algorithm is described in Algorithm \ref{al:vb_bac},
which makes use of the update equations for the log variational factors given below.
% We now provide the variational factors,
% which can be used to approximate the marginal posterior distributions for the parameters and sequence
% labels,
% and explain how to incorporate existing sequence taggers into the algorithm.
%that
%predict only sequence labels rather than full posterior distributions.
\begin{algorithm}[t]
\DontPrintSemicolon
 \KwIn{ Annotations, $\bs c$}
 \nl Randomly initialise $\mathbb{E}\ln A^{(k)},\forall k$,
 $\mathbb{E}\ln B^{(s)},\forall s$, $\mathbb{E}\ln \bs T_j,\forall j$ 
 and $\hat{d}^{(s)}_{n,\tau}(i), \forall s,\forall n,\forall \tau,\forall i$.\;
%  randomly or to expectations with respect to the priors\;
 \While{$\mathrm{not\_converged}(r_{n,\tau,j},\forall n,\forall \tau, \forall j)$}
 {
 \nl Update $r_{j,n,\tau}$, $s_{t_{j,n,\tau\!-\!1}, t_{\iota,n,\tau}}$, $\forall j,\forall \tau,\forall i,\forall \iota$,
 %given $\bs c$, 
 %$\mathbb{E}\left[\ln \bs A \right]$, $\mathbb{E}\left[\ln \bs B \right]$ and $\mathbb{E}\left[\ln \bs T \right]$
 using forward-backward algorithm\;%~\cite{ghahramani2001introduction}\;
 %\nl Set current true label predictions $\mathbb{E}\left[t_{n,\tau}=j\right] = r_{j,n,\tau}$ \;
 \nl Retrain all sequence taggers using $\tilde{\bs d}^{(s)}$ as training labels for tagger $s$\;
 \nl Use sequence taggers to predict $\hat{d}^{(s)}_{n,\tau}(i)$, $\forall s,\forall n,\forall \tau,\forall i$\;
 \nl Update $\ln q(A^{(k)})$ and $\mathbb{E}\ln A^{(k)},\forall k$ \;%given current $\bs c$, $r_{j,n,\tau}$\;
 \nl Update $\ln q(B^{(s)})$ and $\mathbb{E}\ln B^{(s)},\forall s$ \;%given current $\hat{\bs d}$, $r_{j,n,\tau}$\;
 \nl Update $\ln q(\bs T_j)$ and $\mathbb{E}\ln T_{j,\iota},\forall j,\forall \iota$ \;%given current $s_{t_{j,n,\tau\!-\!1}, t_{\iota,n,\tau}}$\;
 }
\KwOut{ Label posteriors, $r_{n,\tau,j},\forall n,\forall \tau, \forall j$, %\mathbb{E}[\bs t]$, 
most probable sequence of labels, $\hat{\bs t}_n,\forall n$ using Viterbi algorithm }
\;
\caption{The VB algorithm for BSC.}
\label{al:vb_bac}
\end{algorithm}

For the true labels, $\bs t$, the variational factor is:
 \begin{flalign}
& \ln q(\bs t_n) \!=\! 
\sum_{n=1}^N \sum_{\tau=1}^{L_n} 
\sum_{s=1}^S \mathbb{E}%_{\bs B,\bs d^{(s)}} \!\left[ 
\ln \!B^{(s)}\!\!\left(t_{n,\tau},d_{n,\tau}^{(s)},d_{n,\tau\!-\!1}^{(s)}\!\right) %\right]
%\bigg\{ \mathbb{E}%_{\bs T} \left[ 
%\ln T_{t_{n,\tau\!-\!1}, t_{n,\tau}} %\right] 
&&\nonumber \\
& + \sum_{n=1}^N \sum_{\tau=1}^{L_n} \sum_{k=1}^K  \mathbb{E}%_{\bs A} \left[ 
\ln \!A^{(k)}\left(t_{n,\tau},c_{n,\tau}^{(k)},c_{n,\tau-1}^{(k)}\right) %\right]  
&\nonumber\\
&+ \mathbb{E}\ln T_{t_{n,\tau-1}, t_{n,\tau}}+\mathrm{const}. & \label{eq:qstar_t}
 \end{flalign}
From this factor, we compute the posterior probability of each true token label, 
$r_{n,\tau,j}= \mathbb{E}[p(t_{n,\tau}=j | \bs c)]$,  %_{\bs T,\bs A,\bs B,\bs d}
and of each label transition, $s_{n,\tau,j,\iota} = 
\mathbb{E}%_{\bs T,\bs A,\bs B,\bs d}
[p(t_{n,\tau\!-\!1}=j, t_{n,\tau}=\iota | \bs c)]$,
%
% terms provide the output predictions of the class labels. 
% To compute $q(\bs T_j)$, $q(\bs A^{(k)} )$, and $q(\bs B^{(s)})$, 
% we require expectations for the individual 
% true labels $r_{n,\tau,j} 
using the forward-backward algorithm~\cite{ghahramani2001introduction},
 which consists of two passes. 
 The \emph{forward pass} for each document, $n$, starts from $\tau=1$
 and computes:% for each value of $\tau$:
 % the posterior given crowdsourced annotations for tokens $\leq\tau$. 
 \begin{flalign}
   & \ln r^{-}_{n,\tau,j} = \ln \sum_{\iota=1}^J \left\{ r^{-}_{n,\tau-1,\iota} \mathrm{e}^{\mathbb{E}\ln T_{\iota,j}} \right\} + ll_{n,\tau}(j), & \nonumber \\
 \end{flalign}
%  where the log likelihood $ll(j,n,\tau)$ of the annotations for token $\tau$ in document $n$ given
%  label $j$ is:
 \begin{flalign} 
   & ll_{n,\tau}(j) = \sum_{k=1}^K \mathbb{E}%_{\bs A}
   \ln A^{(k)}\left(j, c_{n,\tau}^{(k)}, c_{n,\tau\!-\!1}^{(k)} \right) +  \sum_{s=1}^S
   & \nonumber \\
   &  \sum_{i=1}^J\sum_{\iota=1}^J \mathbb{E}%_{\bs B}
   \ln B^{(s)} \!\left(j, i, \iota \right)  
   \hat{d}_{n,\tau}^{(s)}(i) \hat{d}_{n,\tau-1}^{(s)}(\iota), & 
 \end{flalign}
 where $\hat{d}_{n,\tau}^{(s)}(i)$ is %the probability of label $d_{n,\tau}^{(s)}$ produced 
% by the sequence tagger $s$, which we explain in more detail below (see Equation \ref{eq:hatp}).
defined below in Equation \ref{eq:hatp}, and $r^{-}_{n,0,\iota}  = 1$ where $\iota=$`O' and $0$ otherwise.
The \emph{backwards pass} starts from $\tau=L_n$ and scrolls backwards, computing:
%at each token computing the likelihoods of the annotations from $\tau+1$ to $L_n$:
 \begin{flalign}
  & \ln \lambda_{n,L_n,j} = 0, \hspace{1cm}
   \ln \lambda_{n,\tau,j} = \ln\sum_{\iota=1}^J \exp \big\{ 
   & \nonumber \\
& \ln \lambda_{i,\tau+1,\iota} + \mathbb{E}\ln T_{j,\iota} + ll_{n,\tau+1}(\iota) \big\} .&
 \end{flalign}
% To avoid $r^{-}_{n,\tau,j}$ and $\lambda_{n,\tau,j}$ becoming too small over a long sequence, we normalize them after each iteration of the forward and backward pass
% by dividing by their sum over $j$.
 By %taking the exponents and 
 applying Bayes' rule, we arrive at $r_{n,\tau,j}$ and $s_{n,\tau,j,\iota}$:
 \begin{flalign}
  & r_{n,\tau,j} = \frac{r^{-}_{n,\tau,j}\lambda_{n,\tau,j}}{\sum_{j'=1}^J r^{-}_{n,\tau,j'}\lambda_{n,\tau,j'}} &\\
%} {\sum_{\iota=1}^J \sum_{\iota'=1}^J  
%  r^{-}_{n,\tau\!-\!1,\iota}\lambda_{n,\tau,\iota'} \exp\mathbb{E}[\ln T_{\iota,\iota'}] 
% + ll(\iota',n,\tau)  } . &
& s_{n,\tau,j,\iota} = \frac{ \tilde{s}_{n,\tau,j,\iota} }{ \sum_{j'=1}^J\sum_{\iota'=1}^J  \tilde{s}_{n,\tau,j',\iota'} } & \\
  & \tilde{s}_{n,\tau,j,\iota} =  r^{-}_{n,\tau-1,j} \lambda_{n,\tau,\iota} \exp\{\mathbb{E}\ln T_{j,\iota}
+ ll_{n,\tau}(\iota)\}. & \nonumber 
 \end{flalign}

%\textbf{Variational factor for} $\bs T$: e
Each row of the transition matrix has the factor:
\begin{flalign}
& \ln q(\bs T_{j}) 
  %= \sum_{\iota=1}^J N_{j,\iota}  + \ln \mathrm{Dir}(\bs T_j | \bs\gamma_j) + \mathrm{const} & \nonumber\\
= \ln \mathrm{Dir}\left(\left[ N_{j,\iota} + \gamma_{j,\iota}, \forall \iota \in \{1,..,J\} \right]\right), &
\end{flalign}
where $N_{j,\iota} = \sum_{n=1}^N \sum_{\tau=1}^{L_n}  s_{n,\tau,j,\iota}$ is the expected number of times that label $\iota$ follows label $j$.  
%The variational factor $q(\bs t)$ requires the following expectation:
The forward-backward algorithm requires the terms:
 \begin{flalign}
& \mathbb{E}\ln T_{j,\iota} = \Psi\!\left(N_{j,\iota} \!\!+ \gamma_{j,\iota}\right) 
 - \Psi\!\left(\sum_{\iota=1}^J (N_{j,\iota} \!\!+ \gamma_{j,\iota}) \!\right), &
\end{flalign}
 where $\Psi$ is the digamma function.
 
%\textbf{Variational factors for} $\bs A$ and $\bs B$:
The variational factor for each annotator model is a distribution over its parameters, 
which differs between models.
For \emph{seq}, the variational factor is:
 \begin{flalign}
  & \ln q\!\left(\! A^{(k)}\!\right) %= \sum_{j=1}^J  \sum_{l=1}^J \bigg\{ \sum_{m=1}^J N_{j,l,m}^{(k)}\ln\pi_{j,l,m}^{(k)} & \nonumber\\
 % & \hspace{2.7cm} 
 % + \ln p\left(\bs\pi_{j,l}^{(k)} | \bs \alpha_{j,l}^{(k)} \right) \bigg\} + \mathrm{const}, & \nonumber \\
  \!=\! \sum_{j=1}^J \! \sum_{l=1}^J \!\mathrm{Dir}\! \left(\left[ \bs N_{j,l,m}^{(k)} \! 
 %+ \alpha_{j,l,m}^{(k)}, \! 
 %\right.\right.\nonumber&\\
%&  \hspace{3.5cm} \left.\left.
\forall m \! \in \! \{1,..,J\} \!\right] \right) & \nonumber \\
& N^{(k)}_{j,l,m} \!\!=\!  \alpha_{j,l,m}^{(k)} \!\!\! + \!\sum_{n=1}^N \!\sum_{\tau=1}^{L_n} \!
r_{n,\tau,j} \delta_{l,c^{(k)}_{n,\tau\!-\!1}}\!\delta_{m,c^{(k)}_{n, \!\tau}}, \!& 
\end{flalign}
 where $\delta$ is the Kronecker delta. 
% For the \emph{CM} model, the variational factor is simplified to:
%  \begin{flalign}
%   & \ln q\left( A^{(k)}\right) = \sum_{j=1}^J  \mathrm{Dir} \bigg( \bigg[ \sum_{n=1}^N \sum_{\tau=1}^{L_n} r_{n,\tau,j} \delta_{m,c^{(k)}_{n,\tau}} 
%   & \nonumber \\ 
% & \hspace{2.0cm} + \alpha_{j,m}^{(k)}, \! \forall m \! \in \! \{1,..,J\} \bigg] \bigg) .
% \end{flalign}
For \emph{CM}, \emph{MACE}, \emph{CV} and \emph{acc}, the factors follow a similar pattern of summing pseudo-counts of correct and incorrect answers. 
%For reasons of space, we omit the equations for these variants. 
The forward-backward passes
% variational factor $q(\bs t)$ 
also require the following expectation terms for \emph{seq},
which can be simplified for the other annotator models:
 \begin{flalign}
 \label{eq:elna}
& \mathbb{E}\ln \! A^{(k)}\!(j,l,m) \!=\! \Psi\!\left(\!N^{(k)}_{j,l,m}\!\right)
 \!-\! \Psi\!\left(\!\sum_{\;m'\!=1\!}^J \!\!\left( \!N^{(k)}_{j,l,m'} \!\right) \!\!\right)\!. &
 \end{flalign}
% For \emph{CM}, the equation can be adapted by omitting the $l$ subscripts on the right-hand side, which refer to the previous annotation in the sequence. 
The variational factor, $q(B^{(s)} )$, for each sequence tagger's annotator model
 has the same form as $q(A^{(k)} )$, substituting $\delta_{l,c^{(k)}_{n,\tau\!-\!1}}$ 
 for $\hat{d}_{n,\tau}^{(s)}(i)$, as defined in below in Equation \ref{eq:hatp}.


\textbf{Black-box sequence taggers}: the parameters of tagger $s$ have
%Our inference approach can incorporate either pre-trained sequence taggers, or 
%train the sequence tagger using the crowdsourced data while performing inference over the complete BSC model.
%In both cases, the tagger's reliability will be modeled by an annotator model, $B^{(s)}$, 
%so it is possible to incorporate noisy sequence taggers into the ensemble.
%----
% can be pre-trained, in which case their parameters, $\bs\theta^{(s)}$, 
% are fixed and we do not update the variational factor $q(\bs\theta^{(s)} )$
% or trained as part of our VB algorithm.
the following variational factor:
\begin{flalign}
 \ln q\Big( & \bs\theta^{(s)}\!\Big) \!=\! %\ln p(\bs\theta^{(s)}) + \mathbb{E}%_{\bs d_n^{(s)}}
%\left[ \ln p(\bs x | \bs\theta^{(s)}, \bs d_n^{(s)} ) \right] & \nonumber \\
%& \approx 
\ln p\!\left(\!\bs x | \bs\theta^{(s)}\!\!, \tilde{\bs d}^{(s)} \!\right) \!+\! \ln p\!\left(\!\bs\theta^{(s)}\!\right) \!+\! \mathrm{const}, & \nonumber \\
% \end{flalign}
% where $\tilde{\bs d_n}$ is an expectation:
% %takes the place of training labels
% %for obtaining a posterior distribution over $\bs \theat^{(s)}$.
% %The approximation above enables us to train the sequence tagger using its standard training or fitting function:
% %we compute $\ln q(\bs\theta^{(s)})$ by running the training function of the black-box sequence tagger, 
% %passing in a set of expectations over the labels in place of gold labels:
% \begin{flalign}
\tilde{d}_{n,\tau} &= 
\mathbb{E}\left[ p(d_{n,\tau}^{(s)} = i | B^{(s)}, t_{n,\tau} ) \right] 
& \nonumber\\
& = \sum_{j=1}^J \sum_{\iota=1}^J
r_{n,\tau,j} \tilde{d}_{n,\tau\!-\!1} \mathbb{E}B^{(s)}(j, i, \iota) .\!
 \label{eq:tildepd}
\end{flalign}
% d can be marginalized analytically so does not need a separate variational factor. 
% This results in the phat terms in the update equations. 
%The term $d_{n,\tau}^{(s)}$ can be marginalized without recourse to its own variational factor. 
%since it
%is independent of all other variables given $t_{n,\tau}$, $\bs x_n$, $B^{(s)}$,
%$d_{n,\tau\!-\!1}^{(s)}$
%and $\bs\theta^{(s)}$.
The expecations, $\tilde{\bs d}_n^{(s)}$, fill the role of training labels,
allowing us to use the training function of the black-box sequence taggers
to update the variational factor, $q\left(\theta^{(s)}\right)$.
%hence we can marginalize $\bs \theta^{(s)}$
%by training the sequence labeller using its own training procedure. 
Many black-box sequence taggers, including most neural networks, 
use maximum likelihood (ML) to find optimal point values, $\hat{\bs\theta}^{(s)}$,
rather than their posterior distribution.
If we integrate such sequence taggers, 
our complete inference procedure becomes 
a hybrid between VB and ML expectation maximization (EM)~\cite{bishop_pattern_2007}.
The sequence tagger may also require training using discrete labels, 
in which case we introduce a further ML step and 
approximate $\tilde{\bs d}_n^{(s)}$ 
by the most probable values at each token.
% instead of Equation \ref{eq:tildepd}: 
%\begin{flalign}
%& \tilde{d}_{n,\tau}^{(s)} \!= \argmax_{i} \;
%\mathbb{E}\!\left[ p(d_{n,\tau}^{(s)} \!= i | B^{(s)}\!, t_{n,\tau}) \right]. & \label{eq:discrete1}
%\end{flalign}

The update equations for other factors require 
expectations of $\bs d_{n}$ 
with respect to $\bs\theta^{(s)}$, or their ML approximation:
%over $\bs d_n^{(s)}$, defined as:
\begin{flalign}
 \hat{d}_{n,\tau}^{(s)}(i) & = \mathbb{E}%_{\bs\theta^{(s)}} 
\left[p(d_{n,\tau}^{(s)}=i | \bs x_n, \bs\theta^{(s)}) \right] \nonumber\\
& \approx p\left(d_{n,\tau}^{(s)}=i | \bs x_n, \hat{\bs\theta}^{(s)}\right) & \label{eq:hatp}
\end{flalign}
These values are the predictions obtained from the black-box sequence tagger given tokens $\bs x$.
% If possible, we obtain this posterior through the prediction function of the sequence tagger.
% However, some sequence tagger implementations may output only discrete predictions of the following form:
% \begin{flalign}
% & \hat{d}_{n,\tau}^{(s)}(i) = \argmax_{i} \;p\left(d_{n,\tau}^{(s)}=i | \bs x_n, \hat{\bs\theta}^{(s)}\right), &
% \end{flalign}
% where $\hat{\bs\theta}^{(s)}$ is the value of $\bs\theta^{(s)}$ learned using maximum likelihood or MAP optimization.
% As in Equation \ref{eq:discrete1}, we can use these discrete predictions in place of probabilities to perform an M-step from maximum likelihood-EM in place of taking  expectations over $\bs d^{(s)}$.
Therefore, our method requires only training and prediction functions 
to integrate a sequence tagger,
while its annotator model, $B^{(s)}$, accounts for the sequence tagger's reliability.
%error rates and 
%provides confidence estimates based on their reliability,
This means we can treat sequence taggers as black boxes, %and ignore their internal details,
even if their predictions are noisy or over-confident.
Pre-trained taggers can also be used, for example, to make use of taggers that were trained on different domains with more annotated data.

\subsection{Predicting the Sequence Labels}

%Two types of output from the BSC inference algorithm are of particular interest: (1) 
The approximate posterior probabilities of the true labels, $r_{j,n,\tau}$, provide confidence estimates for the labels. However, it is often useful to  compute 
the most probable sequence of labels, $\hat{\bs t}_n$, using the
Viterbi algorithm~\cite{viterbi1967error}. 
To apply the algorithm, we use the converged variational factors to compute 
$\mathbb{E}[\bs T]$, $\mathbb{E}[A^{(k)}],\forall k$, $\mathbb{E}[B^{(s)}],\forall s$ and
$\hat{d}_{n,\tau}^{(s)}(i), \forall s, \forall n, \forall \tau, \forall i$.
The most probable sequence is particularly useful because, unlike $r_{j,n,\tau}$,
the sequence will be consistent with any transition 
constraints imposed by the priors on the transition matrix $\bs T$, 
such as preventing `O'$\rightarrow$`I' transitions by assigning them zero probability.
We can also make predictions for unlabelled documents in a similar manner,
simply omitting the human annotations, $\bs c$, and relying only on the predictions
of the black-box sequence taggers, $\hat{\bs d}^{(s)}$.

% \subsection{Variational Lower Bound}
%
% The VB algorithm optimises the lower bound on model evidence, so it is useful to compute the lower bound
% to check for convergence, or to compare models with different hyperparameters when performing model selection. The lower bound for Bayesian annotator combination is:
% \begin{flalign}
%  &\mathcal{L}  = \mathbb{E}_{q} \left[ \ln p\left(\bs c, \bs t | \bs A, \bs\pi^{(1)},..,\bs\pi^{(K)}\right)
%  - \ln q(\bs t) \right] & \nonumber \\
%  & + \sum_{j=1}^L \bigg\{ \mathbb{E}_{q} \left[ \ln p\left(\bs A_j | \bs\beta_j\right)  -  \ln q(\bs A_j ) \right] & \nonumber\\
%  & + \sum_{l=1}^J \sum_{k=1}^K 
%  \mathbb{E}_{q}
%  \left[ \ln p \left( \bs\pi_{j,l}^{(k)} | \bs\alpha_{j,l}^{(k)} \right)
%  - \ln q \left( \bs\pi_{j,l}^{(k)} \right) \right]  \&& nonumer\\
% &  + \mathcal{L}_v \left(\bs\theta_v; \bs V, \bs c^{(b)} \right) \bigg\},&&
% \end{flalign}
% where $\mathcal{L}_v$ is provided by the text classifier implementation and 
% is the marginal log-likelihood of the text data $\bs V$, either given the maximum likelihood estimate of parameters $\theta_v$, or, in the case of a Bayesian classifier, an expectation over $\theta$.
% The lower bound computation uses the equations described in the previous section for the variational
% factors, $q(\bs A_j)$ and $q \left( \bs\pi_{j,l}^{(k)} \right)$, and the prior distributions for the parameters, 
% and inserts the expectations $\mathbb{E}\left[\ln \bs A_j \right]$ and $\mathbb{E}\left[\ln\bs\pi_{j,l}^{(k)} \right]$. The first term of $\mathcal{L}$ makes use of auxiliary variables from the 
% forward-backward algorithm:
% \begin{align}
%  && \mathbb{E}_q \left[ \ln p\left(\bs c, \bs t | \bs A, \bs\pi^{(1)},..,\bs\pi^{(K)}\right)\right] = 
%  \nonumber\\
%  && \sum_{i=1}^N \sum_{\tau=1}^{T_i} \sum_{j=1}^L r_{i,\tau,j} \ln r^{-}_{i,\tau,j} &
% \end{align}
