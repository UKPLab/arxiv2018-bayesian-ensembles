\section{Inference using Variational Bayes} \label{sec:vb}
 
Given a set of annotations, $\bs c$, 
%=\{\bs c^{(1)}, .., \bs c^{(K)} \}$, from $K$ annotators,
we obtain a posterior distribution over 
%the parameters, 
%$\bs T$, $\bs\theta = \{ \bs\theta^{(1)},..,\bs\theta^{(S)} \}$, 
%$\bs A=\{ A^{(1)},..,A^{(K)} \}$ and
%$\bs B = \{ B^{(1)},..,B^{(S)} \}$, 
%and thereby compute the posterior over 
sequence labels, $\bs t$, using
\emph{variational Bayes} (\emph{VB})~\cite{attias_advances_2000}.
Unlike maximum likelihood methods such as standard expectation maximization (EM),
VB considers prior distributions 
and accounts for parameter uncertainty due to 
noisy or sparse data.
In comparison to other Bayesian approaches such as Markov chain Monte Carlo (MCMC),
VB is often faster, readily allows incremental learning, and provides easier ways
to determine convergence~\cite{Bishop2006}. 
It has been successfully applied to a wide range of methods,
including as the standard learning procedure for LDA~\cite{blei2003},
and to combining non-sequential crowdsourced classifications~\cite{simpsonlong}.

The trade-off is that we must approximate the posterior distribution with
%the \emph{mean field} assumption,
an approximation that
factorises between subsets of latent variables:
% so that each subset, $z$, has a \emph{variational factor}, $q(z)$:
%over $\bs t$ and the model parameters, 
%$\bs T$, $\bs\theta = \{ \bs\theta^{(1)},..,\bs\theta^{(S)} \}$, 
%$\bs A=\{ A^{(1)},..,A^{(K)} \}$ and
%$\bs B = \{ B^{(1)},..,B^{(S)} \}$.
%The labels produced by the sequence taggers, $\bs d$, 
%can be marginalized, so do not appear in the approximate posterior, which is given by:
\begin{flalign} \label{eq:vb_posterior}
& p(\bs t, \bs A, \bs T, \bs\rho | \bs c, \bs x, \bs \alpha, \bs\gamma, \bs\kappa ) &   \nonumber\\
&  \approx \prod_{k=1}^K  q(A^{(k)}) \prod_{j=1}^J \left\{
q(\bs T_j) q(\bs \rho_j)\right\} \prod_{n=1}^N q(\bs t_n) .
\end{flalign}
VB performs approximate inference by
%we optimize Equation \ref{eq:vb_posterior} 
%using coordinate ascent to 
updating each variational factor, $q(z)$, in turn,  
optimising the approximate posterior distribution until it converges.
%taking expectations with respect to the current estimates of the other variational factors.
%(see Algorithm \ref{al:vb_bac}).
%Each iteration reduces the KL-divergence between the true and approximate posteriors
%of Equation \ref{eq:vb_posterior}, and hence optimizes a lower bound on the 
%log marginal likelihood, also called the evidence lower bound or ELBO
Details of the theory are beyond the scope of this paper, but are  
explained by ~\citet{Bishop2006}.
The VB algorithm is described in Algorithm \ref{al:vb_bac},
making use of update equations for the variational factors given below.
% We now provide the variational factors,
% which can be used to approximate the marginal posterior distributions for the parameters and sequence
% labels,
% and explain how to incorporate existing sequence taggers into the algorithm.
%that
%predict only sequence labels rather than full posterior distributions.
\begin{algorithm}
\DontPrintSemicolon
 \KwIn{ Annotations $\bs c$, words $\bs x$}
 \nl Compute initial values of $\mathbb{E}\ln A^{(k)},\forall k$,
 $\mathbb{E}\ln \rho_j,\forall j$, 
 $\mathbb{E}\ln \bs T_j,\forall j$ 
 from their prior distributions.\;
 \While{$\mathrm{not\_converged}(r_{n,\tau,j},\forall n,\forall \tau, \forall j)$}
 {
 \nl Update $r_{j,n,\tau}$, $s_{t_{j,n,\tau\!-\!1}, t_{\iota,n,\tau}}$, $\forall j,\forall \tau,\forall i,\forall \iota$,
 %given $\bs c$, 
 %$\mathbb{E}\left[\ln \bs A \right]$, $\mathbb{E}\left[\ln \bs B \right]$ and $\mathbb{E}\left[\ln \bs T \right]$
 using forward-backward algorithm
 given $\bs x$, $\bs c$, $\mathbb{E}\ln \bs T_j,\forall j$,
 $\mathbb{E}\ln\bs\rho_j,\forall j$, 
  $\mathbb{E}\ln\bs A^{(k)},\forall k$.\;%~\cite{ghahramani2001introduction}\;
 %\nl Set current true label predictions $\mathbb{E}\left[t_{n,\tau}=j\right] = r_{j,n,\tau}$ \;
%  \nl Retrain all sequence taggers using $\tilde{\bs d}^{(s)}$ as training labels for 
%  each tagger $s$\;
%  \nl Use sequence taggers to predict $\hat{d}^{(s)}_{n,\tau}(i)$, $\forall s,\forall n,\forall \tau,\forall i$\;
 \nl Update $\mathbb{E}\ln A^{(k)},\forall k$, 
 given $\bs c$, $r_{j,n,\tau}$.\;
%  \nl Update $\ln q(B^{(s)})$ and $\mathbb{E}\ln B^{(s)},\forall s$ ,
%  given current $\hat{\bs d}$, $r_{j,n,\tau}$\;
 \nl Update $\mathbb{E}\ln T_{j,\iota},\forall j,\forall \iota$, 
 given $s_{t_{j,n,\tau\!-\!1}, t_{\iota,n,\tau}}$.\;
 \nl Update $\mathbb{E}\ln \rho_j),\forall j$ given $\bs x$, $r_{j,n,\tau}$.
 }
\KwOut{ Label posteriors, $r_{n,\tau,j},\forall n,\forall \tau, \forall j$, %\mathbb{E}[\bs t]$, 
most probable sequence of labels, $\hat{\bs t}_n,\forall n$ using Viterbi algorithm }
\;
\caption{The VB algorithm for BSC.}
\label{al:vb_bac}
\end{algorithm}

We compute the posterior probability of each true token label, 
$r_{n,\tau,j}= \mathbb{E}[p(t_{n,\tau}=j | \bs c)]$,  %_{\bs T,\bs A,\bs B,\bs d}
and of each label transition, $s_{n,\tau,j,\iota} = 
\mathbb{E}%_{\bs T,\bs A,\bs B,\bs d}
[p(t_{n,\tau\!-\!1}=j, t_{n,\tau}=\iota | \bs c)]$,
%
% terms provide the output predictions of the class labels. 
% To compute $q(\bs T_j)$, $q(\bs A^{(k)} )$, and $q(\bs B^{(s)})$, 
% we require expectations for the individual 
% true labels $r_{n,\tau,j} 
using the forward-backward algorithm~\cite{ghahramani2001introduction}.
In the forward pass, we compute:
\begin{flalign}
   & \ln r^{-}_{n,\tau,j} = \ln \sum_{\iota=1}^J \left\{ r^{-}_{n,\tau-1,\iota} \mathrm{e}^{\mathbb{E}\ln T_{\iota,j}} \right\} + ll_{n,\tau}(j), & \nonumber \\
% \end{flalign}
%  where the log likelihood $ll(j,n,\tau)$ of the annotations for token $\tau$ in document $n$ given
%  label $j$ is:
% \begin{flalign} 
   & ll_{n,\tau}(j) = \sum_{k=1}^K \mathbb{E}%_{\bs A}
   \ln A^{(k)}\left(j, c_{n,\tau}^{(k)}, c_{n,\tau\!-\!1}^{(k)} \right)
   \mathbb{E}\ln\rho_{j,x_{n,\tau}}, & 
   %+  \sum_{s=1}^S
%    & \nonumber \\
%    &  \sum_{i=1}^J\sum_{\iota=1}^J \mathbb{E}%_{\bs B}
%    \ln B^{(s)} \!\left(j, i, \iota \right)  
%    \hat{d}_{n,\tau}^{(s)}(i) \hat{d}_{n,\tau-1}^{(s)}(\iota), & 
 \end{flalign}
 and in the backward pass we compute:
  \begin{flalign}
  %& \ln \lambda_{n,L_n,j} = 0, \hspace{1cm}
   \ln \lambda_{n,\tau,j} & = \ln\sum_{\iota=1}^J \exp \big\{ 
   \ln \lambda_{i,\tau+1,\iota}
   & \nonumber \\
&  + \mathbb{E}\ln T_{j,\iota} + ll_{n,\tau+1}(\iota) \big\} .&
 \end{flalign}
 Then we can calculate the posteriors as follows:
 \begin{flalign}
 r_{n,\tau,j} &\propto r^{-}_{n,\tau,j}\lambda_{n,\tau,j}, &\\
 s_{n,\tau,j,\iota} &\propto  r^{-}_{n,\tau-1,j} \lambda_{n,\tau,\iota} \exp\{\mathbb{E}\ln T_{j,\iota}+ ll_{n,\tau}(\iota)\}. &
 \end{flalign}
 
%The variational factor $q(\bs t)$ requires the following expectation:
The expectations of $\ln \bs T$ and $\ln\bs\rho$
can be computed using standard equations for a Dirichlet distribution:
 \begin{flalign}
& \mathbb{E}\ln T_{j,\iota} = \Psi\!\left(N_{j,\iota} \!\!+ \gamma_{j,\iota}\right) 
 - \Psi\!\left(\sum_{i=1}^J (N_{j,i} \!\!+ \gamma_{j,i}) \!\right), & \\
 & \mathbb{E} \ln\rho_j = \Psi\!\left(o_{j,w} \!\!+ \gamma_{j,w}\right) 
 - \Psi\!\left(\sum_{v=1}^J (o_{j,v} \!\!+ \gamma_{j,v}) \!\right), &
\end{flalign}
 where $\Psi$ is the digamma function,
  $N_{j,\iota} = \sum_{n=1}^N \sum_{\tau=1}^{L_n}  s_{n,\tau,j,\iota}$ is the expected number of times that label $\iota$ follows label $j$,
 and $o_{j,w}$ is the expected number of times that word $w$
 occurs with sequence label $j$.
Similarly, for the \emph{seq} annotator model, the expectation terms are:
 \begin{flalign}
 \label{eq:elna}
& \mathbb{E}\ln \! A^{(k)}\!(j,l,m) \!=\! \Psi\!\left(\!N^{(k)}_{j,l,m}\!\right)
 \!-\! \Psi\!\left(\!\sum_{\;m'\!=1\!}^J \!\!\left( \!N^{(k)}_{j,l,m'} \!\right) \!\!\right)\!. & \\
  & N^{(k)}_{j,l,m} \!\!=\!  \alpha_{j,l,m}^{(k)} \!\!\! + \!\sum_{n=1}^N \!\sum_{\tau=1}^{L_n} \!
r_{n,\tau,j} \delta_{l,c^{(k)}_{n,\tau\!-\!1}}\!\delta_{m,c^{(k)}_{n, \!\tau}}, \!& 
 \end{flalign}
 where $\delta$ is the Kronecker delta. 
For other annotator models, this equation is simplified as the values of
the previous labels are ignored.
% counts are be simplified for the other annotator models
 
% For \emph{CM}, the equation can be adapted by omitting the $l$ subscripts on the right-hand side, which refer to the previous annotation in the sequence. 
% The variational factor, $q(B^{(s)} )$, for each sequence tagger's annotator model
%  has the same form as $q(A^{(k)} )$, substituting $\delta_{l,c^{(k)}_{n,\tau\!-\!1}}$ 
%  for $\hat{d}_{n,\tau}^{(s)}(i)$, as defined in below in Equation \ref{eq:hatp}.


% \textbf{Black-box sequence taggers}: the parameters of tagger $s$ have
% %Our inference approach can incorporate either pre-trained sequence taggers, or 
% %train the sequence tagger using the crowdsourced data while performing inference over the complete BSC model.
% %In both cases, the tagger's reliability will be modeled by an annotator model, $B^{(s)}$, 
% %so it is possible to incorporate noisy sequence taggers into the ensemble.
% %----
% % can be pre-trained, in which case their parameters, $\bs\theta^{(s)}$, 
% % are fixed and we do not update the variational factor $q(\bs\theta^{(s)} )$
% % or trained as part of our VB algorithm.
% the following variational factor:
% \begin{flalign}
%  \ln q\Big( & \bs\theta^{(s)}\!\Big) \!=\! %\ln p(\bs\theta^{(s)}) + \mathbb{E}%_{\bs d_n^{(s)}}
% %\left[ \ln p(\bs x | \bs\theta^{(s)}, \bs d_n^{(s)} ) \right] & \nonumber \\
% %& \approx 
% \ln p\!\left(\!\bs x | \bs\theta^{(s)}\!\!, \tilde{\bs d}^{(s)} \!\right) \!+\! \ln p\!\left(\!\bs\theta^{(s)}\!\right) \!+\! \mathrm{const}, & \nonumber \\
% % \end{flalign}
% % where $\tilde{\bs d_n}$ is an expectation:
% % %takes the place of training labels
% % %for obtaining a posterior distribution over $\bs \theat^{(s)}$.
% % %The approximation above enables us to train the sequence tagger using its standard training or fitting function:
% % %we compute $\ln q(\bs\theta^{(s)})$ by running the training function of the black-box sequence tagger, 
% % %passing in a set of expectations over the labels in place of gold labels:
% % \begin{flalign}
% \tilde{d}_{n,\tau} &= 
% \mathbb{E}\left[ p(d_{n,\tau}^{(s)} = i | B^{(s)}, t_{n,\tau} ) \right] 
% & \nonumber\\
% & = \sum_{j=1}^J \sum_{\iota=1}^J
% r_{n,\tau,j} \tilde{d}_{n,\tau\!-\!1} \mathbb{E}B^{(s)}(j, i, \iota) .\!
%  \label{eq:tildepd}
% \end{flalign}
% % d can be marginalized analytically so does not need a separate variational factor. 
% % This results in the phat terms in the update equations. 
% %The term $d_{n,\tau}^{(s)}$ can be marginalized without recourse to its own variational factor. 
% %since it
% %is independent of all other variables given $t_{n,\tau}$, $\bs x_n$, $B^{(s)}$,
% %$d_{n,\tau\!-\!1}^{(s)}$
% %and $\bs\theta^{(s)}$.
% The expectations, $\tilde{\bs d}_n^{(s)}$, fill the role of training labels,
% allowing us to use the training function of the black-box sequence taggers
% to update the variational factor, $q\left(\theta^{(s)}\right)$.
% %hence we can marginalize $\bs \theta^{(s)}$
% %by training the sequence labeller using its own training procedure. 
% Many black-box sequence taggers, including most neural networks, 
% use maximum likelihood (ML) to find optimal point values, $\hat{\bs\theta}^{(s)}$,
% rather than their posterior distribution.
% If we integrate such sequence taggers, 
% our complete inference procedure becomes 
% a hybrid between VB and ML expectation maximization (EM) (see Bishop~\shortcite{Bishop2006}).
% The sequence tagger may also require training using discrete labels, 
% in which case we introduce a further ML step and 
% approximate $\tilde{\bs d}_n^{(s)}$ 
% by the most probable values at each token.
% % instead of Equation \ref{eq:tildepd}: 
% %\begin{flalign}
% %& \tilde{d}_{n,\tau}^{(s)} \!= \argmax_{i} \;
% %\mathbb{E}\!\left[ p(d_{n,\tau}^{(s)} \!= i | B^{(s)}\!, t_{n,\tau}) \right]. & \label{eq:discrete1}
% %\end{flalign}
%
% The update equations for other factors require 
% expectations of $\bs d_{n}$ 
% with respect to $\bs\theta^{(s)}$, or their ML approximation:
% %over $\bs d_n^{(s)}$, defined as:
% \begin{flalign}
%  \hat{d}_{n,\tau}^{(s)}(i) & = \mathbb{E}%_{\bs\theta^{(s)}} 
% \left[p(d_{n,\tau}^{(s)}=i | \bs x_n, \bs\theta^{(s)}) \right] \nonumber\\
% & \approx p\left(d_{n,\tau}^{(s)}=i | \bs x_n, \hat{\bs\theta}^{(s)}\right) & \label{eq:hatp}
% \end{flalign}
% These values are the predictions obtained from the black-box sequence tagger given tokens $\bs x$.
% % If possible, we obtain this posterior through the prediction function of the sequence tagger.
% % However, some sequence tagger implementations may output only discrete predictions of the following form:
% % \begin{flalign}
% % & \hat{d}_{n,\tau}^{(s)}(i) = \argmax_{i} \;p\left(d_{n,\tau}^{(s)}=i | \bs x_n, \hat{\bs\theta}^{(s)}\right), &
% % \end{flalign}
% % where $\hat{\bs\theta}^{(s)}$ is the value of $\bs\theta^{(s)}$ learned using maximum likelihood or MAP optimization.
% % As in Equation \ref{eq:discrete1}, we can use these discrete predictions in place of probabilities to perform an M-step from maximum likelihood-EM in place of taking  expectations over $\bs d^{(s)}$.
% Therefore, our method requires only training and prediction functions 
% to integrate a sequence tagger,
% while its annotator model, $B^{(s)}$, accounts for the sequence tagger's reliability.
% %error rates and 
% %provides confidence estimates based on their reliability,
% This means we can treat sequence taggers as black boxes, %and ignore their internal details,
% even if their predictions are noisy or over-confident.
% Pre-trained taggers can also be used, for example, to make use of taggers that were trained on different domains with more annotated data.
%

%TODO: provide the equations in the appendix
\subsection{Most Likely Sequence Labels}

%Two types of output from the BSC inference algorithm are of particular interest: (1) 
The approximate posterior probabilities of the true labels, $r_{j,n,\tau}$, provide confidence estimates for the labels. However, it is often useful to  compute 
the most probable sequence of labels, $\hat{\bs t}_n$, using the
Viterbi algorithm~\cite{viterbi1967error}. 
%To apply the algorithm, we use the converged variational factors to compute 
%$\mathbb{E}[\bs T]$ and $\mathbb{E}[A^{(k)}],\forall k$. %$\mathbb{E}[B^{(s)}],\forall s$ and
% $\hat{d}_{n,\tau}^{(s)}(i), \forall s, \forall n, \forall \tau, \forall i$.
The most probable sequence is particularly useful because, unlike $r_{j,n,\tau}$,
the sequence will be consistent with any transition 
constraints imposed by the priors on the transition matrix $\bs T$, 
such as preventing `O'$\rightarrow$`I' transitions by assigning them zero probability.
% We can also make predictions for unlabeled documents in a similar manner,
% simply omitting the human annotations, $\bs c$, and relying only on the predictions
% of the black-box sequence taggers, $\hat{\bs d}^{(s)}$.

%TODO provide the lower bound in the appendix

% \subsection{Variational Lower Bound}
%
% The VB algorithm optimises the lower bound on model evidence, so it is useful to compute the lower bound
% to check for convergence, or to compare models with different hyperparameters when performing model selection. The lower bound for Bayesian annotator combination is:
% \begin{flalign}
%  &\mathcal{L}  = \mathbb{E}_{q} \left[ \ln p\left(\bs c, \bs t | \bs A, \bs\pi^{(1)},..,\bs\pi^{(K)}\right)
%  - \ln q(\bs t) \right] & \nonumber \\
%  & + \sum_{j=1}^L \bigg\{ \mathbb{E}_{q} \left[ \ln p\left(\bs A_j | \bs\beta_j\right)  -  \ln q(\bs A_j ) \right] & \nonumber\\
%  & + \sum_{l=1}^J \sum_{k=1}^K 
%  \mathbb{E}_{q}
%  \left[ \ln p \left( \bs\pi_{j,l}^{(k)} | \bs\alpha_{j,l}^{(k)} \right)
%  - \ln q \left( \bs\pi_{j,l}^{(k)} \right) \right]  \&& nonumer\\
% &  + \mathcal{L}_v \left(\bs\theta_v; \bs V, \bs c^{(b)} \right) \bigg\},&&
% \end{flalign}
% where $\mathcal{L}_v$ is provided by the text classifier implementation and 
% is the marginal log-likelihood of the text data $\bs V$, either given the maximum likelihood estimate of parameters $\theta_v$, or, in the case of a Bayesian classifier, an expectation over $\theta$.
% The lower bound computation uses the equations described in the previous section for the variational
% factors, $q(\bs A_j)$ and $q \left( \bs\pi_{j,l}^{(k)} \right)$, and the prior distributions for the parameters, 
% and inserts the expectations $\mathbb{E}\left[\ln \bs A_j \right]$ and $\mathbb{E}\left[\ln\bs\pi_{j,l}^{(k)} \right]$. The first term of $\mathcal{L}$ makes use of auxiliary variables from the 
% forward-backward algorithm:
% \begin{align}
%  && \mathbb{E}_q \left[ \ln p\left(\bs c, \bs t | \bs A, \bs\pi^{(1)},..,\bs\pi^{(K)}\right)\right] = 
%  \nonumber\\
%  && \sum_{i=1}^N \sum_{\tau=1}^{T_i} \sum_{j=1}^L r_{i,\tau,j} \ln r^{-}_{i,\tau,j} &
% \end{align}
