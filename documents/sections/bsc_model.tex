\section{A Generative Model for Bayesian Sequence Combination}\label{sec:model}
%TODO do we state the obvious: that the annotator models effectively weight good annotators more heavily and discard spammers?
The generative story for our approach, \emph{Bayesian sequence combination (BSC)}, is as follows.
We assume a transition matrix, $\bs T$, where each entry is $T_{j,\iota} = p(t_{\tau} = \iota | t_{\tau-1} = j)$.
We draw each row of the transition matrix, $T_j \sim \mathrm{Dir}(\bs \gamma_j)$, where $\mathrm{Dir}$ is the Dirichlet distribution. 
For each document, $n$, in a set of $N$ documents, we draw a sequence of class labels, 
$\bs t_n = [t_{n,1}, ..., t_{n, L_n}]$, of length $L_n$, from a categorical distribution:
$t_{n,\tau} \sim \mathrm{Cat}(\bs T_{t_{n,\tau-1}})$. The set of all labels for all documents is referred to as $\bs t = \{\bs t_1,...,\bs t_N\}$.

%TODO mention the text features!
In the generative model, we assume one of the annotator 
models described in Section \ref{sec:annomodels} for each of $K$ annotators.
The number of parameters depends on the choice of annotator model:
%( t_{\tau}, c_{\tau}, c_{\tau-1})$.
for \emph{acc}, only one parameter, $\pi^{(k)}$, is drawn for annotator $k$;
for \emph{MACE}, we draw a single value $\pi^{(k)}$ and a vector $\xi^{(k)}$ of length $J$, 
while for \emph{CV} we draw $J$ independent values of $\pi_j^{(k)}$, 
and for \emph{CM}  
we draw a vector $\bs\pi^{(k)}_j$ of size $J$ for each true label value $j\in \{1,...,J\}$; in the case of \emph{seq}, 
we draw vectors $\bs\pi^{(k)}_{j,\iota}$ for each true label value 
for each previous label value, $\iota$.\
All parameters of these annotator models are probabilities, 
so are drawn from Dirichlet priors. We refer to the set of hyperparameters 
for $k$'s annotator model as $\bs\alpha^{(k)}$.
Given its parameters, the annotator model defines a likelihood function,
$A^{(k)}(t_{n,\tau}, \bs c_{n,\tau}, \bs c_{n,\tau-1})$, where $\bs c_{n,\tau}$ is the $\tau$th label of document $n$.
The argument $\bs c_{n,\tau-1}$ is only required if $A^{(k)}$ is an instance
of \emph{seq} and is ignored by the other annotator models.
We draw annotator $k$'s label for each token $\tau$ in each document $n$ 
according to:
\begin{flalign}
& c^{(k)}_{n,\tau} \sim \mathrm{Cat}( [A^{(k)}(t_{n,\tau}, 1, \bs c_{n,\tau-1}^{(k)}), ..., & \nonumber \\
& \hspace{3cm} A^{(k)}(t_{n,\tau}, J, \bs c_{n,\tau-1}^{(k)}) ]). &
\end{flalign}
The annotators are assumed to be conditionally independent of one another given the true labels,
$\bs t$, which means that their errors are assumed to be uncorrelated. This is a strong assumption
when considering that the annotators have to make their decisions based
on the same input data. However, in practice, dependencies do not usually cause the 
most probable label to change~\citep{zhang2004optimality}, hence the performance of classifier combination methods 
is only slightly degraded, while avoiding the complexity of modeling dependencies between annotators~\citep{kim2012bayesian}.
%
% \textbf{Black-box Sequence Taggers}:
% As an extension to our model, we can integrate $S$ automated methods as
% additional noisy annotators. 
%  In comparison to human annotators,
% sequence taggers can quickly label large numbers of documents, 
% providing a cheap source of additional annotations across the whole dataset.
% We model each sequence tagger, $s$, 
% using an annotator model, $B^{(s)}$,
% of one of the types described in Section \ref{sec:annomodels} (analogous to $A^{(k)}$ for a human annotator),
% with hyperparameters $\bs \beta^{(s)}$.
%
% We extend the generative model for BSC with additional steps as follows.
% Each sequence tagger generates a sequence of labels, $\bs d_{n}^{(s)}$, for each document $n$ 
% (analogous to $\bs c_n^{(k)}$ produced by human annotators)
%  according to: 
%  \begin{flalign}
%  & d_{n,\tau}^{(s)} \sim \mathrm{Cat}(
% [B^{(s)}(\bs t_{n,\tau}, 1, d_{n,\tau-1}^{(s)}), ..., && \nonumber \\
% & \hspace{3.0cm} B^{(s)}(\bs t_{n,\tau}, J, d_{n,\tau-1}^{(s)})]). &&
% \end{flalign}
%
% In the generative model, we draw a sequence of text tokens, $\bs x_n$, 
% from a likelihood, $p \left(\bs x_n | \bs d_{n}^{(s)}, \bs\theta^{(s)} \right) $,
%  given internal parameters, $\bs\theta^{(s)}$, and
% label sequence, $\bs d_{n}^{(s)}$.
% This likelihood is defined by the black-box sequence tagger.
% If the sequence tagger is Bayesian, its parameters, $\bs\theta^{(s)}$, may also be drawn from 
% an unknown prior distribution.
% However, since we are treating the tagger as a black box, we do not need to know these internal details.
% In the next section, we explain how we can avoid computing this likelihood explicitly during inference,
% and instead use only the sequence tagger's existing training and prediction functions to learn
% $\bs\theta^{(s)}$ in parallel with the parameters of the BSC model.
% Like the human annotators, each sequence tagger is assumed to produce labels that are conditionally independent 
% of the other sequence taggers given $\bs t$. 
% %Due to the fact that sequence taggers will typically use
% %the same features, i.e. the text of the documents, this independence assumption may be violated, 
% %yet
% %%as with the human annotators, 
% %this assumption in other models
% % has been shown not to hamper performance in 
% %many practical situations~\citep{zhang2004optimality}.

\textbf{Joint distribution}: the complete model can be represented by the
joint distribution, given by:
\begin{flalign}
& p(\bs t, \bs A, \bs B, \bs T, \bs\theta, \bs c, \bs d, \bs x | \bs \alpha, \bs \beta, \bs\gamma ) &  \\
%  & \approx q(\bs t, \bs A, \bs B, \bs\A^{(1)},...,\bs\A^{(K)},\bs\A^{(1)},...,\bs\A^{(S)}, \bs d^{(1)}, ...,\bs d^{(S)}) = q(\bs B) \prod_{n=1}^N q(\bs t_n) & \nonumber \\
& = \prod_{k=1}^K \left\{ p(A^{(k)} | \bs \alpha^{(k)}) \prod_{n=1}^N p(\bs c_n^{(k)} | A^{(k)}, \bs t)  \right\}
& \nonumber \\
& \prod_{j=1}^J p(\bs T_j | \bs\gamma_j)  \prod_{n=1}^N \prod_{\tau=1}^{L_n} p(\bs t_n | \bs T_{t_{n,\tau-1}}) 
%\prod_{s=1}^S \bigg\{ p(\bs \theta^{(s)})  \nonumber \\
%& p(B^{(s)} | \bs\beta^{(s)}) \! \! \prod_{n=1}^N \!\! \left\{ p(\bs x | \bs d^{(s)}, \bs \theta^{(s)}) p(\bs d^{(s)} | B^{(s)}, \bs t)  \right\} \!\! \bigg\}, \nonumber
& \label{eq:joint}
\end{flalign}
where each term is defined by the distributions of the generative model described in this section.

%\begin{flalign}
%& 
%=
%p\left(d_{n,\tau}^{(s)} | \bs t, d_{n,\tau-1}^{(s)}, A^{(s)} \right) & \nonumber\\
%& p \left(d_{n,\tau}^{(s)} | \bs\phi_n, \bs\theta^{(s)} \right) / p \left(d_{n,\tau}^{(s)} | \bs\theta^{(s)} \right),  & 
%\end{flalign}
%% Imagine what happens for each of the different combinations of values of d. We could train a squillion different sequence taggers on these, then take a weighted sum.
%
%% multiply by p(phi | theta) to get joint, divide by p(d) to get likelihood of phi given d, then since phi is independent of t and A,  p(phi | theta) cancels out. This follows from the generative model, which should be described here. The inference section should 
%% talk about learning theta, and computing the expectation of d.
%%We could avoid all this if the sequence tagger just learns to represent the left hand side. So t and A stay in the condition for the likelihood of the features. In which case, the first term on the right hand side is like a prior that needs to be
%% used when learning theta.
%where the first term on the right-hand side is defined by the annotator model
%with parameters $A^{(s)}$, and 
%of the sequence tagger, $s$.
%integrating existing sequence taggers using the learning
%procedure described in the next section.
