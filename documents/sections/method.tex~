\section{Modelling Text Span Annotations}\label{sec:model}

We model annotations using the IOB schema, in which each token in a document is labelled as either I (in), O (out), or B (begin). The IOB schema requires that the label I cannot directly follow a label O, since a B token must precede the first I in any span. The IOB schema allows us to identify whether a token forms part of an annotation or not, and the use of the B label enables us to separate annotations when one annotation span begins immediately after another without any gap. This schema does not permit overlapping annotations, which are typically undesirable in crowdsourcing tasks where the crowd is instructed to provide one type of annotation. The schema also does not consider different types of annotation, although it is trivial to extend both the schema and our model to permit this case. Using a single model for different types of annotation may be desirable if the annotators are likely to have consistent confusion patterns betweeen different annotation types. 

We propose an extension of the independent Bayesian classifier combination (IBCC) 
model~\cite{kim2012bayesian} for combining annotations provided by a crowd of unreliable annotators. We refer to our model as Bayesian annotator combination or BAC. In BAC, we model the text annotation task as a sequential classification problem, where the true class, $t_i$, of token $i$ may be I, O, or B, and is dependent on the class of the previous token, $t_{i-1}$. This dependency is modelled by a transition matrix, $A$, as used in a hidden markov model. Rows of the transition matrix correspond to the class of the previous token, $t_{i-1}$, while columns correspond to values of $t_i$. Each row is therefore a categorical distribution. 

We model the annotators using a confusion matrix similar to that used in \cite{simpsonlong}, which captures the likelihood that annotator $k$ labels token $i$ with class $c_i^{(k)}$, given the true class label, $t_i$, and the previous annotation from $k$, $c_{i-1}^{(k)}$. The dependency between $c_i^{(k)}$ and $t_i$ allows us to infer the ground truth from noisy or biased crowdsourced annotations. There is also a dependency on the previous worker annotation, since these are constrained in a similar way to the true labels, i.e. the class I cannot follow immediately from class O. Furthermore, mistakes in the class labels are likely to be correlated across several neighbouring tokens, since annotations cover continuous
spans of text. The confusion matrix, $\bs\pi^{(k)}$, is therefore expanded in our model to a three dimensional transition-confusion mtrax, where the element $\pi_{j,l,m}^{(k)} = p(c_i^{(k)} = m | c_{i-1}^{(k)}=l, t_i=j)$. Within $\bs\pi^{(k)}$, the vector $\bs\pi_{j,l}^{(k)} = \{ \pi_{j,l,1}^{(k)},...,\pi_{j,l,L}^{(k)}\} $, where $L$ is the number of class labels, represents a categorical distribution over the worker's annotations conditioned on the ground truth and their previous annotation.

\subsection{Generative Model}

In the BAC approach, the model described above is given a Bayesian treatment by placing prior distributions over the state transition matrix $A$ and worker confusion matrices $\bs\pi^{(k)}$. The generative process is as follows. 

\textbf{Ground truth:} For each class label $j=\{I, O, B\}$, we draw a row of the transition matrix, $A_j \sim \mathrm{Dir}(\bs\beta_j)$, where $\mathrm{Dir}$ is the Dirichlet distribution. 
%We also draw an initial state distribution, $\bs\kappa=\{\kappa_1, ..., \kappa_L\} \sim \mathrm{Dir}(\bs
%\nu)$, where $\kappa_j=p(t_1=j)$. 
For each document $i$ in a set of $N$ documents, we now draw a sequence of class labels $\bs t_i = [t_{i,1}, ..., t_{i, T_i}]$ of length $T_i$. For $\tau=1$, we draw the first label in each sequence from 
$t_{i,\tau} \sim \mathrm{Categorical}(\bs A_{O})$, 
%$t_{i,\tau} \sim \mathrm{Categorical}(\bs \kappa)$, 
then for $\tau > 1$, we draw subsequent labels from $t_{i,\tau} \sim \mathrm{Categorical}(\bs A_{t_{i,\tau-1}})$. The first label in each sequence uses hyperparameters $\bs A_{O}$ because there is no previous annotation, so we assume that the state $t_{i,0}$ prior to the document start is not part of an annotation, and therefore $t_{i,0}=O$ is an outside or $O$ token. 

\textbf{Worker annotations:} For each worker $k\in\{1,...,K\}$, true label $j\in\{1,...,L\}$, and previous worker label $l=\{1,...,L\}$, we draw vectors $\bs\pi_{j,l}^{(k)} \sim \mathrm{Dir}(\bs\alpha^{(k)}_{j,l})$, which make up the three-dimensional transition-confusion matrix. We now draw annotations for each worker $k$ for each document $i$, starting with the first term, $c_{i,1}^{(k)} \sim \mathrm{Categorical}( \bs\alpha^{(k)}_{t_{i,1}, O} )$, then subsequent terms $c_{i,\tau}^{(k)} \sim \mathrm{Categorical}( \bs\alpha^{(k)}_{t_{i,\tau}, c_{i,\tau-1}^{(k)}} )$. As with the true labels, the first annotation in each sequence uses hyperparameters $\bs\alpha^{(k)}_{t_{i,1}, O}$ because we assume that the annotation prior to token $1$ is equivalent to an $O$ annotation. 

\subsection{Variational Bayes (VB) Algorithm}

We modify the mean-field variational Bayes algorithm proposed by \cite{simpsonlong}, 
which assumes an approximate posterior distribution that factorises between the parameters and 
latent variables. For our proposed model, the variational approximation is given by:
\begin{equation}
  q(\bs t, \bs A, \bs\pi^{(1)},...,\bs\pi^{(K)}) = q(\bs t)\prod_{j=1}^L \left\{ q(\bs A_j)\prod_{l=1}^L \prod_{k=1}^K  q(\bs\pi_{j,l}^{(k)}) \right\}
\end{equation}
Below, we summarise the algorithm used to optimise this distribution to obtain an approximate posterior.
We then define the variational factors and expectation terms needed to perform each step of the algorithm. The procedure is as follows:
\begin{enumerate}
 \item \label{step:1} Initialise variational factors for parameters
$\bs A_j$, $\forall j$ and
$\bs\pi_{j,l}^{(k)}, \forall j, \forall l, \forall k$, e.g. by setting to prior distributions.
 \item \label{step:2} Calculate $\mathbb{E}\left[\log \bs A \right]$ 
and $\mathbb{E}\left[\log\bs\pi^{(k)} \right], \forall k$ given the current factors 
$q(\bs A_j)$ and $q(\bs\pi_j^{(k)})$.
 \item Update the variational factor for the ground truth labels, $q(\bs t)$, given 
the expectations $\mathbb{E}\left[\log\bs\pi^{(k)} \right], \forall k$, and
$\mathbb{E}\left[\log \bs A \right]$, using the forward-backward
algorithm\cite{ghahramani2001introduction}, which will be explained further below.
 \item Update the variational factors $q(\bs\pi_j^{(s)}), \forall j, \forall s$ for the confusion matrices given current estimate for $q(\bs t)$.
 \item \label{step:4} Update the variational factor for the transition matrix rows
$q(\bs A_j), \forall j$ given the current estimate for $q(\bs t)$.
 \item \label{step:6} Check for convergence in the ground truth label predictions,
$\mathbb{E}\left[\bs t\right]$, or in the variational lower bound. 
The latter may be more expensive to compute but gives stronger guarantees of convergence. 
If not converged, repeat from step \ref{step:2}.
\item \label{step:7} Output the predictions for the true labels, $\mathbb{E}\left[\bs t\right]$ given the converged estimates of the variational factors.
\end{enumerate}
latent variables. For our proposed model, the variational approximation is given by:
\begin{equation}
  q(\bs t, \bs A, \bs\pi^{(1)},...,\bs\pi^{(K)}) = q(\bs t)\prod_{j=1}^L \left\{ q(\bs A_j)\prod_{l=1}^L \prod_{k=1}^K  q(\bs\pi_{j,l}^{(k)}) \right\}
\end{equation}
Below, we summarise the algorithm used to optimise this distribution to obtain an approximate posterior.
The following subsection then defines the variational factors and expectation terms needed to perform each step of the algorithm. The procedure is as follows:
\begin{algorithm}
 \KwIn{ Crowdsourced annotations, $\bs c$}
 Initialise $\mathbb{E}\left[\log \bs A \right]$ 
 and $\mathbb{E}\left[\log\bs\pi^{(k)} \right], \forall k$\;
 \While{not converged}%$\mathbb{E}\left[\bs t\right]$
 {
 Update $q^*(t_{i,\tau}=j)$ and $q^*(t_{i,\tau-1}=j, t_{i,\tau}=j')$, $\forall i,\forall \tau,\forall j$,
 given $\mathbb{E}\left[\log \bs A \right]$ and 
 $\mathbb{E}\left[\log\bs\pi^{(k)} \right], \forall k$
 using the forward-backward algorithm\;
 Update $q^*(\bs A_j), \forall j$ given current $q^*(t_{i,\tau}=j)$\;
 Update $q^*\left(\bs\pi_j^{(k)}\right), \forall j, \forall k$ given current $q^*(t_{i,\tau-1}=j, t_{i,\tau}=j')$\;
 Recompute $\mathbb{E}\left[\log \bs A \right]$ 
 and $\mathbb{E}\left[\log\bs\pi^{(k)} \right], \forall k$ given current estimates of
 $q(\bs A_j)$ and $q(\bs\pi_j^{(k)})$\; 
 }
\KwOut{ Predictions for the true labels, $\mathbb{E}[t_{i,\tau}]$. }
%\caption{The key steps of the VB algorithm for Bayesian annotator combination.
%\label{al:vb_bac}
\end{algorithm}

There are several ways to initialise the expectation terms $\mathbb{E}\left[\log \bs A \right]$ 
 and $\mathbb{E}\left[\log\bs\pi^{(k)} \right], \forall k$. One possibility is to estimate the means 
 of the distributions using a cheaper method, such as maximum likelihood expectation maximisation, then
 take logarithms. Values may also be chosen at random.
  In our experiments we find initialising $\mathbb{E}\left[\log \bs A \right]$ 
 and $\mathbb{E}\left[\log\bs\pi^{(k)} \right], \forall k$ to their prior values to be effective
 as we use weakly informative priors.
 
The algorithm above iteratively updates each variational factor in turn. Each update increases the lower bound on the model evidence, $\mathcal{L}$, by optimising one variational factor given the current estimates of the others.  
Convergence can be checked cheaply by comparing values of $\mathbb{E}[t_{i,\tau}]$ between iterations. However, a more reliable method is to check $\mathcal{L}$ for convergence. We now present equations for the variational factors and expectation terms required by the algorithm, followed by the lower bound, $\mathcal{L}$. 

\subsection{Variational Factors}

For the sequence of true labels, $\bs t$, the optimal variational factor given the current estimates of $q(\bs A_j)$ and $q(\bs\pi_j^{(k)})$, is:
%\log\pi^{(k)}_{t_{i,\tau},c^{(k)}_{i,\tau-1},c^{(k)}_{i,\tau}}
\begin{flalign}
  &\log q^*(\bs t) = \mathbb{E}_{q} \left[ \sum_{i=1}^N \sum_{\tau=1}^{T_i} \bigg\{ \log p(t_{i,\tau} | t_{i,\tau-1}, \bs A ) \right. &&\nonumber \\
  &\left. + \sum_{k=1}^K p(c_{i,\tau}^{(k)} | t_{i,\tau}, c_{i,\tau-1}^{(k)}, \bs\pi^{(k)})
  \bigg\} \right] + \mathrm{const}, && \nonumber\\
   \label{eq:qstar_t}
   &=  \sum_{i=1}^N \sum_{\tau=1}^{T_i} %\sum_{j=1}^L \mathbb{E}_q[p(t_{i,\tau-1}=j | \bs c_{i,1:\tau})] 
 \mathbb{E}[\log A_{j,t_{i,\tau}}] 
 %\sum_{j'=1}^L p(t_{i,\tau}=j') % \sum_{i=1}^N \sum_{\tau=1}^{T_i}
  + \sum_{k=1}^K \log \tilde{\pi}^{(k)}_{i,\tau,t_{i,\tau}}
   + \mathrm{const}, &&  %+  p(\bs c_{i,\tau+1:T_i} | t_{i,\tau}) 
\end{flalign}
where for notational convenience we define $\log\tilde{\pi}^{(k)}_{i,\tau,j} = \mathbb{E}\left[\log\pi^{(k)}_{j,c^{(k)}_{i,\tau-1},c^{(k)}_{i,\tau}} \right]$. 
In the VB algorithm, the parameters updates to $q(\bs A_j)$ and $q(\bs\pi_j^{(k)})$
require expectations for the individual true labels and transitions from one each label to the next:
\begin{align}
 r_{i,\tau,j} & = q^*(t_{i,\tau}=j) = \mathbb{E}_q[p(t_{i,\tau}=j | \bs c)], &\\
 s_{i,\tau,j,j'} & = q^*(t_{i,\tau-1}=j, t_{i,\tau}=j') & \nonumber\\ 
 & = \mathbb{E}_q[p(t_{i,\tau-1}=j, t_{i,\tau}=j' | \bs c)].&
\end{align}
These terms can be computed using the forward-backward algorithm\cite{ghahramani2001introduction},
which consists of two passes. 
The forward pass starts from $\tau=1$ and computes for each value of $\tau$ the posterior given crowdsourced annotations for tokens up to and including $\tau$. 
\begin{flalign}
  & \log r^{-}_{i,\tau,j} = \mathbb{E}_q\left[ \log p(t_{i,\tau}=j | \bs c_{i,1:\tau}^{(1)},...,\bs c_{i,1:\tau}^{(K)}) \right] &
  \nonumber\\
  & = \sum_{j'=1}^L \left\{ \log r^{-}_{i,\tau-1,j'} + \mathbb{E}[\log A_{j',j}] \right\}
  + \sum_{k=1}^K \log\tilde{\pi}^{(k)}_{i,\tau,j}, &
\end{flalign}
where $\bs c_{i,1:\tau}^{(k)}$ is the set of labels from 1 to $\tau$ in document $i$.
We assume that the label at position, $t_{i,0} = O$, is always an 'outside' label.
The backwards pass starts from $\tau=T_i$ and scrolls backwards, computing the likelihood of the annotations at positions from $\tau+1$ to $T_i$ given the true label $t_{i, \tau}$.
\begin{flalign}
  & \log \lambda_{i,T_i,j} = 1 & \nonumber\\
  & \log \lambda_{i,\tau,j} = \mathbb{E}_q  \! \left[ \log p(\bs c_{i,\tau+1:T_i}^{(1)},...,\bs c_{i,\tau+1:T_i}^{(K)} | t_{i,\tau}=j) \right]
  & \!\!\!\!\nonumber\\
  & = \log \lambda_{i,\tau+1,j} + \sum_{j'=1}^L \! \left\{ \mathbb{E}[\log A_{j,j'}] + \sum_{k=1}^K \log \tilde{\pi}^{(k)}_{i,\tau+1,j'} \right\} \!\!\!\!\! &
\end{flalign}
By taking the exponents and applying Bayes' rule we arrive at the terms $r_{i,\tau,j}$ and $s_{i,\tau,j,j'}$:
\begin{flalign}
 & r_{i,\tau,j} \propto r^{-}_{i,\tau,j}\lambda_{i,\tau,j} &\\%}{\sum_{j'=1}^L r^{-}_{i,\tau,j'}\lambda_{i,\tau,j'}} \\
 & s_{i,\tau,j,j'} \propto r^{-}_{i,\tau-1,j}\lambda_{i,\tau,j'} \exp\! \left(\mathbb{E}[\log A_{j,j'}] 
+ \log \tilde{\pi}^{(k)}_{i,\tau,j'}  \right) . \!\!\!\! &
\end{flalign}
The $r_{i,\tau,j}$ terms are normalised by a sum over $j$, and the  $s_{i,\tau,j,j'}$ terms are 
normalised by a sum over $j$ and $j'$.
We also use the $r_{i,\tau,j}$ terms to produce the output predictions from the VB algorithm.

The optimal variational factor for each row of the ground truth transition matrix is:
\begin{flalign}
  & \log q^*(\bs A_{j}) & \nonumber\\ 
  & = \sum_{i=1}^N\sum_{\tau=1}^{T_i} \sum_{j'=1}^L s_{i,\tau,j,j'}\log\bs A_{j,j'} 
 + \log p(\bs A_j | \bs\beta_j) + \mathrm{const} &\nonumber\\
 & = \sum_{j'=1}^L N_{j,j'}\log\bs A_{j,j'} 
  + \log p(\bs A_j | \bs\beta_j) + \mathrm{const},&
\end{flalign}
where $N_{j,j'} = \sum_{i=1}^N \sum_{\tau=1}^{T_i} s_{i,\tau,j,j'}$ are pseudo-counts of the 
number of times that class $j$ follows class $j'$. Since we assumed Dirichlet priors over $\bs A_j$, the variational factor for $\bs A_j$ is Dirichlet distribution with parameters $\bs b_j = \bs\beta_j + \bs N_{j}$, where $\bs N_{j} = \left\{ N_{j,j'} , \forall j' \right\}$.

The VB algorithm requires a term $\mathbb{E}[\log A]$ to update the variational factors for the ground truth labels. We can compute each element using:
\begin{align}
  \mathbb{E}[\log A_{j,j'}] = \Psi\left(b_{j,j'}\right) - \Psi\left(\sum_{j'=1}^L b_{j,j'} \right),
\end{align}
where $\Psi$ is the digamma function.

For the three-dimensional worker transition-confusion matrices, $\bs\pi^{(k)}$, 
the optimal variational factors are given by:
\begin{flalign}
 \log q^*\left(\bs\pi_{j,l}^{(k)}\right) = &  \sum_{m=1}^J N_{j,l,m}^{(k)}\log\pi_{j,l,m}^{(k)} & \nonumber\\
 & + \log p\left(\bs\pi_{j,l}^{(k)} | \alpha_{j,l}^{(k)} \right) + \mathrm{const}, &
\end{flalign}
where $N^{(k)}_{j,l,m} = \sum_{i=1}^N\sum_{\tau=1}^{T_i} r_{i,\tau,j} \delta_{m,c^{(k)}_{i,\tau}}$ are
pseudo-counts and $\delta$ is the Kronecker delta. The variational 
factor is also a Dirichlet distribution with parameters $\bs a_{j,l}^{(k)} = \bs\alpha_{j,l}^{(k)} + \bs N_{j}^{(k)}$, where $\bs N_j^{(k)}=\left\{ N_{j,l,m}^{(k)}, \forall m \right\}$. 

To update the variational factor for the true class, the VB algorithm requires a three-dimensional expectation term, $\mathbb{E}[\log \pi^{(k)}]$, whose elements are computed using the following:
\begin{align}
  \mathbb{E}\left[\log \pi_{j,l,m}^{(k)}\right] = \Psi\left(a^{(k)}_{j,l,m}\right) - \Psi\left(\sum_{m=1}^L a^{(k)}_{j,l} \right).
\end{align}

\subsection{Variational Lower Bound}

The VB algorithm optimises the lower bound on model evidence, so it is useful to compute the lower bound
to check for convergence, or to compare models with different hyperparameters when performing model selection. The lower bound for Bayesian annotator combination is:
\begin{flalign}
 &\mathcal{L}  = \mathbb{E}_{q} \left[ \log p\left(\bs c, \bs t | \bs A, \bs\pi^{(1)},...,\bs\pi^{(K)}\right)
 - \log q(\bs t) \right] & \nonumber \\
 & + \sum_{j=1}^L \bigg\{ \mathbb{E}_{q} \left[ \log p\left(\bs A_j | \bs\beta_j\right)  -  \log q(\bs A_j ) \right] & \nonumber\\
 & + \sum_{l=1}^J \sum_{k=1}^K 
 \mathbb{E}_{q}
 \left[ \log p \left( \bs\pi_{j,l}^{(k)} | \bs\alpha_{j,l}^{(k)} \right)
 - \log q \left( \bs\pi_{j,l}^{(k)} \right) \right] \bigg\}.&&
\end{flalign}
The lower bound computation uses the equations described above for the variational
factors, $q(\bs A_j)$ and $q \left( \bs\pi_{j,l}^{(k)} \right)$, and the prior distributions for the parameters, 
and inserts the expectations $\mathbb{E}\left[\log \bs A_j \right]$ and $\mathbb{E}\left[\log\bs\pi_{j,l}^{(k)} \right]$. The first term of $\mathcal{L}$ makes use of auxiliary variables from the 
forward-backward algorithm:
\begin{align}
 && \mathbb{E}_q \left[ \log p\left(\bs c, \bs t | \bs A, \bs\pi^{(1)},..,\bs\pi^{(K)}\right)\right] = 
 \nonumber\\
 && \sum_{i=1}^N \sum_{\tau=1}^{T_i} \sum_{j=1}^L r_{i,\tau,j} \log r^{-}_{i,\tau,j} &
\end{align}

\section{Alternative Methods}\label{sec:alt}

To date, a number of methods have been used to reduce annotations from multiple workers to a single gold-standard set. These approaches make use of both heuristic and statistical techniques. This section outlines commonly-used baselines and state-of-the-art methods that we later compare against our method.

\subsection{Majority/Plurality Voting}

For classifications, a simple heuristic is to take the majority label, or for multi-class problems, the most popular label. Examples for NLP classification problems include sentiment analysis\cite{sayeed2011crowdsourcing},.... With text spans, we can use the IOB classes and choose the most popular label for each word, but there are a number of cases where the resulting spans would not follow the constraints of the schema, and an additional step is required to resolve these issues. The problems occur when annotators disagree about the starting and ending points of an annotation:
\begin{itemize}
  \item The votes for a token being inside a span can be split between the classes I and B, which could lead to tokens being excluded from spans even when most have marked them as inside. 
  \item The voting process can lead to spans of I tokens with no preceding B token if there is only a minority of annotators who marked did not agree on the first token. 
  \item The spans from different annotators could partly overlap, causing the overlap area itself to be marked as a separate span. In some cases, this may be a valid annotation, while in others it would be obvious to anyone reviewing the annotation that it is an artefact of the aggregation method. There does not seem to be a simple fix here, except for requesting more annotations from other workers. With a sufficient number of annotations, we expect the problem to be resolved.
\end{itemize}
In our experiments, we define a baseline \emph{majority voting} method, which addresses the problems described above as follows. We resolve the first problem using a two-stage voting process. First, we combine the I and B votes and determine whether each token should be labelled as O or not. Then, for each token marked as I or B, we and perform another voting step to determine the correct label. This resolves cases where annotators disagree about whether a span should be split into two annotations. To resolve the second problem of aggregated spans without a B token at the start, we mark the first I token in any aggregate span as B.  

The voting procedure outlined above produces annotations where the annotations of at least 50\% of workers intersect. A stricter approach can be used, which requires that all the annotators mark a token for it to be included (e.g. \cite{farra2015annotating}). We refer to this approach as the \emph{intersect} method. For tasks where workers are likely to miss many spans, it is also possible to lower the threshold so that we do not require a majority of workers to mark a token as I/B before we accept it as such during aggregation.

\subsection{Worker Accuracy-Based Methods}

Determine worker accuracy from a number of gold-standard tasks. Weight the workers' votes by accuracy and apply the majority voting approach above to produce a \emph{weighted majority voting} method.

An interesting approach is used by \cite{hsueh2009data} that takes into account amibiguity in sentiment classifications. It is unclear whether this can be generalised to other types of annotation such as argument components. 

The weights can also be obtained using unsupervised and semi-supervised learning. In this case we use an EM algorithm, in which we initialise the true annotations using the majority voting method, then use these to compute worker accuracies. The true annotations are then re-estimated using a weighted majority vote. The process repeats until convergence. This method is labelled \emph{weighted majority voting (EM)}. 

\subsection{Clustering Methods}

Cluster the annotations, e.g. using a mixture model with annotation centre and spread, or by merging the boundaries somehow. See Zooniverse annotation work -- could discretize this?

\subsection{Other Solutions}

The level of disagreement in annotations for a particular piece of text can be used to determine whether an annotation is of a insufficient quality to keep (e.g. \cite{sayeed2011crowdsourcing,hsueh2009data}. This can be achieved using the majority voting method, but adjusting the threshold for classifying a token as I/B from 50\% to something higher. 

\emph{Human resolution}: an additional worker selects the correct answer from the annotations provided by the initial set of workers, e.g. \cite{dagan2016specifying}. To reduce costs, the human resolution step could be applied only to text with large amounts of disagreement.

