\section{Introduction}\label{sec:intro}

The high demand for labeled training data in current NLP methods,
particularly deep learning, is widely recognized
%with deep learning performance often improved by making use of larger training sets
\cite{zoph2016transfer, rastogi2016weighting, P14-1111}.
A common NLP task that has benefited from deep learning is \emph{sequence tagging},
which involves classifying sequences of tokens for tasks such as named entity recognition (NER),
part-of-speech tagging (POS), or information extraction (IE). 
Neural network sequence taggers are typically trained on 
tens of thousands of documents %containing hundreds of thousands of tokens
~\cite{ma2016end,lample2016neural}.
This requirement for large labeled datasets presents a challenge 
when facing new domains or tasks, where obtaining labels is often time-consuming or costly.
%Unlike I.I.D. classification tasks, such as document classification, the class labels in a sequence tagging task are dependent on the labels of the previous tokens.

One way to obtain labeled data relatively cheaply is crowdsourcing, in which large
numbers of untrained workers annotate documents instead of more expensive experts.
However, this requires aggregating multiple unreliable labels for each document.
We could also obtain noisy labels from models trained on different domains,  
 multiple experts, or users of applications who click on and interact
with text.
Probabilistic methods for aggregating unreliable classifications 
have been shown to be more accurate than simple heuristics 
such as majority voting~\cite{Raykar2010,sheshadri2013square,rodrigues2013learning,hovy2013learning}.
However, work on sequence tagging is less extensive 
and existing methods cannot model some common annotator error patterns or
the effects of the order of annotators' labels~\cite{rodrigues2014sequence,nguyen2017aggregating}.

%%%% In this paper... we develop Bayesian model for aggregating sequence labels and compare different annotator models.
The sequence labeling tasks we consider in this paper follow a \emph{beginning, inside, outside} (\emph{BIO})
 scheme, 
in which the first token in a span of type `x' is labeled `B-x',  
subsequent tokens in the same span are labeled `I-x', 
and tokens outside spans are labeled `O'.
%This results in a sequence of labels, where each label depends on its predecessor.
We propose an aggregation method that takes advantage of the sequential dependencies between BIO tags
to learn the reliability of individual annotators and predict the true sequence.

When learning from noisy or small datasets, commonly-used
methods based on maximum likelihood estimation may produce over-confident predictions~\cite{xiong2011bayesian,srivastava2014dropout}. 
We therefore apply a Bayesian treatment to our method to account for model uncertainty in our predictions.
The resulting posterior probabilities facilitate active learning~\cite{settles2010active},
which aims to reduce the number of labels required to train a 
model by iteratively
selecting the most informative data points to label.

When aggregating crowdsourced data, we can improve performance and 
make predictions for unlabeled documents by modeling the text features as well as the annotators~\cite{simpson2015language,Felt2016SemanticAA}.
%e.g. when performing active learning. 
For complex tasks such as sequence tagging, 
we may wish to exploit existing state-of-the-art models, such as neural networks
that do not account for model uncertainty.
%potentially forming an ensemble of multiple methods to boost performance.
In this paper, we show how to integrate existing black box methods into the aggregation model 
to construct ensembles of deep learners and human annotators. 
Our method learns the reliability of each black box method,
since they may not always perform well, particularly given small training datasets,
and avoids the need to aggregate crowdsourced data using a separate pre-processing step 
before training a sequence tagger.

% Bayesian methods have successfully been applied to various crowdsourcing tasks in NLP, such as
% sentence fragment classification~\cite{fang2014active},
% sentiment analysis~\cite{levenberg2014predicting,venanzi2014community},
% intent classification~\cite{yang2018leveraging}
% and argument ranking~\cite{simpson2018finding}, 
% but have not been adapted to sequence tagging. 

% Conversely, the methods proposed for sequence tagging
% \cite{rodrigues2014sequence,nguyen2017aggregating} do not use a fully Bayesian approach.
% We introduce a fully Bayesian approach for aggregating sequence tags from multiple sources and  
% providing a modular inference method using variational Bayes that facilitates modifications to the model.
%

% Slight re-emphasis on the black-box integration: we adapt an existing idea, and show that it works but only
% once enough crowdsourced data is obtained. So our contribution is more about evaluating an existing idea
% with a tweak that should help performance by combining two existing ideas (the reliability model with the integrated learning using EM). Less about presenting a finished, robust method.
% This needs some error analysis -- what are the confusion matrices learned for the seq tagger? E.g. do they
% successfully capture reliability of it, or do they collapse?
This paper provides the following contributions:
\begin{itemize}
 \item A theoretical comparison of annotator reliability models 
 and evaluation on sequence tagging tasks
 \item \emph{Bayesian sequence combination (BSC)}, 
 a method for aggregating sequence labels from multiple annotators 
 that can model sequential dependencies between tags 
 \item A technique for wrapping existing black-box sequence taggers into the aggregation model to improve 
 the quality of aggregated labels
\end{itemize}
The following sections discuss related work, 
then detail annotator models for sequence tagging,
and present our variational approach that enables us to integrate existing classifiers.
We then describe the modular implementation of our proposed method, which is
made public with all of our experimental code\footnote{\url{http://github.com/ukplab/arxiv2018-bayesian-ensembles}}
and can easily be extended to new aggregation problems. 
The next sections compare different aggregation methods
with simulated annotators and two crowdsourced NLP datasets,
showing that our Bayesian aggregation method consistently outperforms the previous state-of-the-art.
Our experiments evaluate both active and passive learning scenarios with varying dataset sizes, 
analyze types of errors, and visualize the annotator models learned by our method.
Finally, we give conclusions and ideas for future work.

\subsection{Related Work}

A number of works have investigated methods for aggregating non-sequential
classifications from crowds, including 
Sheshadri and Lease~\shortcite{sheshadri2013square}, who benchmarked several 
aggregation methods. 
They found the most consistent performance from the method of
Raykar et al.~\shortcite{Raykar2010}, which employs probabilistic confusion matrices 
to model the reliability of individual annotators,
as proposed by Dawid and Skene~\cite{dawid_maximum_1979}.
In this paper, we develop and compare variations of this model for sequence tagging,
including a variant based on MACE~\cite{hovy2013learning}.
We focus on the core annotator representation, rather than extensions 
for clustering annotators~\cite{venanzi2014community,moreno_bayesian_2015},
modeling their dynamics~\cite{simpsonlong},
adapting to task difficulty~\cite{whitehill2009whose,bachrach2012grade},
or time spent~\cite{venanzi2016time}.

%Previous works have applied crowdsourcing to sequence labeling tasks such as named entity recognition~\cite{ritter2011named}
%and POS-tagging~\cite{hovy2014experiments}, but these do not develop new aggregation methods.
For aggregating sequence tags, Rodrigues et al.~\shortcite{rodrigues2014sequence} proposed 
a CRF-based model, \emph{CRF-MA}, that assumes only one annotator is correct for any given label.
Recently, Nguyen et al.~\shortcite{nguyen2017aggregating} proposed an approach that outperformed CRF-MA,
based on hidden Markov models (HMMs), called \emph{HMM-crowd}.
%HMM-crowd models the distribution of text tokens conditioned on the hidden state.
%More sophisticated LSTM-based sequence taggers must be trained separately given the true labels estimated by HMM-crowd.
Both CRF-MA and HMM-crowd use simpler annotator models than Dawid and Skene~\shortcite{dawid_maximum_1979}
that do not capture the effect of sequential dependencies on annotator reliability.
Neither CRF-MA nor HMM-crowd use a fully Bayesian approach,
which has been shown to be effective for handling uncertainty
due to noise in crowdsourced data for non-sequential classification~\cite{kim2012bayesian,simpsonlong,venanzi2014community,moreno_bayesian_2015}.
In this paper, we develop a sequential annotator model and a fully Bayesian method for aggregating sequence labels  that improves performance over previous approaches.


The HMM adapted by Nguyen et al \shortcite{nguyen2017aggregating} uses only a simple conditional independence model of text features. 
The authors also show how to train 
neural network sequence taggers directly on crowdsourced data
using an additional network layer to handle worker reliability, similar to Rodrigues and Pereira~\shortcite{rodrigues2018deep}.
However, the proposed approaches did not outperform either CRF-MA~\cite{rodrigues2018deep} or HMM-crowd\cite{nguyen2017aggregating}. 
Albarqouni et al.~\shortcite{albarqouni2016aggnet}
integrate a CNN classifier for image annotation
into an aggregation method based on 
expectation maximization (EM)~\cite{dempster_maximum_1977}.
% % While this approach improves the CNN performance over using a separate pre-processing step, 
% % their method cannot model label dependencies in sequence tagging.
Yang et al.~\shortcite{yang2018leveraging}  
adapt a Bayesian neural network so that it can be trained concurrently with an annotator model, also using EM. 
In contrast to previous work,
we do not require neural networks to be adapted, 
nor assume that their predictions are reliable 
when aggregating annotations.
Instead, we propose to learn the reliability of  
existing sequence taggers using a variational approach, 
allowing untrusted, off-the-shelf sequence taggers to enhance the performance of
the aggregation method. 

%
% takes as input only the crowdsourced labels 
%and not the features of data points~\cite{yan2011,chen2013},
%other methods improves performance by expanding 
% Their work follows earlier research on active learning from crowds that uses simpler 
% models of independent data points~\cite{fang2014active,simpson2015bayesian}. 
