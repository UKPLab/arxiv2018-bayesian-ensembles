\section{Introduction}\label{sec:intro}

%The high demand for labelled training data in 
%A common NLP task that has benefited from deep learning is \emph{sequence tagging},
%which involves classifying sequences of tokens for tasks such as named entity recognition,
%part-of-speech tagging, or information extraction. 
Current methods for \emph{sequence tagging}, a core task in NLP,
use deep neural networks 
%sequence taggers 
that require
tens of thousands of labelled documents for training %containing hundreds of thousands of tokens
~\cite{ma2016end,lample2016neural}.
%This requirement for large labeled datasets 
This presents a challenge 
when facing new domains or tasks, where obtaining labels is often time-consuming or costly.
%Unlike I.I.D. classification tasks, such as document classification, the class labels in a sequence tagging task are dependent on the labels of the previous tokens.
Labelled data can be obtained cheaply by crowdsourcing, in which large
numbers of untrained workers annotate documents instead of more expensive experts.
For sequence tagging, this results in multiple sequences of unreliable labels for each document.
%Noisy labels could also be obtained from models trained on different domains,  
% multiple experts, or users of applications who click on and interact
%with text.

Probabilistic methods for aggregating crowdsourced data
have been shown to be more accurate than simple heuristics 
such as majority voting~\cite{Raykar2010,sheshadri2013square,rodrigues2013learning,hovy2013learning}.
However, existing methods for aggregating sequence labels
cannot model dependencies between the annotators' labels 
%miss error patterns such as a tendency to label overly long spans
~\cite{rodrigues2014sequence,nguyen2017aggregating}
and hence do not account for their effect on annotator noise and bias.
%%%% In this paper... we develop Bayesian model for aggregating sequence labels and compare different annotator models.
In this paper, we remedy this by proposing a sequential annotator model and applying it to
 tasks that follow a \emph{beginning, inside, outside} (\emph{BIO})
 scheme, 
in which the first token in a span of type `x' is labelled `B-x',  
subsequent tokens are labelled `I-x', 
and tokens outside spans are labelled `O'.
%This results in a sequence of labels, where each label depends on its predecessor.
%We propose an aggregation method that takes advantage of the sequential dependencies between BIO tags
%to learn the reliability of individual annotators and predict the true sequence.

When learning from noisy or small datasets, commonly-used
methods based on maximum likelihood estimation may produce over-confident predictions~\cite{xiong2011bayesian,srivastava2014dropout}. 
In contrast, Bayesian inference accounts for model uncertainty when making predictions.
%and enables hyperparameter tuning in unsupervised scenarios through Bayesian model selection~\cite{Bishop2006}. 
Unlike alternative methods that optimize the values for model parameters, Bayesian inference
integrates over all possible values of a parameter, weighted by a prior distribution that captures background knowledge.
The resulting posterior probabilities improve downstream decision making
as they include the probability of errors due to a lack of knowledge. For example, 
during active learning, posterior probabilities assist with selecting the most informative data points~\cite{settles2010active}.

In this paper, we develop \emph{Bayesian sequence combination (BSC)}, 
building on prior work that has demonstrated the advantages of Bayesian inference for aggregating unreliable classifications~\cite{kim2012bayesian,simpsonlong,Felt2016SemanticAA,paun2018comparing}.
%arxiv2018-bayesian-ensembles\else *** \fi}}. 
%Bayesian inference also facilitates composing a model from separate components,
%which we exploit to construct ensembles of sequence taggers and to test different annotator models.
%TODO check whether any of these should be included
% Aggregated label quality can be improved by modeling the text features as well as the annotators~\cite{simpson2015language,Felt2016SemanticAA}.
% %e.g. when performing active learning. 
% For complex tasks such as sequence tagging, 
% we may wish to exploit existing state-of-the-art models, such as neural networks
% that do not account for model uncertainty.
% %potentially forming an ensemble of multiple methods to boost performance.
% In this paper, we show how to integrate existing black box methods into the aggregation model 
% to construct ensembles of deep learners and human annotators. 
% Our method learns the reliability of each black box method
% %since they may not always perform well, particularly given small training datasets,
% and avoids the need to aggregate crowdsourced data using a separate pre-processing step 
% before training a sequence tagger.
% Bayesian methods have successfully been applied to various crowdsourcing tasks in NLP, such as
% sentence fragment classification~\cite{fang2014active},
% sentiment analysis~\cite{levenberg2014predicting,venanzi2014community},
% intent classification~\cite{yang2018leveraging}
% and argument ranking~\cite{simpson2018finding}, 
% but have not been adapted to sequence tagging. 
% Conversely, the methods proposed for sequence tagging
% \cite{rodrigues2014sequence,nguyen2017aggregating} do not use a fully Bayesian approach.
% We introduce a fully Bayesian approach for aggregating sequence tags from multiple sources and  
% providing a modular inference method using variational Bayes that facilitates modifications to the model.
%
% Slight re-emphasis on the black-box integration: we adapt an existing idea, and show that it works but only
% once enough crowdsourced data is obtained. So our contribution is more about evaluating an existing idea
% with a tweak that should help performance by combining two existing ideas (the reliability model with the integrated learning using EM). Less about presenting a finished, robust method.
% This needs some error analysis -- what are the confusion matrices learned for the seq tagger? E.g. do they
% successfully capture reliability of it, or do they collapse?
BSC is 
the first fully-Bayesian method for aggregating sequence labels from multiple annotators.
As a core component of BSC, we also introduce the \emph{sequential confusion matrix (seq)},
a probabilistic model of annotator noise and bias, 
which goes beyond previous work by modelling sequential dependencies between annotators' labels.
Further contributions include a theoretical comparison of the probabilistic models of annotator noise and bias,
and an empirical evaluation on three sequence labelling tasks,
in which \emph{BSC} with \emph{seq} consistently outperforms the previous state of the art. 
We make 
%the modular implementation of our proposed method freely available, along with 
all of our code and data freely available\footnote{\url{http://github.com/ukplab/arxiv2018-bayesian-ensembles}}.

\section{Related Work}
%TODO: note that the HMM crowd study did not establish whether performance benefited from the text features,
% sequence model, or a combination. In our experiments we perform ablation and establish that the combination is needed to realise benefits of the sequential model.

%A number of works have investigated methods for aggregating non-sequential
%classifications from crowds.
Sheshadri and Lease~\shortcite{sheshadri2013square} benchmarked several 
aggregation models for non-sequential classifications, 
obtaining the most consistent performance from 
that of Raykar et al.~\shortcite{Raykar2010}, who
model the reliability of individual annotators
using probabilistic confusion matrices,
as proposed by Dawid and Skene~\shortcite{dawid_maximum_1979}.
%Nguyen et al.~\shortcite{hung2013evaluation} 
Simpson et al.~\shortcite{simpsonlong} showed that a Bayesian variant of
Dawid and Skene's model
can outperform maximum likelihood approaches and simple heuristics
when combining crowds of image annotators.
This Bayesian variant, independent Bayesian classifier combination
(\emph{IBCC})~\cite{kim2012bayesian}, 
was originally used to combine ensembles of automated classifiers
rather than human annotators. While
traditional ensemble methods such as boosting focus on how to generate
new classifiers~\cite{dietterich2000ensemble},
IBCC is concerned with modelling the reliability of each classifier in a given
set of classifiers.
To reduce the number of parameters in multi-class problems,
Hovy et al.~\shortcite{hovy2013learning} proposed \emph{MACE}, 
and showed that it performed better under a Bayesian treatment on NLP tasks.
Paun et al.~\shortcite{paun2018comparing} further illustrated
the advantages of Bayesian models of annotator ability on NLP classification tasks
with different levels of annotation sparsity and noise.

We expand this previous work by detailing the relationships between several annotator models
and extending them to sequential classification. 
% Could we mention this: The uncertainty in such inferences may then be
% used in applications such as jointly training clas-
% sifiers (Smyth et al., 1995; Raykar et al., 2010),
% comparing crowdsourcing systems (Lease and
% Kazai, 2011), or characterizing corpus accuracy
% (Passonneau and Carpenter, 2014).
% (OR maybe mention it in the intro paragraph).
% For classification tasks in NLP, 
%  evaluated six Bayesian methods,
% % pooled model -- assumes equal annotators but allows noise
% % DS, or rather my IBCC
% % MACE
% % hierarchical DS, basically original IBCC with pooled priors
% % item difficulty model, which ignores annotator reliability
% % logrndeff, item difficulty and annotator ability
% % Bayesian: uses Hamilonian Monte Carlo
% for their ability to obtain gold labels from crowdsourced data, model annotators and item difficulty.
% They found that...
% \cite{}. 
Here we focus on the core annotator representation, rather than extensions 
for clustering annotators~\cite{venanzi2014community,moreno_bayesian_2015},
modeling their dynamics~\cite{simpsonlong},
adapting to task difficulty~\cite{whitehill2009whose,bachrach2012grade},
or time spent by annotators~\cite{venanzi2016time}.

%Previous works have applied crowdsourcing to sequence labeling tasks such as named entity recognition~\cite{ritter2011named}
%and POS-tagging~\cite{hovy2014experiments}, but these do not develop new aggregation methods.
%To account for disagreement between annotators when training a sequence tagger, Plank et al. ~\shortcite{plank2014learning} 
%modify the loss function of the learner.
%to account for confusion between labels. 
%However, typical cross entropy loss
%naturally accommodates probabilities of labels as well as discrete labels~\cite{bekker2016training}.
%so it is unnecessary to adapt the loss function if  
%probabilities of training labels are used to account for disagreement between 
%multiple annotators
Methods for aggregating sequence labels 
include \emph{CRF-MA}~\cite{rodrigues2014sequence},
a CRF-based model that assumes only one annotator is correct for any given label.
Recently, Nguyen et al.~\shortcite{nguyen2017aggregating} proposed a hidden Markov model (HMM) approach that outperformed CRF-MA, called \emph{HMM-crowd}.
%HMM-crowd models the distribution of text tokens conditioned on the hidden state.
%More sophisticated LSTM-based sequence taggers must be trained separately given the true labels estimated by HMM-crowd.
Both CRF-MA and HMM-crowd use simpler annotator models than Dawid and Skene~\shortcite{dawid_maximum_1979}
that do not capture the effect of sequential dependencies on annotator reliability.
Neither CRF-MA nor HMM-crowd use a fully Bayesian approach,
which has been shown to be more effective for handling uncertainty
due to noise in crowdsourced data for non-sequential classification~\cite{kim2012bayesian,simpsonlong,venanzi2014community,moreno_bayesian_2015}.
In this paper, we develop a sequential annotator model and an approximate Bayesian method for aggregating sequence labels.  %that improves performance over previous approaches.

%TODO check if any of this needs to be kept from the black-box integration stuff.
% While HMM-crowd uses only a simple conditional independence model of text features,
% Nguyen et al.~\shortcite{nguyen2017aggregating} and 
% Rodrigues and Pereira~\shortcite{rodrigues2018deep} also train 
% neural network sequence taggers directly on crowdsourced data
% by adding a layer to handle worker reliability.
% However, the proposed approaches did not outperform either CRF-MA~\cite{rodrigues2018deep} or HMM-crowd~\cite{nguyen2017aggregating}. 
% A similar approach by Albarqouni et al.~\shortcite{albarqouni2016aggnet}
% integrates a CNN classifier for image annotation
% into an aggregation method based on 
% expectation maximization (EM)~\cite{dempster_maximum_1977}.
% % % While this approach improves the CNN performance over using a separate pre-processing step, 
% % % their method cannot model label dependencies in sequence tagging.
% Yang et al.~\shortcite{yang2018leveraging}  
% adapt a Bayesian neural network so that it can be trained concurrently with an annotator model, also using EM. 
% In contrast to previous work,
% we do not require neural networks to be adapted, 
% nor assume that their predictions are reliable 
% when aggregating annotations.
% Instead, we propose to learn the reliability of  
% existing sequence taggers,
% % using a variational approach, 
% allowing untrusted, off-the-shelf taggers to enhance the performance of
% the aggregation method. 

%
% takes as input only the crowdsourced labels 
%and not the features of data points~\cite{yan2011,chen2013},
%other methods improves performance by expanding 
% Their work follows earlier research on active learning from crowds that uses simpler 
% models of independent data points~\cite{fang2014active,simpson2015bayesian}. 
