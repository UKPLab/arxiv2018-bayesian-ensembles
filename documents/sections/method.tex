\section{Modelling Text Span Annotations}\label{sec:model}

We model annotations using the IOB schema, in which each token in a document is labelled as either I (in), O (out), or B (begin). The IOB schema requires that the label I cannot directly follow a label O, since a B token must precede the first I in any span. The IOB schema allows us to identify whether a token forms part of an annotation or not, and the use of the B label enables us to separate annotations when one annotation span begins immediately after another without any gap. This schema does not permit overlapping annotations, which are typically undesirable in crowdsourcing tasks where the crowd is instructed to provide one type of annotation. The schema also does not consider different types of annotation, although it is trivial to extend both the schema and our model to permit this case. Using a single model for different types of annotation may be desirable if the annotators are likely to have consistent confusion patterns betweeen different annotation types. 

We propose an extension of the independent Bayesian classifier combination (IBCC) 
model~\cite{kim2012bayesian} for combining annotations provided by a crowd of unreliable annotators. We refer to our model as Bayesian annotator combination or BAC. In BAC, we model the text annotation task as a sequential classification problem, where the true class, $t_i$, of token $i$ may be I, O, or B, and is dependent on the class of the previous token, $t_{i-1}$. This dependency is modelled by a transition matrix, $A$, as used in a hidden markov model. Rows of the transition matrix correspond to the class of the previous token, $t_{i-1}$, while columns correspond to values of $t_i$. Each row is therefore a categorical distribution. 

We model the annotators using a confusion matrix similar to that used in \cite{simpsonlong}, which captures the likelihood that annotator $k$ labels token $i$ with class $c_i^{(k)}$, given the true class label, $t_i$, and the previous annotation from $k$, $c_{i-1}^{(k)}$. The dependency between $c_i^{(k)}$ and $t_i$ allows us to infer the ground truth from noisy or biased crowdsourced annotations. There is also a dependency on the previous worker annotation, since these are constrained in a similar way to the true labels, i.e. the class I cannot follow immediately from class O. Furthermore, mistakes in the class labels are likely to be correlated across several neighbouring tokens, since annotations cover continuous
spans of text. The confusion matrix, $\bs\pi^{(k)}$, is therefore expanded in our model to three dimensions, where the element $\pi_{j,l,m}^{(k)} = p(c_i^{(k)} = m | c_{i-1}^{(k)}=l, t_i=j)$. Within $\bs\pi^{(k)}$, the vector $\bs\pi_{j,l}^{(k)} = \{ \pi_{j,l,1}^{(k)},...,\pi_{j,l,L}^{(k)}\} $, where $L$ is the number of class labels, represents a categorical distribution over the worker's annotations.

\subsection{Generative Model}

In the BAC approach, the model described above is given a Bayesian treatment by placing prior distributions over the state transition matrix $A$ and worker confusion matrices $\bs\pi^{(k)}$. The generative process is as follows. 

\textbf{Ground truth:} For each class label $j=\{I, O, B\}$, we draw a row of the transition matrix, $A_j \sim \mathrm{Dir}(\bs\beta_j)$, where $\mathrm{Dir}$ is the Dirichlet distribution. 
%We also draw an initial state distribution, $\bs\kappa=\{\kappa_1, ..., \kappa_L\} \sim \mathrm{Dir}(\bs
%\nu)$, where $\kappa_j=p(t_1=j)$. 
For each document $i$ in a set of $N$ documents, we now draw a sequence of class labels $\bs t_i = [t_{i,0}, ..., t_{i, T_i}]$ of length $T_i$. For $\tau=1$, we draw the first label in each sequence from 
$t_{i,\tau} \sim \mathrm{Categorical}(\bs A_{O})$, 
%$t_{i,\tau} \sim \mathrm{Categorical}(\bs \kappa)$, 
then for $\tau > 1$, we draw subsequent labels from $t_{i,\tau} \sim \mathrm{Categorical}(\bs A_{t_{i,\tau-1}})$. The first label in each sequence uses hyperparameters $\bs A_{O}$ because there is no previous annotation, so the state prior to token $1$ can be seen as an outside or $O$ token. 

\textbf{Worker annotations:} For each worker $k\in\{1,...,K\}$, and labels $j\in\{1,...,L\}$ and $l=\{1,...,L\}$, we draw each row of the confusion matrix $\bs\pi_{j,l}^{(k)} \sim \mathrm{Dir}(\bs\alpha^{(k)}_{j,l})$. We now draw annotations for each worker $k$ for each document $i$, starting with the first term, $c_{i,1}^{(k)} \sim \mathrm{Categorical}( \bs\alpha^{(k)}_{t_{i,1}, O} )$, then subsequent terms $c_{i,\tau}^{(k)} \sim \mathrm{Categorical}( \bs\alpha^{(k)}_{t_{i,\tau}, c_{i,\tau-1}^{(k)}} )$. As with the true labels, the first annotation in each sequence uses hyperparameters $\bs\alpha^{(k)}_{t_{i,1}, O}$ because we assume that the annotation prior to token $1$ is equivalent to an $O$ annotation. 

\subsection{Variational Bayes}

We modify the mean-field variational Bayes algorithm proposed by \cite{simpsonlong}, 
which assumes an approximate posterior distribution that factorises between the parameters and 
latent variables. The definitions of the variational factors are given in \cite{simpsonlong},
with required modifications explained in the rest of this section. 
The algorithm summarised by the following steps:
\begin{enumerate}
 \item \label{step:1} Initialise variational factors for parameters
$\bs A_j$, $\forall j$ and
$\bs\pi_{j,l}^{(k)}, \forall j, \forall l, \forall k$, e.g. by setting to prior distributions.
 \item \label{step:2} Calculate $\mathbb{E}\left[\log \bs A \right]$ 
and $\mathbb{E}\left[\log\bs\pi^{(k)} \right], \forall k$ given the current factors 
$q(\bs A_j)$ and $q(\bs\pi_j^{(k)})$.
 \item Update the variational factor for the ground truth labels, $q(\bs t)$, given 
the expectations $\mathbb{E}\left[\log\bs\pi^{(k)} \right], \forall k$, and
$\mathbb{E}\left[\log \bs A \right]$, using the forward-backward
algorithm\cite{ghahramani2001introduction}, which will be explained further below.
 \item Update the variational factors $q(\bs\pi_j^{(s)}), \forall j, \forall s$ for the confusion matrices given current estimate for $q(\bs t)$.
 \item \label{step:4} Update the variational factor for the transition matrix rows
$q(\bs A_j), \forall j$ given the current estimate for $q(\bs t)$.
 \item \label{step:6} Check for convergence in the ground truth label predictions,
$\mathbb{E}\left[\bs t\right]$, or in the variational lower bound. 
The latter may be more expensive to compute but gives stronger guarantees of convergence. 
If not converged, repeat from step \ref{step:2}.
\item \label{step:7} Output the predictions for the true labels, $\mathbb{E}\left[\bs t\right]$ given the converged estimates of the variational factors.
\end{enumerate}

\section{Alternative Methods}\label{sec:alt}

To date, a number of methods have been used to reduce annotations from multiple workers to a single gold-standard set. These approaches make use of both heuristic and statistical techniques. This section outlines commonly-used baselines and state-of-the-art methods that we later compare against our method.

\subsection{Majority/Plurality Voting}

For classifications, a simple heuristic is to take the majority label, or for multi-class problems, the most popular label. Examples for NLP classification problems include sentiment analysis\cite{sayeed2011crowdsourcing},.... With text spans, we can use the IOB classes and choose the most popular label for each word, but there are a number of cases where the resulting spans would not follow the constraints of the schema, and an additional step is required to resolve these issues. The problems occur when annotators disagree about the starting and ending points of an annotation:
\begin{itemize}
  \item The votes for a token being inside a span can be split between the classes I and B, which could lead to tokens being excluded from spans even when most have marked them as inside. 
  \item The voting process can lead to spans of I tokens with no preceding B token if there is only a minority of annotators who marked did not agree on the first token. 
  \item The spans from different annotators could partly overlap, causing the overlap area itself to be marked as a separate span. In some cases, this may be a valid annotation, while in others it would be obvious to anyone reviewing the annotation that it is an artefact of the aggregation method. There does not seem to be a simple fix here, except for requesting more annotations from other workers. With a sufficient number of annotations, we expect the problem to be resolved.
\end{itemize}
In our experiments, we define a baseline \emph{majority voting} method, which addresses the problems described above as follows. We resolve the first problem using a two-stage voting process. First, we combine the I and B votes and determine whether each token should be labelled as O or not. Then, for each token marked as I or B, we and perform another voting step to determine the correct label. This resolves cases where annotators disagree about whether a span should be split into two annotations. To resolve the second problem of aggregated spans without a B token at the start, we mark the first I token in any aggregate span as B.  

The voting procedure outlined above produces annotations where the annotations of at least 50\% of workers intersect. A stricter approach can be used, which requires that all the annotators mark a token for it to be included (e.g. \cite{farra2015annotating}). We refer to this approach as the \emph{intersect} method. For tasks where workers are likely to miss many spans, it is also possible to lower the threshold so that we do not require a majority of workers to mark a token as I/B before we accept it as such during aggregation.

\subsection{Worker Accuracy-Based Methods}

Determine worker accuracy from a number of gold-standard tasks. Weight the workers' votes by accuracy and apply the majority voting approach above to produce a \emph{weighted majority voting} method.

An interesting approach is used by \cite{hsueh2009data} that takes into account amibiguity in sentiment classifications. It is unclear whether this can be generalised to other types of annotation such as argument components. 

The weights can also be obtained using unsupervised and semi-supervised learning. In this case we use an EM algorithm, in which we initialise the true annotations using the majority voting method, then use these to compute worker accuracies. The true annotations are then re-estimated using a weighted majority vote. The process repeats until convergence. This method is labelled \emph{weighted majority voting (EM)}. 

\subsection{Clustering Methods}

Cluster the annotations, e.g. using a mixture model with annotation centre and spread, or by merging the boundaries somehow. See Zooniverse annotation work -- could discretize this?

\subsection{Other Solutions}

The level of disagreement in annotations for a particular piece of text can be used to determine whether an annotation is of a insufficient quality to keep (e.g. \cite{sayeed2011crowdsourcing,hsueh2009data}. This can be achieved using the majority voting method, but adjusting the threshold for classifying a token as I/B from 50\% to something higher. 

\emph{Human resolution}: an additional worker selects the correct answer from the annotations provided by the initial set of workers, e.g. \cite{dagan2016specifying}. To reduce costs, the human resolution step could be applied only to text with large amounts of disagreement.

