\subsection{Modular Implementation of Variational Inference} \label{seq:modular}
 
The variational inference method described in Section  \ref{sec:vb}
is naturally suited to a modular implementation. We divide the BSC model, 
as defined in Section \ref{sec:model} and Equation \ref{eq:joint}, into 
three modules: 
(a) the true label model, which defines the distribution over sequences of labels, $q(\bs t_n)$;
(b) the annotator model, which may be one of those described in Section \ref{sec:annomodels} and implements $q(A^{(k)})$ and $q(B^{(s)})$;
and (c) black-box sequence taggers, which
% The true label model , 
% %and implements lines 2, 3 and 8 in Algorithm \ref{al:vb_bac}. 
% The annotator model 
%  lines 6 and 7.
%The black-box sequence taggers 
are existing implementations that provide training and prediction functions
to predict true labels given text tokens, $\bs x$.
% and are used in lines 4 and 5.
The true label model exposes methods to compute 
$r_{n,\tau,j}$ and $s_{n,\tau,j,\iota}$, $\forall n,\forall \tau,\forall j,\forall \iota$,
%given $\bs c$.
%$\mathbb{E}[p( c_{n,\tau} \!\!=\! i | c_{n,\tau-1}, t_{n,\tau} \!=\! j, \bs\pi )]$.
%In BSC, the true label model learns a transition matrix,
%$\bs B$, which assumes a first-order Markov chain, but true label models with longer memory 
%could also be used, as long as they provide the terms $r_{n,\tau,j}$ and $s_{n,\tau,j,\iota}$
%required by the other components.
while the annotator models provide methods to initialise and update $q(A^{(k)})$ and $q(B^{(s)})$,
%update $q(\bs A)$ and $q(\bs B)$ during the VB algorithm,
and compute expectations according to Equation \ref{eq:elna}.%involving $\bs A$ and $\bs B$ required for other parts of the model.
%As discussed in Section \ref{sec:annomodels}, various annotator models are possible.
%given $\bs c$, $\hat{p}\bs{d}}$, and $\mathbb{E}[\bs t]$, and
%compute $\mathbb{E}[p( c_{n,\tau} \!\!=\! i | c_{n,\tau-1}, t_{n,\tau} \!=\! j, \bs\pi )]$.
By allowing individual functions to be replaced without rewriting the inference
method, the modular implementation makes it easier to adapt the model to different types of annotations,
and to test each component part.
For example, new annotator models could, in future, be introduced to aggregate 
continuous-valued ratings or pairwise preferences.

% \rough
% \section{Modelling Text Span Annotations}\label{sec:model}
%
% % \begin{itemize}
% % \item model description, VB equations, algorithm summary... 
% % \item ...How to set priors for IOB â€“ incorporating sequence knowledge
% % \item If we find that MACE and HMM_Crowd outperform IBCC and BAC, then the 
% % confusion matrix model may have too many parameters. We can also test variants of BAC, therefore, that use (a) BAC confusion matrix, (b) IBCC confusion matrix, (c) MACE spamming patterns, (d) an adaptation of MACE model to sequential data. The purpose
% % of (d) is to allow for constraints in the sequence of labels and detect any sequential patterns such as late starts or short spans. A naive solution to this would be to make the spamming model depend on the previous label, but this removes the advantage of having few parameters when the number of labels is large. Instead, we could make a single MACE spamming pattern, apply constraints that simply zero out disallowed transitions and re-normalise. This would remove any modelling of overly long/short or late/early annotations and could underweight B annotations that start late.   Alternatively, allow separate spamming models or BAC confusion matrices depending on I, O and B but any more specific annotation types share common I, O and B models.
% % \item It's unclear whether BAC is correctly measuring early/late starts/ends. Think this through...
% % \item Late starts: worker labels O when ground truth is B or I if their previous label was O. 
% % This would also be modelled by IBCC.  Worker then labels B when ground truth is I and previous label was O. 
% % BAC models the relevance of the B label to indicating an I or a B if the previous label was O. 
% % But IBCC would also be able to model this because it is not clear that it really matters what the previous label was. 
% % \item split annotations: worker labels B when ground truth is I and previous label was I. IBCC would need to model this in the same way as a late starter and cannot distinguish the cases. How often we need to distinguish these cases is hard to say.
% % \item Early starts: worker labels B then I when ground truth is O and previous label was  O. We down-weight their I votes because they follow B or I. This is bad when we reach the actual annotation. Again not clear how the previous label helps here...
% % \item Erroneous annotation span: worker labels B then I when ground truth is O. We down-weight their i votes because they follow a B or I, but their next B vote still carries weight if it follows an O vote. The B votes become the relevant ones which is bad if workers disagree on the starting token.
% % \item Late finishes: worker labels I when ground truth is O because they already labelled I. This is modelled by BAC but means that the I votes carry less weight. The result is that shorter spans will be recognised by BAC when annotators disagree (the conjunction), 
% % rather than a kind of average of the workers' spans.
% % \item Not splitting spans when they should be: worker labels I when B is true label and I or B is previous label. Since I can only follow I or B, it's not clear why splitting these cases is useful.
% % \item Short finishes: worker labels O when ground truth is I and previous label was I. This is modelled by BAC and would cancel out the point before.
% % \item Early starts/late finishes could better be modelled by a dependency on the true labels several steps away? While other problems are reasonably captured by IBCC anyway.
% % \end{itemize}
% %
%
% We model annotations using the IOB schema, in which each token in a document is labelled as either I (in), O (out), or B (begin). The IOB schema requires that the label I cannot directly follow a label O, since a B token must precede the first I in any span. The IOB schema allows us to identify whether a token forms part of an annotation or not, and the use of the B label enables us to separate annotations when one annotation span begins immediately after another without any gap. This schema does not permit overlapping annotations, which are typically undesirable in crowdsourcing tasks where the crowd is instructed to provide one type of annotation. The schema also does not consider different types of annotation, although it is trivial to extend both the schema and our model to permit this case. Using a single model for different types of annotation may be desirable if the annotators are likely to have consistent confusion patterns betweeen different annotation types. 
%
% We propose an extension of the independent Bayesian classifier combination (IBCC) 
% model~\cite{kim2012bayesian} for combining annotations provided by a crowd of unreliable annotators. We refer to our model as Sequential Bayesian classifier combination or Seq-BCC. In Seq-BCC, we model the text annotation task as a sequential classification problem, where the true class, $t_i$, of token $i$ may be I, O, or B, and is dependent on the class of the previous token, $t_{i-1}$. This dependency is modelled by a transition matrix, $A$, as used in a hidden markov model. Rows of the transition matrix correspond to the class of the previous token, $t_{i-1}$, while columns correspond to values of $t_i$. Each row is therefore a categorical distribution. Seq-BCC also extends IBCC by incorporating an arbitrary classifier to model the relationship between text tokens and the true class labels,
% providing a generalisation of the method proposed by Simpson et al. ~\shortcite{simpson2015language}. In this case,
% we assume that the integrated classifier can be used to model the likelihood of the text tokens, $\bs V$, of a document (a sequence of words or word embedding vectors) given its sequence of true labels: $p(\bs V | t_0, ..., t_N)$. % the index for the document should be in here too?
%
% We model the annotators using a confusion matrix similar to that used in \cite{simpsonlong}, which captures the likelihood that annotator $k$ labels token $i$ with class $c_i^{(k)}$, given the true class label, $t_i$, and the previous annotation from $k$, $c_{i-1}^{(k)}$. The dependency between $c_i^{(k)}$ and $t_i$ allows us to infer the ground truth from noisy or biased crowdsourced annotations. There is also a dependency on the previous worker annotation, since these are constrained in a similar way to the true labels, i.e. the class I cannot follow immediately from class O. Furthermore, mistakes in the class labels are likely to be correlated across several neighbouring tokens, since annotations cover continuous
% spans of text. The confusion matrix, $\bs\pi^{(k)}$, is therefore expanded in our model to a three dimensional transition-confusion mtrax, where the element $\pi_{j,l,m}^{(k)} = p(c_i^{(k)} = m | c_{i-1}^{(k)}=l, t_i=j)$. Within $\bs\pi^{(k)}$, the vector $\bs\pi_{j,l}^{(k)} = \{ \pi_{j,l,1}^{(k)},...,\pi_{j,l,L}^{(k)}\} $, where $L$ is the number of class labels, represents a categorical distribution over the worker's annotations conditioned on the ground truth and their previous annotation.
%
% \subsection{Variational Bayes (VB) Algorithm}
%
%
% %proposed by \cite{simpsonlong},

%
% \subsection{Variational Lower Bound}
%
% The VB algorithm optimises the lower bound on model evidence, so it is useful to compute the lower bound
% to check for convergence, or to compare models with different hyperparameters when performing model selection. The lower bound for Bayesian annotator combination is:
% \begin{flalign}
%  &\mathcal{L}  = \mathbb{E}_{q} \left[ \ln p\left(\bs c, \bs t | \bs A, \bs\pi^{(1)},...,\bs\pi^{(K)}\right)
%  - \ln q(\bs t) \right] & \nonumber \\
%  & + \sum_{j=1}^L \bigg\{ \mathbb{E}_{q} \left[ \ln p\left(\bs A_j | \bs\beta_j\right)  -  \ln q(\bs A_j ) \right] & \nonumber\\
%  & + \sum_{l=1}^J \sum_{k=1}^K 
%  \mathbb{E}_{q}
%  \left[ \ln p \left( \bs\pi_{j,l}^{(k)} | \bs\alpha_{j,l}^{(k)} \right)
%  - \ln q \left( \bs\pi_{j,l}^{(k)} \right) \right]  \&& nonumer\\
% &  + \mathcal{L}_v \left(\bs\theta_v; \bs V, \bs c^{(b)} \right) \bigg\},&&
% \end{flalign}
% where $\mathcal{L}_v$ is provided by the text classifier implementation and 
% is the marginal log-likelihood of the text data $\bs V$, either given the maximum likelihood estimate of parameters $\theta_v$, or, in the case of a Bayesian classifier, an expectation over $\theta$.
% The lower bound computation uses the equations described in the previous section for the variational
% factors, $q(\bs A_j)$ and $q \left( \bs\pi_{j,l}^{(k)} \right)$, and the prior distributions for the parameters, 
% and inserts the expectations $\mathbb{E}\left[\ln \bs A_j \right]$ and $\mathbb{E}\left[\ln\bs\pi_{j,l}^{(k)} \right]$. The first term of $\mathcal{L}$ makes use of auxiliary variables from the 
% forward-backward algorithm:
% \begin{align}
%  && \mathbb{E}_q \left[ \ln p\left(\bs c, \bs t | \bs A, \bs\pi^{(1)},..,\bs\pi^{(K)}\right)\right] = 
%  \nonumber\\
%  && \sum_{i=1}^N \sum_{\tau=1}^{T_i} \sum_{j=1}^L r_{i,\tau,j} \ln r^{-}_{i,\tau,j} &
% \end{align}

% \rough
% \section{Alternative Methods}\label{sec:alt}
%
% To date, a number of methods have been used to reduce annotations from multiple workers to a single gold-standard set. These approaches make use of both heuristic and statistical techniques. This section outlines commonly-used baselines and state-of-the-art methods that we later compare against our method.
%
% \subsection{Majority/Plurality Voting}
%
% For classifications, a simple heuristic is to take the majority label, or for multi-class problems, the most popular label. Examples for NLP classification problems include sentiment analysis\cite{sayeed2011crowdsourcing},.... With text spans, we can use the IOB classes and choose the most popular label for each word, but there are a number of cases where the resulting spans would not follow the constraints of the schema, and an additional step is required to resolve these issues. The problems occur when annotators disagree about the starting and ending points of an annotation:
% \begin{itemize}
%   \item The votes for a token being inside a span can be split between the classes I and B, which could lead to tokens being excluded from spans even when most have marked them as inside. 
%   \item The voting process can lead to spans of I tokens with no preceding B token if there is only a minority of annotators who marked did not agree on the first token. 
%   \item The spans from different annotators could partly overlap, causing the overlap area itself to be marked as a separate span. In some cases, this may be a valid annotation, while in others it would be obvious to anyone reviewing the annotation that it is an artefact of the aggregation method. There does not seem to be a simple fix here, except for requesting more annotations from other workers. With a sufficient number of annotations, we expect the problem to be resolved.
% \end{itemize}
% In our experiments, we define a baseline \emph{majority voting} method, which addresses the problems described above as follows. We resolve the first problem using a two-stage voting process. First, we combine the I and B votes and determine whether each token should be labelled as O or not. Then, for each token marked as I or B, we and perform another voting step to determine the correct label. This resolves cases where annotators disagree about whether a span should be split into two annotations. To resolve the second problem of aggregated spans without a B token at the start, we mark the first I token in any aggregate span as B.  
%
% The voting procedure outlined above produces annotations where the annotations of at least 50\% of workers intersect. A stricter approach can be used, which requires that all the annotators mark a token for it to be included (e.g. \cite{farra2015annotating}). We refer to this approach as the \emph{intersect} method. For tasks where workers are likely to miss many spans, it is also possible to lower the threshold so that we do not require a majority of workers to mark a token as I/B before we accept it as such during aggregation.

% \subsection{Worker Accuracy-Based Methods}
% 
% Determine worker accuracy from a number of gold-standard tasks. Weight the workers' votes by accuracy and apply the majority voting approach above to produce a \emph{weighted majority voting} method.
% 
% An interesting approach is used by \cite{hsueh2009data} that takes into account amibiguity in sentiment classifications. It is unclear whether this can be generalised to other types of annotation such as argument components. 
% 
% The weights can also be obtained using unsupervised and semi-supervised learning. In this case we use an EM algorithm, in which we initialise the true annotations using the majority voting method, then use these to compute worker accuracies. The true annotations are then re-estimated using a weighted majority vote. The process repeats until convergence. This method is labelled \emph{weighted majority voting (EM)}. 

% \subsection{Item-response Methods}
%
% *this should be moved to an earlier section and used to build up to the proposed method*
%
% The current state-of-the-art methods are termed \emph{Item-response} models \cite{Felt2016SemanticAA},
% which are based on the approach by \cite{dawid_maximum_1979}. These approaches use a confusion matrix
% to model the likelihood that annotator $k$ gives response $c$ to an item $i$. 
% This approach naturally accounts for bias toward a particular answer and varying accuracy 
% depending on the true class, and has been 
% shown to outperform techniques such as majority voting and weighted sums\cite{simpsonlong,raykar12,kim2003}.
% Recent extensions follow the Bayesian treatment of \cite{kim2003}, called IBCC,
% to deal with specific problems in crowdsourcing with large numbers of workers:
% \cite{moreno_bayesian_2015,venanzi2014community} identify clusters of crowd workers
% with shared confusion matrices to improve performance when information about individual workers is sparse;
% \cite{venanzi2016time} account for the time each worker takes to complete a task;
% \cite{Felt2016SemanticAA, simpson2015language} additionally model language features in text classification tasks
% to improve performance when data is sparse.
% However, none of these methods consider the sequential nature of classifications and treat each item as i.i.d.
% Therefore, they cannot take advantage of the dependencies between each token's annotation to improve
% predictions and ensure valid sequences. 
% In this paper, we propose and evaluate a method that resolves this problem. 
% The modular nature of graphical models means that the extensions described above could in future be combined
% with our approach in suitable situations.
%
% A method that simplifies the confusion matrix, \emph{MACE}, was proposed by \cite{hovy2013learning}
% to reduce the cost of learning. This is particularly suitable for tasks with a large number of classes
% since the number of parameters in the confusion matrix typically grows $\mathcal{O}(J^2)$, where $J$ is the number of classes.
%
% * show mathematically how this method is related to IBCC *
%
% However, there are some potential downsides to this simplification. Bias toward a particular class is fixed,
% and skill level no longer depends on the ground-truth class. The class proportions distribution is also omitted
% in both \cite{hovy2013learning} and the accompanying published software implementation, which
% could lead to reduced performance when classes are highly imbalanced. 
% In our experiments, we compare MACE to both standard IBCC and our proposed method, Seq-BCC to illustrate the 
% types of situation where each approach may be advantageous. 

% \subsection{Clustering Methods}
%
% Cluster the annotations, e.g. using a mixture model with annotation centre and spread, or by merging the boundaries somehow. See Zooniverse annotation work -- could discretize this?
%
% \subsection{Other Solutions}
%
% The level of disagreement in annotations for a particular piece of text can be used to determine whether an annotation is of a insufficient quality to keep (e.g. \cite{sayeed2011crowdsourcing,hsueh2009data}. This can be achieved using the majority voting method, but adjusting the threshold for classifying a token as I/B from 50\% to something higher. 
%
% \emph{Human resolution}: an additional worker selects the correct answer from the annotations provided by the initial set of workers, e.g. \cite{dagan2016specifying}. To reduce costs, the human resolution step could be applied only to text with large amounts of disagreement.
%
% \subsection{How to Include Text Features into the Crowdsourcing Model}
%
% Modelling the text features as part of the aggregation method has been shown to improve classification performance, particularly when few labels are available, allows classification of unlabelled items without training a separate classifier, and provides a basis for active selection of documents for further labelling \cite{settles2010active}.
%
% The difficulty of modelling text features is that it requires a suitable classifier for the task at hand, and so it may not be effective to design a generic crowdsourcing model that describes the relationship between text features and class labels. Instead, we propose a solution that allows us to include task-specific classifiers, e.g. if the task at hand is NER, we show how to integrate a neural network designed specifically for NER.
%
% The modular nature of variational Bayesian inference allows us to reusing the existing inference steps when extending the graphical model. This means that we can add additional components to the model to model the relationship between text features and classifications. This section shows how we can treat task-specific classifiers as black-box extensions to the graphical model, and integrate them into the VB inference procedure. 
