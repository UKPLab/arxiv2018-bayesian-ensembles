\section{Introduction}\label{sec:intro}

\begin{itemize}
\item sequential annotation problems are very frequent in NLP; high cost of labelling means we need crowdsourcing; 
\item Example of a sequential span and disagreement between workers
\item ...we treat the crowd workers as classifiers and combine; can also do this with ensembles of ML classifiers
\item ...current methods don’t take advantage of sequential information
\item Add in description of rival method – non-Bayesian, missing sequential model for workers, has no way to set priors to specify IOB rules, 
\item (Put into a later related work section?) HMM\_crowd includes features but we don't consider that aspect here because it is task-specific. 
Using a generic word-based model may be effective in some cases but could be problematic if using small datasets where the words are not actually good features, e.g. argument labelling. 
The best models for doing this are... Any problem-specific model can be incorporated into VB. Here, we focus on the combination of workers and sequential labelling.
\item Bayesian approaches to classifier combination without sequential information are state-of-the-art
\item We develop a Bayesian approach that takes full advantage of knowledge of sequential labelling rules, such as BIO.
\end{itemize}

Scientific research relies on humans to recognise important patterns in data – even if we employ automated methods, these typically require training labels produced by human annotators. 
Natural language processing (NLP) often requires people to annotate segments of text, which we then use to train machine learning algorithms and evaluate our results.
Many NLP tasks require training data in the form of annotations of phrases and propositions in text. These annotations are spans of varying length, and different pieces of text may contain different numbers of spans. An example is highlighting claims in argumentative text. Annotators will typically make mistakes and may disagree with each other about the correct annotation, even if they are experts. When processing large datasets we may use crowdsourcing to reduce costs/time of experts, which increases the amount of noise and disagreements as the annotators are non-experts. Therefore, we require a method for aggregating text span annotations from multiple annotators.

Heuristic rules could be applied, such as taking intersections of annotations, or majority labels for individual words to determine whether they form part of a span or not. However, this does not account for differing reliability between workers (e.g. there may be spammers, people who do not understand the task) and the theoretical justification for these rules is often unclear. Therefore it may not be possible to apply simple heuristics to obtain gold-standard labels from a crowd. 

In this paper develop a Bayesian machine learning algorithm for combining multiple unreliable text annotations.
The method we propose is based on the classifier combination method described by \cite{kim2012bayesian}, 
which was shown to be effective for handling the unreliable classifications provided by a crowd of workers. A scalable implementation of this method using variational Bayes was described by \cite{simpsonlong}, which we use as the basis for our implementation in the current work. This paper provides the following contributions:
\begin{itemize}
  \item Novel Bayesian methodology
  \begin{itemize}
     \item Propose a probabilistic model for combining classifications to combine annotations over sequences of words
     \item Describes and tests a scalable inference algorithm for the proposed model that adapts the existing variational Bayes implementation for classifier combination
     \item Analysis of priors and how to choose them
     \item Guidelines for NLP crowdsourcing users on why and when to use MACE/IBCC (spam probability vs. confusion matrix) models for workers (this requires a task with many classes to show value of MACE, e.g. from the MACE paper itself)
  \end{itemize}
  \item Compares the approach on real-world NLP datasets with simple heuristic methods (e.g. mode) and alternatives such as weighted combinations
  \begin{itemize}
     \item Demonstrate the technique outperforms previous methods on two crowdsourcing problems?
     \item Show that the technique also works well for forming ensembles of classifiers
  \end{itemize}
  \item Evaluates the different possibilities for integrating a task-specific classifier using one of the following methods:
  \begin{itemize}
     \item (1) an additional base classifier (retrained on soft/hard labels after crowdsourced labels have been combined; predictions then made as a combination of crowd+newly trained classifier)
     \item (2) an additional base classifier (iteratively retrained on soft labels inside the VB loop)
     \item (3) using the probabilities output by the classifier as a q() distribution inside the graphical model (as in WWW 16 paper) (retraining inside the VB loop until convergence). 
     \item (4) Other approach taken in ACL 2017 paper?
     \item The distinction is important because intuitively many people use a pipeline: first, get gold or soft-gold labels by applying MACE to the crowdsourced labels, then train a classifier. We show how iterative training improves things. 
  \end{itemize} 
  \item Demonstrates how using the proposed Bayesian model enables an active learning approach that improves crowdsourcing efficiency
\end{itemize}

\subsection{Notes on Applications and Datasets}

There are several annotation tasks for NLP that we are interested in:
\begin{itemize}
  \item Argument component labelling -- identifying claims and premises that form an argument. This requires marking individual sentences, clauses, or spans that cross sentence boundaries. Some schemas allow for the component to be split so that it consists of multiple spans with excluded text between the spans.
  \item Semantic role labelling (SRL).
\end{itemize}
