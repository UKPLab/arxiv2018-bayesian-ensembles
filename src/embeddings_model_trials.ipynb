{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: change the data structure and context window distribution so that we can have different numbers of context windows per word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edwin/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.stats import multivariate_normal as mvn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 3 # dimensionality of the embeddings\n",
    "S = 2 # number of different senses per word. TODO: make this random for each word\n",
    "V = 5 # vocabulary size\n",
    "N = 10 # number of context windows for each word in the vocabulary\n",
    "C = 3 # context window size\n",
    "\n",
    "tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0. Sense distribution = [0.26194053 0.73805947]\n",
      "Word 1. Sense distribution = [0.44208271 0.55791729]\n",
      "Word 2. Sense distribution = [0.35466243 0.64533757]\n",
      "Word 3. Sense distribution = [0.22277787 0.77722213]\n",
      "Word 4. Sense distribution = [0.10985147 0.89014853]\n",
      "Data shape: (10, 3, 5)\n",
      "Type of cw: int64\n"
     ]
    }
   ],
   "source": [
    "def build_toy_dataset(N):\n",
    "  mu0 = np.zeros(D) # prior mean of the sense means\n",
    "  sigma0 = np.ones(D) # prior covariance of the sense means\n",
    "  a0 = np.ones(D) # prior shape for the scale of the sense covariance\n",
    "  b0 = np.ones(D) # prior scale for the scale of the sense covariance   \n",
    "  alpha0 = np.ones(S) # priors for the sense probabilities\n",
    "  beta0 = np.ones(V) # priors over the word frequencies\n",
    "    \n",
    "  mus_all = []\n",
    "  Sigmas_all = []\n",
    "  pis_all = []\n",
    "\n",
    "  # draw word frequencies\n",
    "  pword = np.random.dirichlet(beta0)\n",
    "  \n",
    "  # draw the word sense distributions\n",
    "  for w in range(V):\n",
    "    mus = []\n",
    "    Sigmas = []\n",
    "    pis = np.random.dirichlet(alpha0)\n",
    "    \n",
    "    # draw the means for each sense\n",
    "    for s in range(S):\n",
    "        mus.append(np.random.multivariate_normal(mu0, np.diag(sigma0)))\n",
    "        Sigmas.append(np.diag(1.0 / np.random.gamma(a0, b0)))\n",
    "        \n",
    "    mus_all.append(mus)\n",
    "    Sigmas_all.append(Sigmas)\n",
    "    pis_all.append(pis)\n",
    "    \n",
    "  s_all = []\n",
    "  z_all = []\n",
    "  c_all = []\n",
    "    \n",
    "  # draw the context windows for each word\n",
    "  for w in range(V):\n",
    "    \n",
    "    # draw the sense for each context\n",
    "    print('Word %i. Sense distribution = %s' % (w, str(pis_all[w])))\n",
    "    s_ws = np.argmax(np.random.multinomial(1, pis_all[w], N), 1)\n",
    "    z_ws = []\n",
    "    \n",
    "    c_ws = []\n",
    "    \n",
    "    for n in range(N):\n",
    "        \n",
    "        #print('Sense for pair n = %i' % s_ws[n])\n",
    "        #print('Mean for chosen sense = %s' % str(mus_all[w][s_ws[n]]))\n",
    "        #print('Cov for chosen sense = %s' % str(Sigmas_all[w][s_ws[n]]))\n",
    "        \n",
    "        # draw the embedding for each context\n",
    "        z_ws.append(np.random.multivariate_normal(mus_all[w][s_ws[n]], Sigmas_all[w][s_ws[n]]))\n",
    "    \n",
    "        # construct the categorical distribution over all words\n",
    "        joint = []\n",
    "        for w2 in range(V):\n",
    "            \n",
    "            pw2 = 0\n",
    "            for s in range(S):\n",
    "                pw2 += pis_all[w2][s] * mvn.pdf(z_ws[-1], mus_all[w2][s], Sigmas_all[w2][s])\n",
    "            pw2 *= pword[w2]\n",
    "            joint.append(pw2)\n",
    "        \n",
    "        pc_giv_z = joint / np.sum(joint)\n",
    "        \n",
    "        c = np.argmax(np.random.multinomial(1, pc_giv_z, C), 1)\n",
    "        c_ws.append(c)\n",
    "        \n",
    "    s_all.append(s_ws)\n",
    "    z_all.append(z_ws)\n",
    "    c_all.append(c_ws)\n",
    "        \n",
    "  c_all = np.array(c_all, dtype=int).swapaxes(0, 2).swapaxes(0, 1) # so we get N x C x V from V x N x C\n",
    "        \n",
    "  return c_all, s_all, z_all, mus_all, Sigmas_all, pis_all\n",
    "\n",
    "c_all, s_all, z_all, mus_all, Sigmas_all, pis_all = build_toy_dataset(N)\n",
    "\n",
    "cw_train = c_all\n",
    "print('Data shape: %s' % str(cw_train.shape))\n",
    "print('Type of cw: %s' % str(cw_train.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5)\n",
      "c_all: Tensor(\"ContextWindow/sample/stack_20:0\", shape=(10, 3, 5), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "from edward.models import Dirichlet, InverseGamma, MultivariateNormalDiag, \\\n",
    "    Normal, ParamMixture, Categorical, OneHotCategorical\n",
    "\n",
    "from edward.models import RandomVariable\n",
    "from tensorflow.contrib.distributions import Distribution\n",
    "\n",
    "class distributions_ContextWindow(Distribution):\n",
    "  def __init__(self, senses, mus, Sigmas, \n",
    "               validate_args=False,\n",
    "               allow_nan_stats=True,\n",
    "               name=\"ContextWindow\"):\n",
    "    \n",
    "    self.senses = tf.identity(senses, name=\"senses\")\n",
    "    self.mus = tf.identity(mus, name=\"mus\")\n",
    "    self.Sigmas = tf.sqrt(Sigmas, name=\"Sigmas\")\n",
    "    self.pword = tf.ones(self.mus.shape[1]) / self.mus.shape.as_list()[1]\n",
    "        \n",
    "    super(distributions_ContextWindow, self).__init__(\n",
    "            dtype=tf.float32,\n",
    "            reparameterization_type=tf.contrib.distributions.FULLY_REPARAMETERIZED,\n",
    "            validate_args=validate_args,\n",
    "            allow_nan_stats=allow_nan_stats,\n",
    "            name=name,\n",
    "            graph_parents=[self.senses, self.mus, self.Sigmas],\n",
    "            parameters={'senses':senses, 'mus':mus, 'Sigmas':Sigmas},\n",
    "    )\n",
    "        \n",
    "  def _log_prob(self, value=None):\n",
    "    # value has shape C x V\n",
    "    if value is None:\n",
    "        value = self   \n",
    "    \n",
    "    value = tf.to_int32(value)\n",
    "    \n",
    "    print('Value in _log_prob: %s' % str(value))    \n",
    "                \n",
    "    C = value.shape[1]\n",
    "    V = value.shape[2]\n",
    "    \n",
    "    logpc_giv_w = []\n",
    "        \n",
    "    for n in range(value.shape[0]):\n",
    "    \n",
    "        print('computed log_p for sample %i' % n)\n",
    "    \n",
    "        log_p_sample = []\n",
    "        \n",
    "        for w in range(V):\n",
    "            \n",
    "            wc = value[n, :, w] # the context words\n",
    "\n",
    "            cw_sense = tf.gather(self.senses[n, :], wc)\n",
    "            w_sense = self.senses[n, w]\n",
    "\n",
    "            idxs = tf.concat([tf.expand_dims(cw_sense, 1), tf.expand_dims(wc, 1)], axis=1)\n",
    "            mus_cw = tf.gather_nd(self.mus, idxs)\n",
    "            sigma_cw = tf.gather_nd(self.Sigmas, idxs)               \n",
    "                \n",
    "            # here we integrate out zw. Second term penalises uncertainty in zw due to vagueness of w's distribution\n",
    "            logEpz_giv_c = tf.log(MultivariateNormalDiag(\n",
    "                mus_cw, \n",
    "                sigma_cw\n",
    "            ).log_prob(self.mus[w_sense, w])) \n",
    "            \n",
    "            varterm = 0.5 * tf.reduce_sum(self.Sigmas[w_sense, w] / tf.diag_part(sigma_cw))\n",
    "            \n",
    "            # pword terms drop out because they factor out as constants independent of the other variables.\n",
    "            \n",
    "            log_p_joint = logEpz_giv_c - varterm\n",
    "                         \n",
    "            # now need to normalise over the sum_c{Elogpz_and_c} to get Elogp_c_giv_z\n",
    "            # Do this using a sample of the negative words \n",
    "            neg_sample_size = 20\n",
    "            sample = Categorical(probs=self.pword).sample(neg_sample_size)\n",
    "                        \n",
    "            cw_sense = tf.gather(self.senses[n, :], sample)\n",
    "            \n",
    "            idxs = tf.concat([tf.expand_dims(cw_sense, 1), tf.expand_dims(sample, 1)], axis=1)\n",
    "            mus_sample = tf.gather_nd(self.mus, idxs)\n",
    "            sigma_sample = tf.gather_nd(self.Sigmas, idxs)\n",
    "            \n",
    "            # here we integrate out zw. Second term penalises uncertainty in zw due to vagueness of w's distribution\n",
    "            logEpz_neg = MultivariateNormalDiag(\n",
    "                    mus_sample, \n",
    "                    sigma_sample\n",
    "            ).log_prob(self.mus[w_sense, w])\n",
    "            \n",
    "            varterm = 0.5 * tf.reduce_sum(sigma_sample / tf.diag_part(sigma_cw), axis=1)\n",
    "            \n",
    "            Elogpz_neg = logEpz_neg - varterm\n",
    "            \n",
    "            pword_sample = tf.gather(self.pword, sample)\n",
    "            \n",
    "            log_p_joint_neg = logEpz_neg + tf.log(pword_sample)\n",
    "                        \n",
    "            # now scale up log_neg_joint to represent all the negative words\n",
    "            w = 1.0 / ( tf.reduce_sum(pword_sample) )\n",
    "            denominator = tf.log(w * tf.reduce_sum(tf.exp(log_p_joint_neg)))\n",
    "\n",
    "            log_p_joint -= denominator\n",
    "            \n",
    "            log_p_sample.append(log_p_joint)\n",
    "            \n",
    "        log_p_sample = tf.stack(log_p_sample, axis=1)\n",
    "        logpc_giv_w.append(log_p_sample)\n",
    "        \n",
    "    logpc_giv_w = tf.stack(logpc_giv_w, axis=0)\n",
    "    \n",
    "    print('Completed log_p for context window:')\n",
    "    print(logpc_giv_w)\n",
    "    #print(tf.is_nan(logpc_giv_w).eval())\n",
    "    \n",
    "    return logpc_giv_w         \n",
    "    \n",
    "  def _sample_n(self, n, seed=None):\n",
    "   \n",
    "    c_all = []\n",
    "    \n",
    "    context_words = []\n",
    "    \n",
    "    pc_giv_z = []\n",
    "            \n",
    "    for x in range(n):\n",
    "        \n",
    "        pw2 = []\n",
    "        sample_weights = []\n",
    "        \n",
    "        for w in range(V):\n",
    "        \n",
    "            sense_nw = self.senses[x, w]\n",
    "\n",
    "            z_w = MultivariateNormalDiag(\n",
    "                    self.mus[sense_nw, w, :], \n",
    "                    self.Sigmas[sense_nw, w, :]\n",
    "            )\n",
    "                                \n",
    "            neg_sample_size = 20\n",
    "            sample = Categorical(probs=self.pword).sample(neg_sample_size)\n",
    "            sense_w2 = tf.gather(self.senses[x, :], sample)\n",
    "                        \n",
    "            idxs = tf.concat([tf.expand_dims(sense_w2, 1), tf.expand_dims(sample, 1)], axis=1)           \n",
    "            mus_w2 = tf.gather_nd(self.mus, idxs)            \n",
    "            sigmas_w2 = tf.gather_nd(self.Sigmas, idxs)\n",
    "            \n",
    "            pword_w2 = tf.gather(self.pword, sample)\n",
    "\n",
    "            p_z_giv_c = MultivariateNormalDiag(\n",
    "                    loc=mus_w2, # this assumes that the senses were instantiated at random from pi and were different for each word\n",
    "                    scale_diag=sigmas_w2\n",
    "            ).prob(z_w)            \n",
    "            \n",
    "            joint = p_z_giv_c * pword_w2                \n",
    "            pw2.append(joint)\n",
    "            \n",
    "            sample_weights.append(1.0 / tf.reduce_sum(pword_w2))\n",
    "\n",
    "        pw2 = tf.stack(pw2, axis=0)\n",
    "        \n",
    "        sample_weights = tf.expand_dims(tf.stack(sample_weights, axis=0), 1)\n",
    "        \n",
    "        pc_giv_z = pw2 / (sample_weights * tf.reduce_sum(pw2, keepdims=True, axis=1)) # shape VxV\n",
    "\n",
    "        c_sample = Categorical(probs=pc_giv_z).sample(C, seed) # V different distributions in one line. Shape = n x C x N x V\n",
    "        c_all.append(c_sample)\n",
    "        \n",
    "    c_all = tf.stack(c_all, axis=0)\n",
    "\n",
    "    print('c_all: %s' % str(c_all))\n",
    "    return c_all\n",
    "   \n",
    "               \n",
    "class ContextWindow(RandomVariable, distributions_ContextWindow):\n",
    "               \n",
    "  def __init__(self, *args, **kwargs):\n",
    "    RandomVariable.__init__(self, *args, **kwargs)\n",
    "    self.conjugate_log_prob = self._log_prob\n",
    "    \n",
    "pi = Dirichlet(tf.ones(S), sample_shape=V, validate_args=True) # sense distributions for each word. Needs replacing with CRP\n",
    "mu = Normal(tf.zeros(D), tf.ones(D), sample_shape=(S, V), validate_args=True)\n",
    "sigmasq = InverseGamma(tf.ones(D), tf.ones(D), sample_shape=(S, V), validate_args=True)\n",
    "\n",
    "senses = Categorical(probs=pi, sample_shape=(N), validate_args=True)\n",
    "\n",
    "print(senses.shape)\n",
    "\n",
    "cw = ContextWindow(senses, mu, sigmasq, sample_shape=(N), validate_args=True) # result should be N x C x V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(2), Dimension(5), Dimension(3)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmasq.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from edward.models import Empirical\n",
    "\n",
    "T = 500\n",
    "\n",
    "# approximate distributions\n",
    "q_mu = Empirical(\n",
    "    tf.get_variable(\"q_mu/params\", [T, S, V, D], initializer=tf.zeros_initializer() )\n",
    ")\n",
    "\n",
    "q_sigmasq = Empirical(\n",
    "    tf.get_variable(\"q_sigmasq/params\", [T, S, V, D], initializer=tf.ones_initializer() )\n",
    ")\n",
    "\n",
    "q_pi = Empirical(\n",
    "    tf.get_variable(\"q_pi/params\", [T, V, S], initializer=tf.constant_initializer(1.0 / S) )\n",
    ")\n",
    "\n",
    "q_senses = Empirical(\n",
    "    tf.get_variable(\"q_senses/probs\", [T, N, V], initializer=tf.zeros_initializer(), dtype=tf.int32) \n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "latent_vars = {\n",
    "    senses: q_senses,\n",
    "    mu: q_mu,\n",
    "    sigmasq: q_sigmasq,\n",
    "    pi: q_pi, \n",
    "}\n",
    "\n",
    "data = {\n",
    "    cw: cw_train,\n",
    "    #senses:senses_data\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "inference = ed.Gibbs(latent_vars, data=data)\n",
    "inference.initialize()\n",
    "sess = ed.get_session()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for i in range(inference.n_iter):\n",
    "    info_dict = inference.update()\n",
    "    inference.print_progress(info_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approximate distributions\n",
    "q_mu = Normal(\n",
    "    loc=tf.Variable(tf.zeros([S, V, D])),\n",
    "    scale=tf.nn.softplus(tf.Variable(tf.ones([S, V, D]))), validate_args=True\n",
    ")\n",
    "\n",
    "q_sigmasq = InverseGamma(\n",
    "    concentration=tf.nn.softplus(tf.Variable(tf.ones([S, V, D])) ),\n",
    "    rate=tf.nn.softplus(tf.Variable(tf.ones([S, V, D])) ), validate_args=True\n",
    ")\n",
    "\n",
    "q_pi = Dirichlet(\n",
    "    concentration=tf.nn.softplus(tf.Variable(tf.ones([V, S])) ), validate_args=True\n",
    ")\n",
    "\n",
    "q_senses = Categorical(\n",
    "    probs=tf.nn.softmax(tf.Variable(tf.ones([N, V, S])) ), validate_args=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5, 3)\n",
      "(2, 5, 3)\n",
      "(2, 5, 3)\n",
      "(2, 5, 3)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(10, 3, 5)\n",
      "(10, 3, 5)\n",
      "(10, 5)\n",
      "(10, 5)\n"
     ]
    }
   ],
   "source": [
    "print(mu.shape)\n",
    "print(q_mu.shape)\n",
    "\n",
    "print(sigmasq.shape)\n",
    "print(q_sigmasq.shape)\n",
    "\n",
    "print(pi.shape)\n",
    "print(q_pi.shape)\n",
    "\n",
    "print(cw.shape)\n",
    "print(cw_train.shape)\n",
    "\n",
    "print(senses.shape)\n",
    "print(q_senses.shape)\n",
    "\n",
    "latent_vars = {\n",
    "    senses: q_senses,\n",
    "    mu: q_mu,\n",
    "    sigmasq: q_sigmasq,\n",
    "    pi: q_pi, \n",
    "}\n",
    "\n",
    "data = {\n",
    "    cw: cw_train,\n",
    "    #senses: np.array(s_all).T\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edwin/anaconda3/lib/python3.6/site-packages/edward/util/random_variables.py:52: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  not np.issubdtype(value.dtype, np.float) and \\\n",
      "/Users/edwin/anaconda3/lib/python3.6/site-packages/edward/util/random_variables.py:53: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  not np.issubdtype(value.dtype, np.int) and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_all: Tensor(\"inference/sample/ContextWindow/sample_1/stack_20:0\", shape=(10, 3, 5), dtype=int32)\n",
      "Value in _log_prob: Tensor(\"inference/sample/ContextWindow/log_prob/ToInt32:0\", shape=(10, 3, 5), dtype=int32)\n",
      "computed log_p for sample 0\n",
      "computed log_p for sample 1\n",
      "computed log_p for sample 2\n",
      "computed log_p for sample 3\n",
      "computed log_p for sample 4\n",
      "computed log_p for sample 5\n",
      "computed log_p for sample 6\n",
      "computed log_p for sample 7\n",
      "computed log_p for sample 8\n",
      "computed log_p for sample 9\n",
      "Completed log_p for context window:\n",
      "Tensor(\"inference/sample/ContextWindow/log_prob/stack_10:0\", shape=(10, 3, 5), dtype=float32)\n",
      "\n",
      "\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]\n",
      " [0.5 0.5]\n",
      " [0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "\n",
      "\n",
      "[[[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]]\n",
      "\n",
      "\n",
      "[[[4.1922197 4.1922197 4.1922197]\n",
      "  [4.1922197 4.1922197 4.1922197]\n",
      "  [4.1922197 4.1922197 4.1922197]\n",
      "  [4.1922197 4.1922197 4.1922197]\n",
      "  [4.1922197 4.1922197 4.1922197]]\n",
      "\n",
      " [[4.1922197 4.1922197 4.1922197]\n",
      "  [4.1922197 4.1922197 4.1922197]\n",
      "  [4.1922197 4.1922197 4.1922197]\n",
      "  [4.1922197 4.1922197 4.1922197]\n",
      "  [4.1922197 4.1922197 4.1922197]]]\n",
      "\n",
      "\n",
      "WARNING:tensorflow:From /Users/edwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/distributions/categorical.py:314: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "VB iteration 0\n",
      "   1/1000 [  0%]                                ETA: 93561s | Loss: nan\n",
      "\n",
      "[[0.5278072  0.47219288]\n",
      " [0.5278072  0.47219288]\n",
      " [0.5        0.5       ]\n",
      " [0.5        0.5       ]\n",
      " [0.47219288 0.5278072 ]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "assertion failed: [] [Condition x > 0 did not hold element-wise:] [x (Softplus:0) = ] [[[nan nan nan]]...]\n\t [[Node: Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_FLOAT], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert/Switch, Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert/data_0, Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert/data_1, Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert/data_2, Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert/Switch_1)]]\n\nCaused by op 'Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert', defined at:\n  File \"/Users/edwin/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/edwin/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-31b12f73c286>\", line 4, in <module>\n    scale=tf.nn.softplus(tf.Variable(tf.ones([S, V, D]))), validate_args=True\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/edward/models/random_variables.py\", line 21, in __init__\n    _RandomVariable.__init__(self, *args, **kwargs)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/edward/models/random_variable.py\", line 112, in __init__\n    super(RandomVariable, self).__init__(*args, **kwargs)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/distributions/normal.py\", line 137, in __init__\n    validate_args else []):\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py\", line 221, in assert_positive\n    return assert_less(zero, x, data=data, summarize=summarize)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py\", line 579, in assert_less\n    return control_flow_ops.Assert(condition, data, summarize=summarize)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 118, in wrapped\n    return _add_should_use_warning(fn(*args, **kwargs))\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 177, in Assert\n    guarded_assert = cond(condition, no_op, true_assert, name=\"AssertGuard\")\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\n    return func(*args, **kwargs)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2027, in cond\n    orig_res_f, res_f = context_f.BuildCondBranch(false_fn)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1868, in BuildCondBranch\n    original_result = fn()\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 175, in true_assert\n    condition, data, summarize, name=\"Assert\")\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 48, in _assert\n    name=name)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): assertion failed: [] [Condition x > 0 did not hold element-wise:] [x (Softplus:0) = ] [[[nan nan nan]]...]\n\t [[Node: Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_FLOAT], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert/Switch, Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert/data_0, Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert/data_1, Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert/data_2, Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert/Switch_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    517\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: assertion failed: [] [Condition x > 0 did not hold element-wise:] [x (Softplus:0) = ] [[[nan nan nan]]...]\n\t [[Node: Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_FLOAT], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert/Switch, Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert/data_0, Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert/data_1, Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert/data_2, Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert/Switch_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3320449d4fef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_pi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_sigmasq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m     \"\"\"\n\u001b[0;32m--> 656\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   4899\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4900\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 4901\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: assertion failed: [] [Condition x > 0 did not hold element-wise:] [x (Softplus:0) = ] [[[nan nan nan]]...]\n\t [[Node: Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_FLOAT], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert/Switch, Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert/data_0, Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert/data_1, Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert/data_2, Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert/Switch_1)]]\n\nCaused by op 'Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert', defined at:\n  File \"/Users/edwin/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/edwin/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-31b12f73c286>\", line 4, in <module>\n    scale=tf.nn.softplus(tf.Variable(tf.ones([S, V, D]))), validate_args=True\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/edward/models/random_variables.py\", line 21, in __init__\n    _RandomVariable.__init__(self, *args, **kwargs)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/edward/models/random_variable.py\", line 112, in __init__\n    super(RandomVariable, self).__init__(*args, **kwargs)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/distributions/normal.py\", line 137, in __init__\n    validate_args else []):\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py\", line 221, in assert_positive\n    return assert_less(zero, x, data=data, summarize=summarize)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py\", line 579, in assert_less\n    return control_flow_ops.Assert(condition, data, summarize=summarize)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 118, in wrapped\n    return _add_should_use_warning(fn(*args, **kwargs))\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 177, in Assert\n    guarded_assert = cond(condition, no_op, true_assert, name=\"AssertGuard\")\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\n    return func(*args, **kwargs)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2027, in cond\n    orig_res_f, res_f = context_f.BuildCondBranch(false_fn)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1868, in BuildCondBranch\n    original_result = fn()\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 175, in true_assert\n    condition, data, summarize, name=\"Assert\")\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 48, in _assert\n    name=name)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/Users/edwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): assertion failed: [] [Condition x > 0 did not hold element-wise:] [x (Softplus:0) = ] [[[nan nan nan]]...]\n\t [[Node: Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_FLOAT], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert/Switch, Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert/data_0, Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert/data_1, Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert/data_2, Normal_1/assert_positive/assert_less/Assert/AssertGuard/Assert/Switch_1)]]\n"
     ]
    }
   ],
   "source": [
    "#from tensorflow.python import debug as tf_debug\n",
    "\n",
    "#sess = tf_debug.LocalCLIDebugWrapperSession(ed.util.get_session(), ui_type=\"curses\")\n",
    "#sess.add_tensor_filter(\"has_inf_or_nan\", tf_debug.has_inf_or_nan)\n",
    "\n",
    "#with sess.as_default():\n",
    "\n",
    "inference = ed.KLqp(latent_vars, data)\n",
    "\n",
    "n_iter = 100\n",
    "\n",
    "#inference.run(n_iter=n_iter)\n",
    "\n",
    "inference.initialize()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for i in range(n_iter):\n",
    "    print('\\n')\n",
    "    print(q_pi.mean().eval())\n",
    "    print('\\n')\n",
    "    print(q_mu.mean().eval())\n",
    "    print('\\n')\n",
    "    print(q_sigmasq.mean().eval())\n",
    "    print('\\n')\n",
    "    print(q_senses.mode().eval())\n",
    "\n",
    "    print('VB iteration %i' % i)\n",
    "    info_dict = inference.update()\n",
    "    inference.print_progress(info_dict)\n",
    "    \n",
    "inference.finalize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the sense labels inferred for all the central word occurrences\n",
    "\n",
    "Esenses = q_senses.eval()\n",
    "\n",
    "print(Esenses)\n",
    "print(s_all)\n",
    "\n",
    "print(Esenses[0, 0])\n",
    "\n",
    "print(s_all[0][0])\n",
    "\n",
    "for w in range(V):\n",
    "    for n in range(N):    \n",
    "        print('word %i, sample %i, -- probability of senses = %s, true sense is %i' % (n, w, str(Esenses[n, w]), s_all[w][n]) )\n",
    "            "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def build_toy_dataset(N, w, noise_std=0.1):\n",
    "  D = len(w)\n",
    "  x = np.random.randn(N, D)\n",
    "  y = np.dot(x, w) + np.random.normal(0, noise_std, size=N)\n",
    "  return x, y\n",
    "\n",
    "N = 40  # number of data points\n",
    "D = 10  # number of features\n",
    "\n",
    "w_true = np.random.randn(D)\n",
    "X_train, y_train = build_toy_dataset(N, w_true)\n",
    "X_test, y_test = build_toy_dataset(N, w_true)\n",
    "\n",
    "from edward.models import Normal\n",
    "\n",
    "X = tf.placeholder(tf.float32, [N, D])\n",
    "w = Normal(loc=tf.zeros(D), scale=tf.ones(D))\n",
    "b = Normal(loc=tf.zeros(1), scale=tf.ones(1))\n",
    "y = Normal(loc=ed.dot(X, w) + b, scale=tf.ones(N))\n",
    "\n",
    "qw = Normal(loc=tf.get_variable(\"qw/loc\", [D]),\n",
    "            scale=tf.nn.softplus(tf.get_variable(\"qw/scale\", [D])))\n",
    "qb = Normal(loc=tf.get_variable(\"qb/loc\", [1]),\n",
    "            scale=tf.nn.softplus(tf.get_variable(\"qb/scale\", [1])))\n",
    "\n",
    "inference = ed.KLqp({w: qw, b: qb}, data={X: X_train, y: y_train})\n",
    "inference.run(n_samples=5, n_iter=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### What makes our method novel?\n",
    "\n",
    "A fully Bayesian approach to learning word embeddings with multiple, potentially infinite numbers of distinct senses per token.\n",
    "\n",
    "The Bayesian treatment is intended to help with:\n",
    "* Rare words in the training corpus, whose embeddings cannot be confidently estimated -- variance means we don't put too much weight onto these uncertain cases during learning\n",
    "* Inferring the number of senses -- priors effectively regularise the model toward fewer senses\n",
    "* Domain adaptation/Transfer learning -- we can inflate variances to indicate uncertainty in new domains\n",
    "* (As in Barkan, Brazinskas et al) context-specific embeddings for each word instance\n",
    "* (As in Barkan, Brazinskas et al) composition of sentence or document embeddings -- word occurrences with more confident or precise embeddings will have stronger influence in the combined sentence embedding. I think this will push the sentence embeddings away from generic words and toward the extremes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we test our model?\n",
    "\n",
    "* Look at the tasks tried by Brazinskas et al, Barkan, and the ACL 2018 paper and ty to reuse their code where possible\n",
    "* Compute the context-specific embeddings for each word (posterior means)\n",
    "* Test what happens if we concatenate the variances to the embedding vector as a vagueness or uncertainty feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_senses.mode().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
