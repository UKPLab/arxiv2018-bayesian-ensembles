{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: change the data structure and context window distribution so that we can have different numbers of context windows per word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Value error parsing header in AFM: b'Notice' b'\\xa9 Copyright SoftMaker Software GmbH and its licensors'\n",
      "Value error parsing header in AFM: b'Notice' b'\\xa9 Copyright SoftMaker Software GmbH and its licensors'\n",
      "Value error parsing header in AFM: b'Notice' b'\\xa9 Copyright SoftMaker Software GmbH and its licensors'\n"
     ]
    }
   ],
   "source": [
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.stats import multivariate_normal as mvn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = 3 # dimensionality of the embeddings\n",
    "S = 2 # number of different senses per word. TODO: make this random for each word\n",
    "V = 5 # vocabulary size\n",
    "N = 6 # number of context windows for each word in the vocabulary\n",
    "C = 10 # context window size\n",
    "\n",
    "tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0. Sense distribution = [0.29787045 0.70212955]\n",
      "Word 1. Sense distribution = [0.45077866 0.54922134]\n",
      "Word 2. Sense distribution = [0.40266363 0.59733637]\n",
      "Word 3. Sense distribution = [0.77429513 0.22570487]\n",
      "Word 4. Sense distribution = [0.82309081 0.17690919]\n",
      "Data shape: (6, 10, 5)\n",
      "Type of cw: int64\n"
     ]
    }
   ],
   "source": [
    "def build_toy_dataset(N):\n",
    "  mu0 = np.zeros(D) # prior mean of the sense means\n",
    "  sigma0 = np.ones(D) # prior covariance of the sense means\n",
    "  a0 = np.ones(D) # prior shape for the scale of the sense covariance\n",
    "  b0 = np.ones(D) # prior scale for the scale of the sense covariance   \n",
    "  alpha0 = np.ones(S) # priors for the sense probabilities\n",
    "  beta0 = np.ones(V) # priors over the word frequencies\n",
    "    \n",
    "  mus_all = []\n",
    "  Sigmas_all = []\n",
    "  pis_all = []\n",
    "\n",
    "  # draw word frequencies\n",
    "  pword = np.random.dirichlet(beta0)\n",
    "  \n",
    "  # draw the word sense distributions\n",
    "  for w in range(V):\n",
    "    mus = []\n",
    "    Sigmas = []\n",
    "    pis = np.random.dirichlet(alpha0)\n",
    "    \n",
    "    # draw the means for each sense\n",
    "    for s in range(S):\n",
    "        mus.append(np.random.multivariate_normal(mu0, np.diag(sigma0)))\n",
    "        Sigmas.append(np.diag(1.0 / np.random.gamma(a0, b0)))\n",
    "        \n",
    "    mus_all.append(mus)\n",
    "    Sigmas_all.append(Sigmas)\n",
    "    pis_all.append(pis)\n",
    "    \n",
    "  s_all = []\n",
    "  z_all = []\n",
    "  c_all = []\n",
    "    \n",
    "  # draw the context windows for each word\n",
    "  for w in range(V):\n",
    "    \n",
    "    # draw the sense for each context\n",
    "    print('Word %i. Sense distribution = %s' % (w, str(pis_all[w])))\n",
    "    s_ws = np.argmax(np.random.multinomial(1, pis_all[w], N), 1)\n",
    "    z_ws = []\n",
    "    \n",
    "    c_ws = []\n",
    "    \n",
    "    for n in range(N):\n",
    "        \n",
    "        #print('Sense for pair n = %i' % s_ws[n])\n",
    "        #print('Mean for chosen sense = %s' % str(mus_all[w][s_ws[n]]))\n",
    "        #print('Cov for chosen sense = %s' % str(Sigmas_all[w][s_ws[n]]))\n",
    "        \n",
    "        # draw the embedding for each context\n",
    "        z_ws.append(np.random.multivariate_normal(mus_all[w][s_ws[n]], Sigmas_all[w][s_ws[n]]))\n",
    "    \n",
    "        # construct the categorical distribution over all words\n",
    "        joint = []\n",
    "        for w2 in range(V):\n",
    "            \n",
    "            pw2 = 0\n",
    "            for s in range(S):\n",
    "                pw2 += pis_all[w2][s] * mvn.pdf(z_ws[-1], mus_all[w2][s], Sigmas_all[w2][s])\n",
    "            pw2 *= pword[w2]\n",
    "            joint.append(pw2)\n",
    "        \n",
    "        pc_giv_z = joint / np.sum(joint)\n",
    "        \n",
    "        c = np.argmax(np.random.multinomial(1, pc_giv_z, C), 1)\n",
    "        c_ws.append(c)\n",
    "        \n",
    "    s_all.append(s_ws)\n",
    "    z_all.append(z_ws)\n",
    "    c_all.append(c_ws)\n",
    "        \n",
    "  c_all = np.array(c_all, dtype=int).swapaxes(0, 2).swapaxes(0, 1) # so we get N x C x V from V x N x C\n",
    "        \n",
    "  return c_all, s_all, z_all, mus_all, Sigmas_all, pis_all\n",
    "\n",
    "c_all, s_all, z_all, mus_all, Sigmas_all, pis_all = build_toy_dataset(N)\n",
    "\n",
    "cw_train = c_all\n",
    "print('Data shape: %s' % str(cw_train.shape))\n",
    "print('Type of cw: %s' % str(cw_train.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_all: Tensor(\"ContextWindow/sample/stack_36:0\", shape=(6, 10, 5), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "from edward.models import Dirichlet, InverseGamma, MultivariateNormalDiag, \\\n",
    "    Normal, ParamMixture, Categorical\n",
    "\n",
    "from edward.models import RandomVariable\n",
    "from tensorflow.contrib.distributions import Distribution\n",
    "\n",
    "class distributions_ContextWindow(Distribution):\n",
    "  def __init__(self, senses, mus, Sigmas, pword, \n",
    "               validate_args=False,\n",
    "               allow_nan_stats=True,\n",
    "               name=\"ContextWindow\"):\n",
    "    \n",
    "    self.senses = senses\n",
    "    self.mus = mus\n",
    "    self.Sigmas = Sigmas\n",
    "    self.pword = pword\n",
    "        \n",
    "    super(distributions_ContextWindow, self).__init__(\n",
    "            dtype=tf.float32,\n",
    "            reparameterization_type=tf.contrib.distributions.FULLY_REPARAMETERIZED,\n",
    "            validate_args=validate_args,\n",
    "            allow_nan_stats=allow_nan_stats,\n",
    "            name=name,\n",
    "            #graph_parents=[pis, mus, Sigmas, pword],\n",
    "            parameters={'senses':senses, 'mus':mus, 'Sigmas':Sigmas, 'pword':pword},\n",
    "    )\n",
    "        \n",
    "  def _log_prob(self, value):\n",
    "    # value has shape C x V\n",
    "    \n",
    "    value = tf.to_int32(value)\n",
    "    \n",
    "    print('Value in _log_prob: %s' % str(value))    \n",
    "            \n",
    "    logpc_giv_w = 0\n",
    "    \n",
    "    C = value.shape[1]\n",
    "    V = value.shape[2]\n",
    "    \n",
    "    for n in range(value.shape[0]):\n",
    "    \n",
    "        for w in range(V):\n",
    "\n",
    "            for c in range(C): # prob of each context word of w, integrating over possible senses and embeddings\n",
    "                wc = value[n, c, w] # the context word\n",
    "\n",
    "                Sigma = 1.0 / (1.0 / self.Sigmas[self.senses[n, wc], wc, :] + 1.0 / self.Sigmas[self.senses[n, w], w, :])\n",
    "\n",
    "                logpc_giv_w +=  MultivariateNormalDiag(\n",
    "                    (self.mus[self.senses[n, wc], wc, :] / self.Sigmas[self.senses[n, wc], wc, :] + \n",
    "                    self.mus[self.senses[n, w], w, :] / self.Sigmas[self.senses[n, w], w, :]) / Sigma, \n",
    "                    Sigma\n",
    "                    # here we integrate out the \n",
    "                )._log_prob(self.mus[self.senses[n, wc], wc, :])\n",
    "\n",
    "                logpc_giv_w += tf.log(self.pword[wc])\n",
    "            \n",
    "    return logpc_giv_w         \n",
    "    \n",
    "  def _sample_n(self, n, seed=None):\n",
    "   \n",
    "    c_all = []\n",
    "    \n",
    "    context_words = []\n",
    "    \n",
    "    pc_giv_z = []\n",
    "    \n",
    "    senses = self.senses\n",
    "        \n",
    "    for x in range(n):\n",
    "        \n",
    "        pw2 = []\n",
    "        \n",
    "        for w in range(V):\n",
    "        \n",
    "            sense_nw = senses[x, w]\n",
    "\n",
    "            z_w = MultivariateNormalDiag(\n",
    "                    self.mus[sense_nw, w, :], \n",
    "                    self.Sigmas[sense_nw, w, :]\n",
    "            )            \n",
    "            joint = []\n",
    "            \n",
    "            for w2 in range(V):\n",
    "                mvn = MultivariateNormalDiag(\n",
    "                    loc=self.mus[senses[x, w2], w2, :], # this assumes that the senses were instantiated at random from pi and were different for each word\n",
    "                    scale_diag=self.Sigmas[senses[x, w2], w2, :]\n",
    "                )\n",
    "            \n",
    "                # Get a V x 1 vector of probabilities\n",
    "                p_z_giv_c = mvn.prob(z_w)\n",
    "                joint.append(p_z_giv_c)\n",
    "            \n",
    "            joint = tf.stack(joint, axis=0) * self.pword\n",
    "            pw2.append(joint)\n",
    "\n",
    "        pw2 = tf.stack(pw2, axis=0)\n",
    "        pc_giv_z = pw2 / tf.reduce_sum(pw2, keepdims=True, axis=1) # shape VxV\n",
    "\n",
    "        c_sample = Categorical(probs=pc_giv_z).sample(C, seed) # V different distributions in one line. Shape = n x C x N x V\n",
    "        c_all.append(c_sample)\n",
    "        \n",
    "    c_all = tf.stack(c_all, axis=0)\n",
    "\n",
    "    print('c_all: %s' % str(c_all))\n",
    "    return c_all\n",
    "   \n",
    "               \n",
    "class ContextWindow(RandomVariable, distributions_ContextWindow):\n",
    "               \n",
    "  def __init__(self, *args, **kwargs):\n",
    "    RandomVariable.__init__(self, *args, **kwargs)\n",
    "    self.conjugate_log_prob = self._log_prob\n",
    "    \n",
    "pword = Dirichlet(tf.ones(V))\n",
    "\n",
    "pi = Dirichlet(tf.ones(S), sample_shape=V) # sense distributions for each word. Needs replacing with CRP\n",
    "mu = Normal(tf.zeros(D), tf.ones(D), sample_shape=(S, V))\n",
    "sigmasq = InverseGamma(tf.ones(D), tf.ones(D), sample_shape=(S, V))\n",
    "\n",
    "senses = Categorical(pi, sample_shape=(N))\n",
    "cw = ContextWindow(senses, mu, sigmasq, pword, sample_shape=(N)) # result should be N x C x V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from edward.models import Empirical\n",
    "\n",
    "T = 500\n",
    "\n",
    "# approximate distributions\n",
    "q_mu = Empirical(\n",
    "    tf.get_variable(\"q_mu/params\", [T, S, V, D], initializer=tf.zeros_initializer() )\n",
    ")\n",
    "\n",
    "q_sigmasq = Empirical(\n",
    "    tf.get_variable(\"q_sigmasq/params\", [T, S, V, D], initializer=tf.ones_initializer() )\n",
    ")\n",
    "\n",
    "q_pi = Empirical(\n",
    "    tf.get_variable(\"q_pi/params\", [T, V, S], initializer=tf.constant_initializer(1.0 / S) )\n",
    ")\n",
    "\n",
    "q_pword = Empirical(\n",
    "    tf.get_variable(\"q_pword/params\", [T, V], initializer=tf.constant_initializer(1.0 / V) )\n",
    ")\n",
    "\n",
    "q_senses = Empirical(\n",
    "    tf.get_variable(\"q_senses/params\", [T, V, S], initializer=tf.zeros_initializer(), dtype=tf.int32) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "(5,)\n",
      "(2, 5, 3)\n",
      "(2, 5, 3)\n",
      "(2, 5, 3)\n",
      "(2, 5, 3)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(6, 10, 5)\n",
      "(6, 10, 5)\n",
      "(6, 5)\n",
      "(5, 2)\n"
     ]
    }
   ],
   "source": [
    "print(pword.shape)\n",
    "print(q_pword.shape)\n",
    "\n",
    "print(mu.shape)\n",
    "print(q_mu.shape)\n",
    "\n",
    "print(sigmasq.shape)\n",
    "print(q_sigmasq.shape)\n",
    "\n",
    "print(pi.shape)\n",
    "print(q_pi.shape)\n",
    "\n",
    "print(cw.shape)\n",
    "print(cw_train.shape)\n",
    "\n",
    "print(senses.shape)\n",
    "print(q_senses.shape)\n",
    "\n",
    "latent_vars = {\n",
    "    senses: q_senses,\n",
    "    mu: q_mu,\n",
    "    sigmasq: q_sigmasq,\n",
    "    pi: q_pi, \n",
    "    pword: q_pword\n",
    "}\n",
    "\n",
    "data = {\n",
    "    cw: cw_train,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/edward/inferences/conjugacy/conjugacy.py\u001b[0m in \u001b[0;36mget_log_joint\u001b[0;34m(cond_set)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;31m# Use log prob tensor if already built in graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mconjugate_log_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m':0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m       \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_tensor_by_name\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3634\u001b[0m                       type(name).__name__)\n\u001b[0;32m-> 3635\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_graph_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3458\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3459\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3500\u001b[0m                          \u001b[0;34m\"exist. The operation, %s, does not exist in the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3501\u001b[0;31m                          \"graph.\" % (repr(name), repr(op_name)))\n\u001b[0m\u001b[1;32m   3502\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"The name 'conjugate_log_joint/Categorical/_conjugate_log_prob:0' refers to a Tensor which does not exist. The operation, 'conjugate_log_joint/Categorical/_conjugate_log_prob', does not exist in the graph.\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e4abe52bcdd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0med\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGibbs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/edward/inferences/gibbs.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, latent_vars, proposal_vars, data)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mproposal_vars\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m       proposal_vars = {z: complete_conditional(z)\n\u001b[0;32m---> 46\u001b[0;31m                        for z in six.iterkeys(latent_vars)}\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m       \u001b[0mcheck_latent_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproposal_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/edward/inferences/gibbs.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mproposal_vars\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m       proposal_vars = {z: complete_conditional(z)\n\u001b[0;32m---> 46\u001b[0;31m                        for z in six.iterkeys(latent_vars)}\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m       \u001b[0mcheck_latent_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproposal_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/edward/inferences/conjugacy/conjugacy.py\u001b[0m in \u001b[0;36mcomplete_conditional\u001b[0;34m(rv, cond_set)\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'complete_conditional_%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;31m# log_joint holds all the information we need to get a conditional.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mlog_joint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_log_joint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;31m# Pull out the nodes that are nonlinear functions of rv into s_stats.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/edward/inferences/conjugacy/conjugacy.py\u001b[0m in \u001b[0;36mget_log_joint\u001b[0;34m(cond_set)\u001b[0m\n\u001b[1;32m    189\u001b[0m           raise NotImplementedError(\"conjugate_log_prob not implemented for\"\n\u001b[1;32m    190\u001b[0m                                     \" {}\".format(type(b)))\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mconjugate_log_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconjugate_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mterms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconjugate_log_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/edward/inferences/conjugacy/conjugate_log_probs.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, val)\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/edward/inferences/conjugacy/conjugate_log_probs.py\u001b[0m in \u001b[0;36mcategorical_log_prob\u001b[0;34m(self, val)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcategorical_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'probs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0mone_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "inference = ed.Gibbs(latent_vars, data=data)\n",
    "inference.initialize()\n",
    "sess = edward.get_session()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for i in range(inference.niter):\n",
    "    info_dict = inference.update()\n",
    "    inference.print_progress(info_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# approximate distributions\n",
    "q_mu = Normal(\n",
    "    loc=tf.Variable(tf.zeros([S, V, D])),\n",
    "    scale=tf.Variable(tf.zeros([S, V, D]))\n",
    ")\n",
    "\n",
    "q_sigmasq = InverseGamma(\n",
    "    concentration=tf.Variable(tf.zeros([S, V, D])),\n",
    "    rate=tf.Variable(tf.zeros([S, V, D]))\n",
    ")\n",
    "\n",
    "q_pi = Dirichlet(\n",
    "    concentration=tf.Variable(tf.zeros([V, S]))\n",
    ")\n",
    "\n",
    "q_pword = Dirichlet(\n",
    "    concentration=tf.Variable(tf.zeros([V]))\n",
    ")\n",
    "\n",
    "q_senses = Categorical(\n",
    "    probs=tf.Variable(tf.zeros([N, V, S]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(pword.shape)\n",
    "print(q_pword.shape)\n",
    "\n",
    "print(mu.shape)\n",
    "print(q_mu.shape)\n",
    "\n",
    "print(sigmasq.shape)\n",
    "print(q_sigmasq.shape)\n",
    "\n",
    "print(pi.shape)\n",
    "print(q_pi.shape)\n",
    "\n",
    "print(cw.shape)\n",
    "print(cw_train.shape)\n",
    "\n",
    "print(senses.shape)\n",
    "print(q_senses.shape)\n",
    "\n",
    "latent_vars = {\n",
    "    senses: q_senses,\n",
    "    mu: q_mu,\n",
    "    sigmasq: q_sigmasq,\n",
    "    pi: q_pi, \n",
    "    pword: q_pword\n",
    "}\n",
    "\n",
    "data = {\n",
    "    cw: cw_train,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inference = ed.KLqp(latent_vars, data)\n",
    "\n",
    "n_iter = 100\n",
    "\n",
    "#inference.run(n_iter=n_iter)\n",
    "\n",
    "inference.initialize()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for i in range(n_iter):\n",
    "    print('VB iteration %i' % i)\n",
    "    info_dict = inference.update()\n",
    "    inference.print_progress(info_dict)\n",
    "    print(q_pi.mean().eval())\n",
    "    \n",
    "inference.finalize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print out the sense labels inferred for all the central word occurrences\n",
    "\n",
    "Esenses = q_senses.eval()\n",
    "\n",
    "for w in range(V):\n",
    "    for n in range(N):    \n",
    "        print('word %i, sample %i, -- probability of senses = %s, true sense is %i' % (n, w, str(Esenses[n, w, s]), s_all[w, n]) )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_toy_dataset(N, w, noise_std=0.1):\n",
    "  D = len(w)\n",
    "  x = np.random.randn(N, D)\n",
    "  y = np.dot(x, w) + np.random.normal(0, noise_std, size=N)\n",
    "  return x, y\n",
    "\n",
    "N = 40  # number of data points\n",
    "D = 10  # number of features\n",
    "\n",
    "w_true = np.random.randn(D)\n",
    "X_train, y_train = build_toy_dataset(N, w_true)\n",
    "X_test, y_test = build_toy_dataset(N, w_true)\n",
    "\n",
    "from edward.models import Normal\n",
    "\n",
    "X = tf.placeholder(tf.float32, [N, D])\n",
    "w = Normal(loc=tf.zeros(D), scale=tf.ones(D))\n",
    "b = Normal(loc=tf.zeros(1), scale=tf.ones(1))\n",
    "y = Normal(loc=ed.dot(X, w) + b, scale=tf.ones(N))\n",
    "\n",
    "qw = Normal(loc=tf.get_variable(\"qw/loc\", [D]),\n",
    "            scale=tf.nn.softplus(tf.get_variable(\"qw/scale\", [D])))\n",
    "qb = Normal(loc=tf.get_variable(\"qb/loc\", [1]),\n",
    "            scale=tf.nn.softplus(tf.get_variable(\"qb/scale\", [1])))\n",
    "\n",
    "inference = ed.KLqp({w: qw, b: qb}, data={X: X_train, y: y_train})\n",
    "inference.run(n_samples=5, n_iter=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### What makes our method novel?\n",
    "\n",
    "A fully Bayesian approach to learning word embeddings with multiple, potentially infinite numbers of distinct senses per token.\n",
    "\n",
    "The Bayesian treatment is intended to help with:\n",
    "* Rare words in the training corpus, whose embeddings cannot be confidently estimated -- variance means we don't put too much weight onto these uncertain cases during learning\n",
    "* Inferring the number of senses -- priors effectively regularise the model toward fewer senses\n",
    "* Domain adaptation/Transfer learning -- we can inflate variances to indicate uncertainty in new domains\n",
    "* (As in Barkan, Brazinskas et al) context-specific embeddings for each word instance\n",
    "* (As in Barkan, Brazinskas et al) composition of sentence or document embeddings -- word occurrences with more confident or precise embeddings will have stronger influence in the combined sentence embedding. I think this will push the sentence embeddings away from generic words and toward the extremes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we test our model?\n",
    "\n",
    "* Look at the tasks tried by Brazinskas et al, Barkan, and the ACL 2018 paper and ty to reuse their code where possible\n",
    "* Compute the context-specific embeddings for each word (posterior means)\n",
    "* Test what happens if we concatenate the variances to the embedding vector as a vagueness or uncertainty feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bac",
   "language": "python",
   "name": "bac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
