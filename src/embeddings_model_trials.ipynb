{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Value error parsing header in AFM: b'Notice' b'\\xa9 Copyright SoftMaker Software GmbH and its licensors'\n",
      "Value error parsing header in AFM: b'Notice' b'\\xa9 Copyright SoftMaker Software GmbH and its licensors'\n",
      "Value error parsing header in AFM: b'Notice' b'\\xa9 Copyright SoftMaker Software GmbH and its licensors'\n"
     ]
    }
   ],
   "source": [
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.stats import multivariate_normal as mvn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = 3 # dimensionality of the embeddings\n",
    "S = 2 # number of different senses per word. TODO: make this random for each word\n",
    "V = 5 # vocabulary size\n",
    "N = 6 # number of context windows for each word in the vocabulary\n",
    "C = 10 # context window size\n",
    "\n",
    "tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0. Sense distribution = [0.93788741 0.06211259]\n",
      "Word 1. Sense distribution = [0.22957736 0.77042264]\n",
      "Word 2. Sense distribution = [0.9560311 0.0439689]\n",
      "Word 3. Sense distribution = [0.16343555 0.83656445]\n",
      "Word 4. Sense distribution = [0.06553183 0.93446817]\n",
      "Data shape: (6, 10, 5)\n",
      "Type of cw: int64\n"
     ]
    }
   ],
   "source": [
    "def build_toy_dataset(N):\n",
    "  mu0 = np.zeros(D) # prior mean of the sense means\n",
    "  sigma0 = np.ones(D) # prior covariance of the sense means\n",
    "  a0 = np.ones(D) # prior shape for the scale of the sense covariance\n",
    "  b0 = np.ones(D) # prior scale for the scale of the sense covariance   \n",
    "  alpha0 = np.ones(S) # priors for the sense probabilities\n",
    "  beta0 = np.ones(V) # priors over the word frequencies\n",
    "    \n",
    "  mus_all = []\n",
    "  Sigmas_all = []\n",
    "  pis_all = []\n",
    "\n",
    "  # draw word frequencies\n",
    "  pword = np.random.dirichlet(beta0)\n",
    "  \n",
    "  # draw the word sense distributions\n",
    "  for w in range(V):\n",
    "    mus = []\n",
    "    Sigmas = []\n",
    "    pis = np.random.dirichlet(alpha0)\n",
    "    \n",
    "    # draw the means for each sense\n",
    "    for s in range(S):\n",
    "        mus.append(np.random.multivariate_normal(mu0, np.diag(sigma0)))\n",
    "        Sigmas.append(np.diag(1.0 / np.random.gamma(a0, b0)))\n",
    "        \n",
    "    mus_all.append(mus)\n",
    "    Sigmas_all.append(Sigmas)\n",
    "    pis_all.append(pis)\n",
    "    \n",
    "  s_all = []\n",
    "  z_all = []\n",
    "  c_all = []\n",
    "    \n",
    "  # draw the context windows for each word\n",
    "  for w in range(V):\n",
    "    \n",
    "    # draw the sense for each context\n",
    "    print('Word %i. Sense distribution = %s' % (w, str(pis_all[w])))\n",
    "    s_ws = np.argmax(np.random.multinomial(1, pis_all[w], N), 1)\n",
    "    z_ws = []\n",
    "    \n",
    "    c_ws = []\n",
    "    \n",
    "    for n in range(N):\n",
    "        \n",
    "        #print('Sense for pair n = %i' % s_ws[n])\n",
    "        #print('Mean for chosen sense = %s' % str(mus_all[w][s_ws[n]]))\n",
    "        #print('Cov for chosen sense = %s' % str(Sigmas_all[w][s_ws[n]]))\n",
    "        \n",
    "        # draw the embedding for each context\n",
    "        z_ws.append(np.random.multivariate_normal(mus_all[w][s_ws[n]], Sigmas_all[w][s_ws[n]]))\n",
    "    \n",
    "        # construct the categorical distribution over all words\n",
    "        joint = []\n",
    "        for w2 in range(V):\n",
    "            \n",
    "            pw2 = 0\n",
    "            for s in range(S):\n",
    "                pw2 += pis_all[w2][s] * mvn.pdf(z_ws[-1], mus_all[w2][s], Sigmas_all[w2][s])\n",
    "            pw2 *= pword[w2]\n",
    "            joint.append(pw2)\n",
    "        \n",
    "        pc_giv_z = joint / np.sum(joint)\n",
    "        \n",
    "        c = np.argmax(np.random.multinomial(1, pc_giv_z, C), 1)\n",
    "        c_ws.append(c)\n",
    "        \n",
    "    s_all.append(s_ws)\n",
    "    z_all.append(z_ws)\n",
    "    c_all.append(c_ws)\n",
    "        \n",
    "  c_all = np.array(c_all, dtype=int).swapaxes(0, 2).swapaxes(0, 1) # so we get N x C x V from V x N x C\n",
    "        \n",
    "  return c_all, s_all, z_all, mus_all, Sigmas_all, pis_all\n",
    "\n",
    "c_all, s_all, z_all, mus_all, Sigmas_all, pis_all = build_toy_dataset(N)\n",
    "\n",
    "cw_train = c_all\n",
    "print('Data shape: %s' % str(cw_train.shape))\n",
    "print('Type of cw: %s' % str(cw_train.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_all: Tensor(\"ContextWindow/sample/stack_36:0\", shape=(6, 10, 5), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "from edward.models import Dirichlet, InverseGamma, MultivariateNormalDiag, \\\n",
    "    Normal, ParamMixture, Categorical\n",
    "\n",
    "from edward.models import RandomVariable\n",
    "from tensorflow.contrib.distributions import Distribution\n",
    "\n",
    "class distributions_ContextWindow(Distribution):\n",
    "  def __init__(self, senses, mus, Sigmas, pword, \n",
    "               validate_args=False,\n",
    "               allow_nan_stats=True,\n",
    "               name=\"ContextWindow\"):\n",
    "    \n",
    "    self.senses = senses\n",
    "    self.mus = mus\n",
    "    self.Sigmas = Sigmas\n",
    "    self.pword = pword\n",
    "        \n",
    "    super(distributions_ContextWindow, self).__init__(\n",
    "            dtype=tf.float32,\n",
    "            reparameterization_type=tf.contrib.distributions.FULLY_REPARAMETERIZED,\n",
    "            validate_args=validate_args,\n",
    "            allow_nan_stats=allow_nan_stats,\n",
    "            name=name,\n",
    "            #graph_parents=[pis, mus, Sigmas, pword],\n",
    "            parameters={'senses':senses, 'mus':mus, 'Sigmas':Sigmas, 'pword':pword},\n",
    "    )\n",
    "    \n",
    "    \n",
    "  def _log_prob(self, value):\n",
    "    # value has shape C x V\n",
    "    \n",
    "    value = tf.to_int32(value)\n",
    "    \n",
    "    print('Value in _log_prob: %s' % str(value))    \n",
    "            \n",
    "    logpc_giv_w = 0\n",
    "    \n",
    "    C = value.shape[1]\n",
    "    V = value.shape[2]\n",
    "    \n",
    "    for n in range(value.shape[0]):\n",
    "    \n",
    "        for w in range(V):\n",
    "\n",
    "            for c in range(C): # prob of each context word of w, integrating over possible senses and embeddings\n",
    "                wc = value[n, c, w] # the context word\n",
    "\n",
    "                Sigma = 1.0 / (1.0 / self.Sigmas[self.senses[n, wc], wc, :] + 1.0 / self.Sigmas[self.senses[n, w], w, :])\n",
    "\n",
    "                logpc_giv_w +=  MultivariateNormalDiag(\n",
    "                    (self.mus[self.senses[n, wc], wc, :] / self.Sigmas[self.senses[n, wc], wc, :] + \n",
    "                    self.mus[self.senses[n, w], w, :] / self.Sigmas[self.senses[n, w], w, :]) / Sigma, \n",
    "                    Sigma\n",
    "                    # here we integrate out the \n",
    "                )._log_prob(self.mus[self.senses[n, wc], wc, :])\n",
    "\n",
    "                logpc_giv_w += tf.log(self.pword[wc])\n",
    "            \n",
    "    return logpc_giv_w         \n",
    "    \n",
    "  def _sample_n(self, n, seed=None):\n",
    "   \n",
    "    c_all = []\n",
    "    \n",
    "    context_words = []\n",
    "    \n",
    "    pc_giv_z = []\n",
    "    \n",
    "    senses = self.senses\n",
    "        \n",
    "    for x in range(n):\n",
    "        \n",
    "        pw2 = []\n",
    "        \n",
    "        for w in range(V):\n",
    "        \n",
    "            sense_nw = senses[x, w]\n",
    "\n",
    "            z_w = MultivariateNormalDiag(\n",
    "                    self.mus[sense_nw, w, :], \n",
    "                    self.Sigmas[sense_nw, w, :]\n",
    "            )            \n",
    "            joint = []\n",
    "            \n",
    "            for w2 in range(V):\n",
    "                mvn = MultivariateNormalDiag(\n",
    "                    loc=self.mus[senses[x, w2], w2, :], # this assumes that the senses were instantiated at random from pi and were different for each word\n",
    "                    scale_diag=self.Sigmas[senses[x, w2], w2, :]\n",
    "                )\n",
    "            \n",
    "                # Get a V x 1 vector of probabilities\n",
    "                p_z_giv_c = mvn.prob(z_w)\n",
    "                joint.append(p_z_giv_c)\n",
    "            \n",
    "            joint = tf.stack(joint, axis=0) * self.pword\n",
    "            pw2.append(joint)\n",
    "\n",
    "        pw2 = tf.stack(pw2, axis=0)\n",
    "        pc_giv_z = pw2 / tf.reduce_sum(pw2, keepdims=True, axis=1) # shape VxV\n",
    "\n",
    "        c_sample = Categorical(probs=pc_giv_z).sample(C, seed) # V different distributions in one line. Shape = n x C x N x V\n",
    "        c_all.append(c_sample)\n",
    "        \n",
    "    c_all = tf.stack(c_all, axis=0)\n",
    "\n",
    "    print('c_all: %s' % str(c_all))\n",
    "    return c_all\n",
    "   \n",
    "               \n",
    "class ContextWindow(RandomVariable, distributions_ContextWindow):\n",
    "               \n",
    "  def __init__(self, *args, **kwargs):\n",
    "    RandomVariable.__init__(self, *args, **kwargs)\n",
    "    \n",
    "pword = Dirichlet(tf.ones(V))\n",
    "\n",
    "pi = Dirichlet(tf.ones(S), sample_shape=V) # sense distributions for each word. Needs replacing with CRP\n",
    "mu = Normal(tf.zeros(D), tf.ones(D), sample_shape=(S, V))\n",
    "sigmasq = InverseGamma(tf.ones(D), tf.ones(D), sample_shape=(S, V))\n",
    "\n",
    "senses = Categorical(pi, sample_shape=(N))\n",
    "cw = ContextWindow(senses, mu, sigmasq, pword, sample_shape=(N)) # result should be C x N x V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# approximate distributions\n",
    "q_mu = Normal(\n",
    "    loc=tf.Variable(tf.zeros([S, V, D])),\n",
    "    scale=tf.Variable(tf.zeros([S, V, D]))\n",
    ")\n",
    "\n",
    "q_sigmasq = InverseGamma(\n",
    "    concentration=tf.Variable(tf.zeros([S, V, D])),\n",
    "    rate=tf.Variable(tf.zeros([S, V, D]))\n",
    ")\n",
    "\n",
    "q_pi = Dirichlet(\n",
    "    concentration=tf.Variable(tf.zeros([V, S]))\n",
    ")\n",
    "\n",
    "q_pword = Dirichlet(\n",
    "    concentration=tf.Variable(tf.zeros([V]))\n",
    ")\n",
    "\n",
    "q_senses = Categorical(\n",
    "    probs=tf.Variable(tf.zeros([N, V, S]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "(5,)\n",
      "(2, 5, 3)\n",
      "(2, 5, 3)\n",
      "(2, 5, 3)\n",
      "(2, 5, 3)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(6, 10, 5)\n",
      "(6, 10, 5)\n",
      "(6, 5)\n",
      "(6, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/edward/util/random_variables.py:52: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  not np.issubdtype(value.dtype, np.float) and \\\n",
      "/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/edward/util/random_variables.py:53: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  not np.issubdtype(value.dtype, np.int) and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_all: Tensor(\"inference_1/sample_1/ContextWindow/sample_1/stack_36:0\", shape=(6, 10, 5), dtype=int32)\n",
      "Value in _log_prob: Tensor(\"inference_1/sample_1/ContextWindow/log_prob/ToInt32:0\", shape=(6, 10, 5), dtype=int32)\n",
      "VB iteration 0\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Received a label value of 2 which is outside the valid range of [0, 2).  Label values: 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n\t [[Node: inference_1/sample_1/Categorical_2/log_prob/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](inference_1/sample_1/Categorical_2/log_prob/SparseSoftmaxCrossEntropyWithLogits/Reshape, inference_1/sample_1/Categorical_2/log_prob/SparseSoftmaxCrossEntropyWithLogits/Reshape_1)]]\n\nCaused by op 'inference_1/sample_1/Categorical_2/log_prob/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-01265a6dfeda>\", line 37, in <module>\n    inference.initialize()\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/edward/inferences/klqp.py\", line 110, in initialize\n    return super(KLqp, self).initialize(*args, **kwargs)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/edward/inferences/variational_inference.py\", line 68, in initialize\n    self.loss, grads_and_vars = self.build_loss_and_gradients(var_list)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/edward/inferences/klqp.py\", line 160, in build_loss_and_gradients\n    return build_score_rb_loss_and_gradients(self, var_list)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/edward/inferences/klqp.py\", line 1067, in build_score_rb_loss_and_gradients\n    qz_copy.log_prob(tf.stop_gradient(dict_swap[z])))\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tensorflow/python/ops/distributions/distribution.py\", line 716, in log_prob\n    return self._call_log_prob(value, name)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tensorflow/python/ops/distributions/distribution.py\", line 698, in _call_log_prob\n    return self._log_prob(value, **kwargs)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tensorflow/python/ops/distributions/categorical.py\", line 307, in _log_prob\n    logits=logits)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 2055, in sparse_softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 4753, in _sparse_softmax_cross_entropy_with_logits\n    labels=labels, name=name)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Received a label value of 2 which is outside the valid range of [0, 2).  Label values: 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n\t [[Node: inference_1/sample_1/Categorical_2/log_prob/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](inference_1/sample_1/Categorical_2/log_prob/SparseSoftmaxCrossEntropyWithLogits/Reshape, inference_1/sample_1/Categorical_2/log_prob/SparseSoftmaxCrossEntropyWithLogits/Reshape_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    517\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Received a label value of 2 which is outside the valid range of [0, 2).  Label values: 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n\t [[Node: inference_1/sample_1/Categorical_2/log_prob/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](inference_1/sample_1/Categorical_2/log_prob/SparseSoftmaxCrossEntropyWithLogits/Reshape, inference_1/sample_1/Categorical_2/log_prob/SparseSoftmaxCrossEntropyWithLogits/Reshape_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-01265a6dfeda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'VB iteration %i'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0minfo_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_pi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/edward/inferences/variational_inference.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, feed_dict)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincrement_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Received a label value of 2 which is outside the valid range of [0, 2).  Label values: 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n\t [[Node: inference_1/sample_1/Categorical_2/log_prob/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](inference_1/sample_1/Categorical_2/log_prob/SparseSoftmaxCrossEntropyWithLogits/Reshape, inference_1/sample_1/Categorical_2/log_prob/SparseSoftmaxCrossEntropyWithLogits/Reshape_1)]]\n\nCaused by op 'inference_1/sample_1/Categorical_2/log_prob/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-01265a6dfeda>\", line 37, in <module>\n    inference.initialize()\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/edward/inferences/klqp.py\", line 110, in initialize\n    return super(KLqp, self).initialize(*args, **kwargs)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/edward/inferences/variational_inference.py\", line 68, in initialize\n    self.loss, grads_and_vars = self.build_loss_and_gradients(var_list)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/edward/inferences/klqp.py\", line 160, in build_loss_and_gradients\n    return build_score_rb_loss_and_gradients(self, var_list)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/edward/inferences/klqp.py\", line 1067, in build_score_rb_loss_and_gradients\n    qz_copy.log_prob(tf.stop_gradient(dict_swap[z])))\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tensorflow/python/ops/distributions/distribution.py\", line 716, in log_prob\n    return self._call_log_prob(value, name)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tensorflow/python/ops/distributions/distribution.py\", line 698, in _call_log_prob\n    return self._log_prob(value, **kwargs)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tensorflow/python/ops/distributions/categorical.py\", line 307, in _log_prob\n    logits=logits)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 2055, in sparse_softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 4753, in _sparse_softmax_cross_entropy_with_logits\n    labels=labels, name=name)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/home/local/UKP/simpson/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Received a label value of 2 which is outside the valid range of [0, 2).  Label values: 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n\t [[Node: inference_1/sample_1/Categorical_2/log_prob/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](inference_1/sample_1/Categorical_2/log_prob/SparseSoftmaxCrossEntropyWithLogits/Reshape, inference_1/sample_1/Categorical_2/log_prob/SparseSoftmaxCrossEntropyWithLogits/Reshape_1)]]\n"
     ]
    }
   ],
   "source": [
    "print(pword.shape)\n",
    "print(q_pword.shape)\n",
    "\n",
    "print(mu.shape)\n",
    "print(q_mu.shape)\n",
    "\n",
    "print(sigmasq.shape)\n",
    "print(q_sigmasq.shape)\n",
    "\n",
    "print(pi.shape)\n",
    "print(q_pi.shape)\n",
    "\n",
    "print(cw.shape)\n",
    "print(cw_train.shape)\n",
    "\n",
    "print(senses.shape)\n",
    "print(q_senses.shape)\n",
    "\n",
    "latent_vars = {\n",
    "    senses: q_senses,\n",
    "    mu: q_mu,\n",
    "    sigmasq: q_sigmasq,\n",
    "    pi: q_pi, \n",
    "    pword: q_pword\n",
    "}\n",
    "\n",
    "data = {\n",
    "    cw: cw_train,\n",
    "}\n",
    "\n",
    "inference = ed.KLqp(latent_vars, data)\n",
    "\n",
    "n_iter = 100\n",
    "\n",
    "#inference.run(n_iter=n_iter)\n",
    "\n",
    "inference.initialize()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for i in range(n_iter):\n",
    "    print('VB iteration %i' % i)\n",
    "    info_dict = inference.update()\n",
    "    inference.print_progress(info_dict)\n",
    "    print(q_pi.mean().eval())\n",
    "    \n",
    "inference.finalize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print out the sense labels inferred for all the central word occurrences\n",
    "\n",
    "Esenses = q_senses.eval()\n",
    "\n",
    "for w in range(V):\n",
    "    for n in range(N):    \n",
    "        print('word %i, sample %i, -- probability of senses = %s, true sense is %i' % (n, w, str(Esenses[n, w, s]), s_all[w, n]) )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_toy_dataset(N, w, noise_std=0.1):\n",
    "  D = len(w)\n",
    "  x = np.random.randn(N, D)\n",
    "  y = np.dot(x, w) + np.random.normal(0, noise_std, size=N)\n",
    "  return x, y\n",
    "\n",
    "N = 40  # number of data points\n",
    "D = 10  # number of features\n",
    "\n",
    "w_true = np.random.randn(D)\n",
    "X_train, y_train = build_toy_dataset(N, w_true)\n",
    "X_test, y_test = build_toy_dataset(N, w_true)\n",
    "\n",
    "from edward.models import Normal\n",
    "\n",
    "X = tf.placeholder(tf.float32, [N, D])\n",
    "w = Normal(loc=tf.zeros(D), scale=tf.ones(D))\n",
    "b = Normal(loc=tf.zeros(1), scale=tf.ones(1))\n",
    "y = Normal(loc=ed.dot(X, w) + b, scale=tf.ones(N))\n",
    "\n",
    "qw = Normal(loc=tf.get_variable(\"qw/loc\", [D]),\n",
    "            scale=tf.nn.softplus(tf.get_variable(\"qw/scale\", [D])))\n",
    "qb = Normal(loc=tf.get_variable(\"qb/loc\", [1]),\n",
    "            scale=tf.nn.softplus(tf.get_variable(\"qb/scale\", [1])))\n",
    "\n",
    "inference = ed.KLqp({w: qw, b: qb}, data={X: X_train, y: y_train})\n",
    "inference.run(n_samples=5, n_iter=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### What makes our method novel?\n",
    "\n",
    "A fully Bayesian approach to learning word embeddings with multiple, potentially infinite numbers of distinct senses per token.\n",
    "\n",
    "The Bayesian treatment is intended to help with:\n",
    "* Rare words in the training corpus, whose embeddings cannot be confidently estimated -- variance means we don't put too much weight onto these uncertain cases during learning\n",
    "* Inferring the number of senses -- priors effectively regularise the model toward fewer senses\n",
    "* Domain adaptation/Transfer learning -- we can inflate variances to indicate uncertainty in new domains\n",
    "* (As in Barkan, Brazinskas et al) context-specific embeddings for each word instance\n",
    "* (As in Barkan, Brazinskas et al) composition of sentence or document embeddings -- word occurrences with more confident or precise embeddings will have stronger influence in the combined sentence embedding. I think this will push the sentence embeddings away from generic words and toward the extremes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we test our model?\n",
    "\n",
    "* Look at the tasks tried by Brazinskas et al, Barkan, and the ACL 2018 paper and ty to reuse their code where possible\n",
    "* Compute the context-specific embeddings for each word (posterior means)\n",
    "* Test what happens if we concatenate the variances to the embedding vector as a vagueness or uncertainty feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bac",
   "language": "python",
   "name": "bac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
