{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Value error parsing header in AFM: b'Notice' b'\\xa9 Copyright SoftMaker Software GmbH and its licensors'\n",
      "Value error parsing header in AFM: b'Notice' b'\\xa9 Copyright SoftMaker Software GmbH and its licensors'\n",
      "Value error parsing header in AFM: b'Notice' b'\\xa9 Copyright SoftMaker Software GmbH and its licensors'\n"
     ]
    }
   ],
   "source": [
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.stats import multivariate_normal as mvn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0. Sense distribution = [0.78987805 0.21012195]\n",
      "Word 1. Sense distribution = [0.6760104 0.3239896]\n",
      "Word 2. Sense distribution = [0.60780286 0.39219714]\n",
      "Word 3. Sense distribution = [0.67907698 0.32092302]\n",
      "Word 4. Sense distribution = [0.55288706 0.44711294]\n",
      "Word 5. Sense distribution = [0.64739543 0.35260457]\n",
      "Word 6. Sense distribution = [0.89756743 0.10243257]\n",
      "Word 7. Sense distribution = [0.25691385 0.74308615]\n",
      "Word 8. Sense distribution = [0.50855823 0.49144177]\n",
      "Word 9. Sense distribution = [0.46731234 0.53268766]\n",
      "Word 10. Sense distribution = [0.12639621 0.87360379]\n",
      "Word 11. Sense distribution = [0.728379 0.271621]\n",
      "Word 12. Sense distribution = [0.65390785 0.34609215]\n",
      "Word 13. Sense distribution = [0.8354555 0.1645445]\n",
      "Word 14. Sense distribution = [0.03579466 0.96420534]\n",
      "Word 15. Sense distribution = [0.26227659 0.73772341]\n",
      "Word 16. Sense distribution = [0.49030342 0.50969658]\n",
      "Word 17. Sense distribution = [0.58381061 0.41618939]\n",
      "Word 18. Sense distribution = [0.1697819 0.8302181]\n",
      "Word 19. Sense distribution = [0.98533847 0.01466153]\n",
      "Word 20. Sense distribution = [0.59412984 0.40587016]\n",
      "Word 21. Sense distribution = [0.8409508 0.1590492]\n",
      "Word 22. Sense distribution = [0.44650985 0.55349015]\n",
      "Word 23. Sense distribution = [0.58158305 0.41841695]\n",
      "Word 24. Sense distribution = [0.31904895 0.68095105]\n",
      "Word 25. Sense distribution = [0.85530932 0.14469068]\n",
      "Word 26. Sense distribution = [0.0550444 0.9449556]\n",
      "Word 27. Sense distribution = [0.95390021 0.04609979]\n",
      "Word 28. Sense distribution = [0.30364329 0.69635671]\n",
      "Word 29. Sense distribution = [0.64668866 0.35331134]\n",
      "Word 30. Sense distribution = [0.22680566 0.77319434]\n",
      "Word 31. Sense distribution = [5.22933516e-04 9.99477066e-01]\n",
      "Word 32. Sense distribution = [0.28028523 0.71971477]\n",
      "Word 33. Sense distribution = [0.10464214 0.89535786]\n",
      "Word 34. Sense distribution = [0.5511308 0.4488692]\n",
      "Word 35. Sense distribution = [0.19753688 0.80246312]\n",
      "Word 36. Sense distribution = [0.20029223 0.79970777]\n",
      "Word 37. Sense distribution = [0.36821031 0.63178969]\n",
      "Word 38. Sense distribution = [0.01988314 0.98011686]\n",
      "Word 39. Sense distribution = [0.44145605 0.55854395]\n",
      "Word 40. Sense distribution = [0.38712949 0.61287051]\n",
      "Word 41. Sense distribution = [0.19617629 0.80382371]\n",
      "Word 42. Sense distribution = [0.63307956 0.36692044]\n",
      "Word 43. Sense distribution = [0.80818299 0.19181701]\n",
      "Word 44. Sense distribution = [0.80306869 0.19693131]\n",
      "Word 45. Sense distribution = [0.83799612 0.16200388]\n",
      "Word 46. Sense distribution = [0.70086554 0.29913446]\n",
      "Word 47. Sense distribution = [0.71123335 0.28876665]\n",
      "Word 48. Sense distribution = [0.19586775 0.80413225]\n",
      "Word 49. Sense distribution = [0.19177205 0.80822795]\n",
      "Word 50. Sense distribution = [0.39301309 0.60698691]\n",
      "Word 51. Sense distribution = [0.05803081 0.94196919]\n",
      "Word 52. Sense distribution = [0.06014926 0.93985074]\n",
      "Word 53. Sense distribution = [0.44621052 0.55378948]\n",
      "Word 54. Sense distribution = [0.16579889 0.83420111]\n",
      "Word 55. Sense distribution = [0.42769861 0.57230139]\n",
      "Word 56. Sense distribution = [0.92595394 0.07404606]\n",
      "Word 57. Sense distribution = [0.86861203 0.13138797]\n",
      "Word 58. Sense distribution = [0.00991714 0.99008286]\n",
      "Word 59. Sense distribution = [0.21425007 0.78574993]\n",
      "Word 60. Sense distribution = [0.35097664 0.64902336]\n",
      "Word 61. Sense distribution = [0.63117793 0.36882207]\n",
      "Word 62. Sense distribution = [0.04522707 0.95477293]\n",
      "Word 63. Sense distribution = [0.44237004 0.55762996]\n",
      "Word 64. Sense distribution = [0.10051203 0.89948797]\n",
      "Word 65. Sense distribution = [0.56363954 0.43636046]\n",
      "Word 66. Sense distribution = [2.16218444e-04 9.99783782e-01]\n",
      "Word 67. Sense distribution = [0.30723022 0.69276978]\n",
      "Word 68. Sense distribution = [0.71693056 0.28306944]\n",
      "Word 69. Sense distribution = [0.81538382 0.18461618]\n",
      "Word 70. Sense distribution = [0.92590737 0.07409263]\n",
      "Word 71. Sense distribution = [0.60810772 0.39189228]\n",
      "Word 72. Sense distribution = [0.45091421 0.54908579]\n",
      "Word 73. Sense distribution = [0.75862871 0.24137129]\n",
      "Word 74. Sense distribution = [0.54134563 0.45865437]\n",
      "Word 75. Sense distribution = [0.36665599 0.63334401]\n",
      "Word 76. Sense distribution = [0.74348699 0.25651301]\n",
      "Word 77. Sense distribution = [0.0912126 0.9087874]\n",
      "Word 78. Sense distribution = [0.92778995 0.07221005]\n",
      "Word 79. Sense distribution = [0.7223175 0.2776825]\n",
      "Word 80. Sense distribution = [0.3486601 0.6513399]\n",
      "Word 81. Sense distribution = [0.60570744 0.39429256]\n",
      "Word 82. Sense distribution = [0.37708175 0.62291825]\n",
      "Word 83. Sense distribution = [0.42131895 0.57868105]\n",
      "Word 84. Sense distribution = [0.06176369 0.93823631]\n",
      "Word 85. Sense distribution = [0.05827716 0.94172284]\n",
      "Word 86. Sense distribution = [0.16997815 0.83002185]\n",
      "Word 87. Sense distribution = [0.23006331 0.76993669]\n",
      "Word 88. Sense distribution = [0.01283994 0.98716006]\n",
      "Word 89. Sense distribution = [0.67570064 0.32429936]\n",
      "Word 90. Sense distribution = [0.39828372 0.60171628]\n",
      "Word 91. Sense distribution = [0.68192647 0.31807353]\n",
      "Word 92. Sense distribution = [0.90726432 0.09273568]\n",
      "Word 93. Sense distribution = [0.09444525 0.90555475]\n",
      "Word 94. Sense distribution = [0.56098035 0.43901965]\n",
      "Word 95. Sense distribution = [0.60420365 0.39579635]\n",
      "Word 96. Sense distribution = [0.6137439 0.3862561]\n",
      "Word 97. Sense distribution = [0.90055297 0.09944703]\n",
      "Word 98. Sense distribution = [2.05152477e-04 9.99794848e-01]\n",
      "Word 99. Sense distribution = [0.2302722 0.7697278]\n"
     ]
    }
   ],
   "source": [
    "def build_toy_dataset(N):\n",
    "  mu0 = np.zeros(D) # prior mean of the sense means\n",
    "  sigma0 = np.ones(D) # prior covariance of the sense means\n",
    "  a0 = np.ones(D) # prior shape for the scale of the sense covariance\n",
    "  b0 = np.ones(D) # prior scale for the scale of the sense covariance   \n",
    "  alpha0 = np.ones(S) # priors for the sense probabilities\n",
    "  beta0 = np.ones(V) # priors over the word frequencies\n",
    "    \n",
    "  mus_all = []\n",
    "  Sigmas_all = []\n",
    "  pis_all = []\n",
    "\n",
    "  # draw word frequencies\n",
    "  pword = np.random.dirichlet(beta0)\n",
    "  \n",
    "  # draw the word sense distributions\n",
    "  for w in range(V):\n",
    "    mus = []\n",
    "    Sigmas = []\n",
    "    pis = np.random.dirichlet(alpha0)\n",
    "    \n",
    "    # draw the means for each sense\n",
    "    for s in range(S):\n",
    "        mus.append(np.random.multivariate_normal(mu0, np.diag(sigma0)))\n",
    "        Sigmas.append(np.diag(1.0 / np.random.gamma(a0, b0)))\n",
    "        \n",
    "    mus_all.append(mus)\n",
    "    Sigmas_all.append(Sigmas)\n",
    "    pis_all.append(pis)\n",
    "    \n",
    "  s_all = []\n",
    "  z_all = []\n",
    "  c_all = []\n",
    "    \n",
    "  # draw the context windows for each word\n",
    "  for w in range(V):\n",
    "    \n",
    "    # draw the sense for each context\n",
    "    print('Word %i. Sense distribution = %s' % (w, str(pis_all[w])))\n",
    "    s_ws = np.argmax(np.random.multinomial(1, pis_all[w], N), 1)\n",
    "    z_ws = []\n",
    "    \n",
    "    c_ws = []\n",
    "    \n",
    "    for n in range(N):\n",
    "        \n",
    "        #print('Sense for pair n = %i' % s_ws[n])\n",
    "        #print('Mean for chosen sense = %s' % str(mus_all[w][s_ws[n]]))\n",
    "        #print('Cov for chosen sense = %s' % str(Sigmas_all[w][s_ws[n]]))\n",
    "        \n",
    "        # draw the embedding for each context\n",
    "        z_ws.append(np.random.multivariate_normal(mus_all[w][s_ws[n]], Sigmas_all[w][s_ws[n]]))\n",
    "    \n",
    "        # construct the categorical distribution over all words\n",
    "        joint = []\n",
    "        for w2 in range(V):\n",
    "            \n",
    "            pw2 = 0\n",
    "            for s in range(S):\n",
    "                pw2 += pis_all[w2][s] * mvn.pdf(z_ws[-1], mus_all[w2][s], Sigmas_all[w2][s])\n",
    "            pw2 *= pword[w2]\n",
    "            joint.append(pw2)\n",
    "        \n",
    "        pc_giv_z = joint / np.sum(joint)\n",
    "        \n",
    "        c = np.argmax(np.random.multinomial(1, pc_giv_z, C), 1)\n",
    "        c_ws.append(c)\n",
    "        \n",
    "    s_all.append(s_ws)\n",
    "    z_all.append(z_ws)\n",
    "    c_all.append(c_ws)\n",
    "        \n",
    "  c_all = np.array(c_all).swapaxes(0, 1).swapaxes(1, 2) # so we get N x C x V from V x N x C\n",
    "        \n",
    "  return c_all\n",
    "\n",
    "C = 10\n",
    "N = 100  # number of context windows for each word in the vocabulary\n",
    "S = 2  # number of different senses per word. TODO: make this random for each word\n",
    "D = 10 # dimensionality of the embeddings\n",
    "V = 100 # vocabulary size\n",
    "\n",
    "c_all = build_toy_dataset(N)\n",
    "\n",
    "cw_train = c_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'T'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-66cdeb7c8e68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;34m{\u001b[0m\u001b[0;34m'mus'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Sigmas'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msigmasq\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mContextWindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 sample_shape=N) # result should be N X V x C\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0msense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/edward/models/param_mixture.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;31m# Generate random variable class similar to autogenerated ones from TensorFlow.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m   \u001b[0mRandomVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/edward/models/random_variable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'collections'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRandomVariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/edward/models/param_mixture.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mixing_weights, component_params, component_dist, validate_args, allow_nan_stats, name)\u001b[0m\n\u001b[1;32m     90\u001b[0m                                         \u001b[0msample_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                                         \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                                         **component_params)\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-66cdeb7c8e68>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mRandomVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/edward/models/random_variable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         raise NotImplementedError(\n",
      "\u001b[0;32m~/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tensorflow/python/ops/distributions/distribution.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, sample_shape, seed, name)\u001b[0m\n\u001b[1;32m    687\u001b[0m       \u001b[0msamples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mprepended\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \"\"\"\n\u001b[0;32m--> 689\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_sample_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/bayesian_annotator_combination/env/lib/python3.5/site-packages/tensorflow/python/ops/distributions/distribution.py\u001b[0m in \u001b[0;36m_call_sample_n\u001b[0;34m(self, sample_shape, seed, name, **kwargs)\u001b[0m\n\u001b[1;32m    666\u001b[0m       sample_shape, n = self._expand_sample_shape_to_vector(\n\u001b[1;32m    667\u001b[0m           sample_shape, \"sample_shape\")\n\u001b[0;32m--> 668\u001b[0;31m       \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m       \u001b[0mbatch_event_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m       \u001b[0mfinal_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_event_shape\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-66cdeb7c8e68>\u001b[0m in \u001b[0;36m_sample_n\u001b[0;34m(self, n, seed)\u001b[0m\n\u001b[1;32m     43\u001b[0m     )\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mpw2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmvn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mpw2\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mpword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mjoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'T'"
     ]
    }
   ],
   "source": [
    "from edward.models import Dirichlet, InverseGamma, MultivariateNormalDiag, \\\n",
    "    Normal, ParamMixture, Categorical\n",
    "\n",
    "from edward.models import RandomVariable\n",
    "from tensorflow.contrib.distributions import Distribution\n",
    "\n",
    "class distributions_ContextWindow(Distribution):\n",
    "  def __init__(self, senses, mus, Sigmas, validate_args=False,\n",
    "                  allow_nan_stats=True,\n",
    "                  name=\"ContextWindow\"):\n",
    "    \n",
    "    self.senses = senses\n",
    "    self.mus = mus\n",
    "    self.Sigmas = Sigmas\n",
    "    \n",
    "    super(distributions_ContextWindow, self).__init__(\n",
    "            dtype=tf.float32,\n",
    "            reparameterization_type=tf.contrib.distributions.FULLY_REPARAMETERIZED,\n",
    "            validate_args=validate_args,\n",
    "            allow_nan_stats=allow_nan_stats,\n",
    "            name=name)\n",
    "    \n",
    "    \n",
    "  def _log_prob(self, value):\n",
    "    raise NotImplementedError(\"log_prob is not implemented\")\n",
    "\n",
    "  def _sample_n(self, n, seed=None):\n",
    "   \n",
    "    c_all = []\n",
    "    \n",
    "    context_words = []\n",
    "\n",
    "    # sample the vectors for each word given its selected sense    \n",
    "    z_w = MultivariateNormalDiag(self.mus[self.senses, range(V)], \n",
    "                                 self.Sigmas[self.senses, range(V)]\n",
    "                                )._sample_n(1, seed)[0]\n",
    "\n",
    "    # construct the categorical distribution over all words\n",
    "    joint = []\n",
    "\n",
    "    mvn = MultivariateNormalDiag(\n",
    "                                    loc=self.mus[self.senses, range(V)], # this assumes that the senses were instantiated at random from pi and were different for each word\n",
    "                                    scale_diag=self.Sigmas[self.senses, range(V)]\n",
    "    )\n",
    "\n",
    "    pw2 = mvn.prob(z_w) # z_w has shape V. Mvn contains V distributions. So pw2 has shape V x V\n",
    "    pw2 *= tf.reshape(pword, shape=(1, V))\n",
    "\n",
    "    pc_giv_z = pw2 / tf.reduce_sum(joint, keepdims=True, axis=1) # shape VxV\n",
    "\n",
    "    c_all = Categorical(probs=pc_giv_z).sample((n, C), seed) # V different distributions in one line. Shape = N x C x V\n",
    "        \n",
    "    return c_all\n",
    "   \n",
    "               \n",
    "class ContextWindow(RandomVariable, distributions_ContextWindow):\n",
    "               \n",
    "  def __init__(self, *args, **kwargs):\n",
    "    \n",
    "    RandomVariable.__init__(self, *args, **kwargs)\n",
    "\n",
    "D = 3\n",
    "S = 2\n",
    "V = 100\n",
    "N = 100 # sample contexts per word\n",
    "C = 10 # context window size\n",
    "\n",
    "pword = Dirichlet(tf.ones(V))\n",
    "\n",
    "pi = Dirichlet(tf.ones(S), sample_shape=V) # sense distributions for each word. Needs replacing with CRP\n",
    "\n",
    "mu = Normal(tf.zeros(D), tf.ones(D), sample_shape=(S, V))\n",
    "sigmasq = InverseGamma(tf.ones(D), tf.ones(D), sample_shape=(S, V))\n",
    "\n",
    "sense = Categorical(probs=pi, sample_shape=N)\n",
    "\n",
    "cw = ContextWindow(sense, mu, sigmas,\n",
    "                sample_shape=N) # result should be N X V x C\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# approximate distributions\n",
    "\n",
    "q_mu = Normal(\n",
    "    loc=tf.get_variable(\"q_mu/loc\", [S * V]),\n",
    "    scale=tf.nn.softplus(tf.get_variable(\"q_mu/scale\", [S * V]))\n",
    ")\n",
    "\n",
    "q_sigmasq = InverseGamma(\n",
    "    concentration=tf.nn.softplus(tf.get_variable(\"q_sigmasq/concentration\", [S * V])),\n",
    "    rate=tf.nn.softplus(tf.get_variable(\"q_sigmasq/rate\", [S * V]))\n",
    ")\n",
    "\n",
    "q_pi = Dirichlet(\n",
    "    concentration=tf.nn.softplus(tf.get_variable(\"q_pi/concentration\", [S * V]))\n",
    ")\n",
    "\n",
    "q_sense = Categorical(\n",
    "    probs=tf.nn.softmax(tf.get_variable(\"q_sense/probs\", [S * V * N]))\n",
    ")\n",
    "\n",
    "q_pword = Categorical(\n",
    "    probs=tf.nn.softmax(tf.get_variable(\"q_pword/probs\", [S * V * N]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latent_vars = {\n",
    "    #sense: q_sense,\n",
    "    mu: q_mu,\n",
    "    sigmasq: q_sigmasq,\n",
    "    pi: q_pi, \n",
    "    pword: q_pword\n",
    "}\n",
    "\n",
    "data = {\n",
    "    cw: cw_train,\n",
    "}\n",
    "\n",
    "inference = ed.KLqp(latent_vars, data)\n",
    "inference.run(n_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate likelihood for each data point and cluster assignment,\n",
    "# averaged over many posterior samples. ``x_post`` has shape (N, 100, K, D).\n",
    "mu_sample = qmu.sample(100)\n",
    "sigmasq_sample = qsigmasq.sample(100)\n",
    "x_post = Normal(loc=tf.ones([N, 1, 1, 1]) * mu_sample,\n",
    "                scale=tf.ones([N, 1, 1, 1]) * tf.sqrt(sigmasq_sample))\n",
    "x_broadcasted = tf.tile(tf.reshape(x_train, [N, 1, 1, D]), [1, 100, K, 1])\n",
    "\n",
    "# Sum over latent dimension, then average over posterior samples.\n",
    "# ``log_liks`` ends up with shape (N, K).\n",
    "log_liks = x_post.log_prob(x_broadcasted)\n",
    "log_liks = tf.reduce_sum(log_liks, 3)\n",
    "log_liks = tf.reduce_mean(log_liks, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters = tf.argmax(log_liks, 1).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import cm \n",
    "plt.scatter(x_train[:, 0], x_train[:, 1], c=clusters, cmap=cm.bwr)\n",
    "plt.axis([-3, 3, -3, 3])\n",
    "plt.title(\"Predicted cluster assignments\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bac",
   "language": "python",
   "name": "bac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
